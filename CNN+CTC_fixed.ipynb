{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5jB4W9YiJfyd",
    "outputId": "0364ec74-ce9a-42b3-bdf7-5ed9d398ff97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
      "Collecting jiwer\n",
      "  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.13.1)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.1)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer)\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.11.12)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.12.1->librosa) (2.0.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.23)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading jiwer-4.0.0-py3-none-any.whl (23 kB)\n",
      "Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-4.0.0 rapidfuzz-3.14.3\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers librosa torch torchaudio jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6-AYZJ0hJr9K",
    "outputId": "797f827e-5dce-41b8-f877-60ee6fa85d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchcodec\n",
      "  Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchcodec\n",
      "Successfully installed torchcodec-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pcbhLNsvJuhM",
    "outputId": "6d321a2b-88ae-4b7c-d40d-1d83a891e7a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "0% [Working]\r",
      "            \r",
      "Get:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
      "\r",
      "0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\r",
      "0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\r",
      "                                                                               \r",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
      "\r",
      "0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Connecting to security.\r",
      "                                                                               \r",
      "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
      "Get:4 https://cli.github.com/packages stable/main amd64 Packages [345 B]\n",
      "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,202 kB]\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
      "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
      "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
      "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
      "Get:11 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
      "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
      "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
      "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,532 kB]\n",
      "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,598 kB]\n",
      "Get:16 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,185 kB]\n",
      "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,961 kB]\n",
      "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,410 kB]\n",
      "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,623 kB]\n",
      "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,287 kB]\n",
      "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,848 kB]\n",
      "Fetched 38.0 MB in 4s (8,888 kB/s)\n",
      "Reading package lists... Done\n",
      "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
      "Requirement already satisfied: librosa in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from soundfile) (2.0.2)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from librosa) (3.1.0)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.60.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.16.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.4.2)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (4.15.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa) (1.1.2)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.23)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (4.5.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.1->librosa) (2.32.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!apt-get update\n",
    "!apt-get install -y ffmpeg\n",
    "!pip install soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 561,
     "referenced_widgets": [
      "75372b74670b48ba9fcde992639e6d85",
      "e3c49a238973406faa4397f203cf6d91",
      "7b3b0675a33845c49440bc5977f4a5f0",
      "4e00232e227a4ad585ebfcede46979ae",
      "2002e1b409b048139a3b53e22f3a7ec0",
      "5e9b50e8f3e1454588a2a9589643d334",
      "02dfee1361c64781b6aeca1df5950b3a",
      "2b6b34f4c4444ce9a457bd8a3de26105",
      "ea567ad76ff64fcfa0fe742024fcca55",
      "41bc7f5784464257a621b3a7e975cb73",
      "6933758f6b2a4e61a57e0d0d32bc4100",
      "35645819382b4d919c845d1414cbf43c",
      "be80c1b26130486daaaa87202d1622b4",
      "5c94b3d3ff08484d9fcb119d5d01416c",
      "fe5941cc24ae47feaad0c6670177e149",
      "f6f2c88829744a0b8a6a41c0c7c7206f",
      "20c5c5557f25430fb849584ed7f49928",
      "3ee06534a6784cf8a4d86d7af777f2eb",
      "ff1ccf5f0e8c4d6e9ca3e20e9f8ca14f",
      "74b46be5a6e6419ba236d618deb65d9e",
      "bc0f1d71e0df462aaab5d1cd90ac80ff",
      "06846dbab8a547318c5ced927c95c734",
      "e7425ad58e394dbf813723a4ddfa33cf",
      "3eb0074b72c74c60ab26a4d53eed2d2b",
      "b34d10f57cf441f88c8e4d09a4754763",
      "2d69ecdd493b43aa8e4f61c0fc1507b4",
      "eb06cf723f2a4577be8d81cb1285b641",
      "24ee5296bcf04849a8b773010de86b49",
      "1e5b014c3bc44d4eb0fc1d6e8142f2b7",
      "a7602156822e4b58bf6310822933ff37",
      "cd1c6f7d35654b12a6b25f78abcdbf0c",
      "7cfc7f7786ef42bd9a39ca0fed360e88",
      "5c30f228ab62464cb8f4d7ce8ec87ae2",
      "767bb798bde0468ab82f5c638a87486b",
      "622c76d13d99438288e1e1a46c9dcf8d",
      "80578d99b1c44e168b45977842056056",
      "f44147e9af1b4526acaf80bb39660951",
      "3e3c09fcaba642b8adf4d7f21331c6df",
      "cc1457bb927f4d53b99492d796164eb1",
      "cd9652bc536942839d70caf9c386df19",
      "464f3df0d6094921801f87a77396e230",
      "4cbcf29d76b84740af07ebff605533da",
      "473ac89690df45529f985a1e1ad755e1",
      "ee6ca743063a4d518487427872ed68b8",
      "4711d341ae22466a8298c83a25aeb940",
      "1e61f122dfdc43c3ae354d418440e877",
      "063843c5a4f6448bafdf091b802ed4e1",
      "44579191fd124f0482eff8a02dbee12e",
      "4bee49f020334b258339269131e433f7",
      "1a4f9bfc08a24ae19b90e070c741a771",
      "184ab444dc5144a1ac2fe61ac848a62a",
      "c166d7e97d8c45038211899453990ce5",
      "086f05dceb2f482c828f7c80af11254e",
      "5eaea8b57e6f451881d59e4b2fd4ad3c",
      "1552f7ee7f104ca188b304ebadbe1569",
      "ba3da5e118704c9883e0b6fa6fd21281",
      "1f6c44250f7043b1850c675c5159e668",
      "775af14a4320419a8471d0d0b75ec667",
      "0c7389362b4a4775a782b26e8833919d",
      "678ace0818b84292ac4dcaffbf0f3d96",
      "adadc3114ced42ac9e2dea7e61be220a",
      "5cfe223a63ff4fab82155290ce89bee8",
      "4f718b8284864be2bbd6052206bdfc08",
      "4fefc59e946443b293452b18331373ac",
      "9224fe2813dd49e4a1e7f067fa9b4d7f",
      "dfcd6ae5f5fc4c02ae3fd831968e6010",
      "25f358cb593d40c6b36c3066f37959bd",
      "ecf30e10719846a49303d396e17b1946",
      "925f9730dc2940de904b3629f0948c5c",
      "59336c8c7791466ebf685879ea3a4045",
      "c4a030eeac23414eb91a7812bb9f7808",
      "5b15ba4b2b3940dfbe48cc9b0105b201",
      "b5dfbc13c8af4d98b04b3dd9393efbd7",
      "9e7db576f4d24c359e3dc9c01251a5c8",
      "33d82e8c4cd3406699f5c48062317b3e",
      "ab8619a2e02848f28158027010bbbb39",
      "6a55fdeb3fec4cf1b1371673d76199c9",
      "2418834a05bc498686519cb432bbc7d2",
      "35e0e83e16ce41b8b50c5365587b9d95",
      "da7814c337934805b4817b5ef0f2d5c5",
      "236b24f31e8a4fa6a53d33aaf4099356",
      "1899f51a9be64bd187af56a7137bad14",
      "7e8940634f1a427b81d982a5a27b64dd",
      "b67fe0972fc5426b97d24ad81e561b68",
      "3a8e2756e0864e29931db8f7655678f6",
      "8b8509ed1f7e4619b5c45976b4a6ea06",
      "5507bfbdf7c64251adcb0543bd8912d5",
      "044e00152710436da51f8ae67ebf5a9b"
     ]
    },
    "id": "Hen6v5ePKBij",
    "outputId": "f05f9cf9-79b5-4f80-af51-c91c991c55fe"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75372b74670b48ba9fcde992639e6d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35645819382b4d919c845d1414cbf43c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7425ad58e394dbf813723a4ddfa33cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767bb798bde0468ab82f5c638a87486b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/303M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4711d341ae22466a8298c83a25aeb940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3da5e118704c9883e0b6fa6fd21281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/28024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f358cb593d40c6b36c3066f37959bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2418834a05bc498686519cb432bbc7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio array shape: (54144,)\n",
      "Transcript: تا حرف پول به میان میآید گوشهایش را تیز میکند.\n",
      "Sample rate: 16000\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 1. DATASET LOADING\n",
    "# ============================================================================\n",
    "from datasets import load_dataset, Audio\n",
    "\n",
    "# Load dataset\n",
    "ds = load_dataset(\"hezarai/common-voice-13-fa\")\n",
    "\n",
    "# Cast audio column to Audio feature with target sampling rate\n",
    "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# Now access audio\n",
    "sample = ds['train'][0]\n",
    "print(f\"Audio array shape: {sample['audio']['array'].shape}\")\n",
    "print(f\"Transcript: {sample['sentence']}\")\n",
    "print(f\"Sample rate: {sample['audio']['sampling_rate']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PrhvqX4sKH3h",
    "outputId": "914fcdd0-a2fb-4a9c-99cc-80ea660bf4b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocabulary...\n",
      "Vocabulary size: 58\n",
      "Sample characters: ['!', '.', '،', '؛', '؟', 'ء', 'آ', 'أ', 'ؤ', 'ئ', 'ا', 'ب', 'ة', 'ت', 'ث', 'ج', 'ح', 'خ', 'د']\n",
      "✓ Vocabulary is clean (no English characters)\n",
      "\n",
      "Log mel shape: torch.Size([80, 339])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 2. VOCABULARY AND FEATURE EXTRACTION - FIXED!\n",
    "# ============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Parameters\n",
    "SAMPLE_RATE = 16000\n",
    "N_MELS = 80\n",
    "N_FFT = 400\n",
    "HOP_LENGTH = 160\n",
    "\n",
    "# 1. Create CLEAN Persian vocabulary\n",
    "def create_vocab(dataset):\n",
    "    \"\"\"Create clean Persian character vocabulary\"\"\"\n",
    "    vocab = set()\n",
    "\n",
    "    print(\"Building vocabulary...\")\n",
    "    for item in dataset['train']:\n",
    "        text = item['sentence']\n",
    "        # Keep only Persian characters, spaces, and essential punctuation\n",
    "        text = re.sub(r'[^\\u0600-\\u06FF\\s\\.\\!\\?،]', '', text)\n",
    "        vocab.update(text)\n",
    "\n",
    "    # Sort and create mapping\n",
    "    vocab = sorted(list(vocab))\n",
    "    char_to_idx = {char: idx + 1 for idx, char in enumerate(vocab)}  # 0 reserved for blank\n",
    "    char_to_idx['<blank>'] = 0\n",
    "    idx_to_char = {idx: char for char, idx in char_to_idx.items()}\n",
    "\n",
    "    print(f\"Vocabulary size: {len(char_to_idx)}\")\n",
    "    print(f\"Sample characters: {list(char_to_idx.keys())[1:20]}\")\n",
    "\n",
    "    # Verify no English characters\n",
    "    english_chars = [c for c in char_to_idx.keys() if c.isascii() and c.isalpha()]\n",
    "    if english_chars:\n",
    "        print(f\"WARNING: Found English characters: {english_chars}\")\n",
    "    else:\n",
    "        print(\"✓ Vocabulary is clean (no English characters)\")\n",
    "\n",
    "    return char_to_idx, idx_to_char\n",
    "\n",
    "char_to_idx, idx_to_char = create_vocab(ds)\n",
    "\n",
    "# 2. Feature extraction function (unchanged)\n",
    "def extract_log_mel(audio_array):\n",
    "    \"\"\"Extract log mel spectrogram features\"\"\"\n",
    "    # Convert to tensor\n",
    "    if not isinstance(audio_array, torch.Tensor):\n",
    "        audio_array = torch.tensor(audio_array, dtype=torch.float32)\n",
    "\n",
    "    # Extract mel spectrogram\n",
    "    mel_spec_transform = T.MelSpectrogram(\n",
    "        sample_rate=SAMPLE_RATE,\n",
    "        n_fft=N_FFT,\n",
    "        hop_length=HOP_LENGTH,\n",
    "        n_mels=N_MELS\n",
    "    )\n",
    "\n",
    "    mel_spec = mel_spec_transform(audio_array)\n",
    "\n",
    "    # Convert to log scale\n",
    "    log_mel = torch.log(mel_spec + 1e-9)\n",
    "\n",
    "    return log_mel\n",
    "\n",
    "# Test feature extraction\n",
    "test_audio = ds['train'][0]['audio']['array']\n",
    "test_features = extract_log_mel(test_audio)\n",
    "print(f\"\\nLog mel shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "G59v0S-jKPVA"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. DATASET AND COLLATE - FIXED TEXT ENCODING\n",
    "# ============================================================================\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text to only include characters in vocabulary\"\"\"\n",
    "    # Remove non-Persian characters\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\s\\.\\!\\?،]', '', text)\n",
    "    # Normalize spaces\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "class ASRDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, char_to_idx):\n",
    "        self.dataset = hf_dataset\n",
    "        self.char_to_idx = char_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        # Extract features\n",
    "        audio = item['audio']['array']\n",
    "        log_mel = extract_log_mel(audio)\n",
    "\n",
    "        # Clean and encode text\n",
    "        text = clean_text(item['sentence'])\n",
    "        labels = [self.char_to_idx[char] for char in text if char in self.char_to_idx]\n",
    "\n",
    "        return log_mel.T, torch.tensor(labels, dtype=torch.long)  # Transpose to (time, n_mels)\n",
    "\n",
    "# Collate function (unchanged)\n",
    "def collate_fn(batch):\n",
    "    features, labels = zip(*batch)\n",
    "\n",
    "    # Pad features\n",
    "    feature_lengths = torch.tensor([f.size(0) for f in features])\n",
    "    features_padded = nn.utils.rnn.pad_sequence(features, batch_first=True)\n",
    "\n",
    "    # Pad labels\n",
    "    label_lengths = torch.tensor([len(l) for l in labels])\n",
    "    labels_padded = nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=0)\n",
    "\n",
    "    return features_padded, labels_padded, feature_lengths, label_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UXUTxJLkKYYg"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. MODEL (unchanged)\n",
    "# ============================================================================\n",
    "\n",
    "class CNNCTC(nn.Module):\n",
    "    def __init__(self, n_mels=80, vocab_size=100):\n",
    "        super().__init__()\n",
    "\n",
    "        # CNN layers - Pool only in frequency dimension, not time\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d((1, 2)),  # Only pool frequency: (time, freq) -> (time, freq/2)\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d((1, 2)),  # Only pool frequency: (time, freq/2) -> (time, freq/4)\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )\n",
    "\n",
    "        # Calculate flattened size (only frequency is reduced by 4)\n",
    "        self.rnn_input_size = 128 * (n_mels // 4)\n",
    "\n",
    "        # RNN layers\n",
    "        self.rnn = nn.LSTM(\n",
    "            self.rnn_input_size,\n",
    "            256,\n",
    "            num_layers=2,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=0.2\n",
    "        )\n",
    "\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(512, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, time, n_mels)\n",
    "        x = x.unsqueeze(1)  # (batch, 1, time, n_mels)\n",
    "        x = self.conv(x)  # (batch, 128, time, n_mels')\n",
    "\n",
    "        # Reshape for RNN\n",
    "        batch, channels, time, freq = x.size()\n",
    "        x = x.permute(0, 2, 1, 3)  # (batch, time, channels, freq)\n",
    "        x = x.reshape(batch, time, channels * freq)\n",
    "\n",
    "        # RNN\n",
    "        x, _ = self.rnn(x)\n",
    "\n",
    "        # Output\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x.log_softmax(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2xMGp6uKsPd",
    "outputId": "58af02af-ca21-4306-a048-4cc249cae201"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating datasets...\n",
      "Training batches: 3503\n",
      "Validation batches: 1305\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 5. CREATE DATALOADERS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nCreating datasets...\")\n",
    "train_dataset = ASRDataset(ds['train'], char_to_idx)\n",
    "val_dataset = ASRDataset(ds['validation'], char_to_idx)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=8,  # Start small due to memory\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1SJR891EKuyk",
    "outputId": "9c1e0af3-d764-46db-e3ac-448bcdfb4d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device: cuda\n",
      "✓ Model initialized and ready for training!\n",
      "✓ Vocab size: 58\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 6. INITIALIZE MODEL\n",
    "# ============================================================================\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "model = CNNCTC(n_mels=N_MELS, vocab_size=len(char_to_idx))\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CTCLoss(blank=0, zero_infinity=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "print(\"✓ Model initialized and ready for training!\")\n",
    "print(f\"✓ Vocab size: {len(char_to_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Sc-y_fjTKyDW"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. TRAINING FUNCTIONS (unchanged)\n",
    "# ============================================================================\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (features, labels, feature_lengths, label_lengths) in enumerate(loader):\n",
    "        features = features.to(device)\n",
    "        labels = labels.to(device)\n",
    "        feature_lengths = feature_lengths.to('cpu')  # CTC needs lengths on CPU\n",
    "        label_lengths = label_lengths.to('cpu')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(features)  # (batch, time, vocab)\n",
    "        outputs = outputs.permute(1, 0, 2)  # (time, batch, vocab) for CTC\n",
    "\n",
    "        # CTC loss\n",
    "        loss = criterion(outputs, labels, feature_lengths, label_lengths)\n",
    "\n",
    "        # Check for invalid loss\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(f\"Warning: Invalid loss at batch {batch_idx}\")\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels, feature_lengths, label_lengths in loader:\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            feature_lengths = feature_lengths.to('cpu')\n",
    "            label_lengths = label_lengths.to('cpu')\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(features)\n",
    "            outputs = outputs.permute(1, 0, 2)\n",
    "\n",
    "            # CTC loss\n",
    "            loss = criterion(outputs, labels, feature_lengths, label_lengths)\n",
    "\n",
    "            if not (torch.isnan(loss) or torch.isinf(loss)):\n",
    "                total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Jd9-wOV3K1KN"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 8. DECODING - FIXED TO HANDLE CLEAN VOCAB\n",
    "# ============================================================================\n",
    "\n",
    "def decode_predictions(outputs, idx_to_char):\n",
    "    \"\"\"Greedy CTC decoding with cleaning\"\"\"\n",
    "    # outputs: (batch, time, vocab)\n",
    "    _, max_indices = torch.max(outputs, dim=2)  # (batch, time)\n",
    "\n",
    "    decoded_texts = []\n",
    "    for sequence in max_indices:\n",
    "        # Remove blanks and repetitions\n",
    "        prev_idx = None\n",
    "        decoded = []\n",
    "        for idx in sequence.cpu().numpy():\n",
    "            idx = int(idx)  # Convert to Python int\n",
    "            if idx != 0 and idx != prev_idx:  # 0 is blank\n",
    "                if idx in idx_to_char:\n",
    "                    decoded.append(idx_to_char[idx])\n",
    "            prev_idx = idx\n",
    "        decoded_texts.append(''.join(decoded))\n",
    "\n",
    "    return decoded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "szHx2uVfK36i"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "from jiwer import wer, cer\n",
    "\n",
    "def clean_persian_text(text):\n",
    "    \"\"\"\n",
    "    Clean and normalize Persian text for metric calculation\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Normalize Unicode (important for Persian)\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "\n",
    "    # Replace Arabic/Urdu characters with Persian equivalents\n",
    "    replacements = {\n",
    "        'ي': 'ی',  # Arabic yeh to Persian yeh\n",
    "        'ك': 'ک',  # Arabic kaf to Persian kaf\n",
    "        'ە': 'ه',  # Arabic heh to Persian heh\n",
    "        'ے': 'ی',  # Urdu yeh to Persian yeh (this was in your output!)\n",
    "        '؛': '',   # Arabic semicolon\n",
    "        '،': '',   # Arabic comma\n",
    "        '؟': '',   # Arabic question mark\n",
    "        '.': '',   # Period\n",
    "        '!': '',   # Exclamation\n",
    "        '‌': '',   # Zero-width non-joiner\n",
    "        '‍': '',   # Zero-width joiner\n",
    "        '\\u200c': '',  # ZWNJ\n",
    "        '\\u200d': '',  # ZWJ\n",
    "    }\n",
    "\n",
    "    for old, new in replacements.items():\n",
    "        text = text.replace(old, new)\n",
    "\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Remove any remaining non-Persian characters (keep only Persian letters, digits, spaces)\n",
    "    # Persian Unicode range: \\u0600-\\u06FF and some additional characters\n",
    "    text = re.sub(r'[^\\u0600-\\u06FF\\uFB8A\\u067E\\u0686\\u06AF\\u0698\\u06A9\\u06CC\\s\\d]+', '', text)\n",
    "\n",
    "    # Final whitespace cleanup\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def compute_metrics(predictions, references):\n",
    "    \"\"\"\n",
    "    Compute WER and CER metrics with proper Persian text cleaning\n",
    "    \"\"\"\n",
    "    # Clean both predictions and references\n",
    "    preds_clean = [clean_persian_text(p) for p in predictions]\n",
    "    refs_clean = [clean_persian_text(r) for r in references]\n",
    "\n",
    "    # Filter out empty references\n",
    "    valid_pairs = [(p, r) for p, r in zip(preds_clean, refs_clean) if r.strip() != '']\n",
    "\n",
    "    if not valid_pairs:\n",
    "        return {'wer': 1.0, 'cer': 1.0, 'num_samples': 0}\n",
    "\n",
    "    # IMPORTANT: Convert tuples to lists\n",
    "    preds, refs = zip(*valid_pairs)\n",
    "    preds = list(preds)  # Convert from tuple to list\n",
    "    refs = list(refs)    # Convert from tuple to list\n",
    "\n",
    "    # Handle empty predictions (replace with single space for metric calculation)\n",
    "    preds = [p if p.strip() != '' else ' ' for p in preds]\n",
    "\n",
    "    try:\n",
    "        wer_score = wer(refs, preds)  # Note: reference first, prediction second\n",
    "        cer_score = cer(refs, preds)\n",
    "    except Exception as e:\n",
    "        print(f\"Error computing metrics: {e}\")\n",
    "        print(f\"Type of refs: {type(refs)}, Type of preds: {type(preds)}\")\n",
    "        print(f\"First ref: {refs[0] if refs else 'empty'}\")\n",
    "        print(f\"First pred: {preds[0] if preds else 'empty'}\")\n",
    "        wer_score = 1.0\n",
    "        cer_score = 1.0\n",
    "\n",
    "    return {\n",
    "        'wer': wer_score,\n",
    "        'cer': cer_score,\n",
    "        'num_samples': len(valid_pairs)\n",
    "    }\n",
    "\n",
    "def evaluate_model(model, loader, device, idx_to_char, num_batches=None):\n",
    "    \"\"\"\n",
    "    Evaluate model on validation set with detailed metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (features, labels, _, _) in enumerate(loader):\n",
    "            if num_batches and batch_idx >= num_batches:\n",
    "                break\n",
    "\n",
    "            features = features.to(device)\n",
    "            outputs = model(features)\n",
    "\n",
    "            # Decode predictions\n",
    "            predictions = decode_predictions(outputs, idx_to_char)\n",
    "\n",
    "            # Decode ground truth\n",
    "            for i in range(labels.size(0)):\n",
    "                ref = ''.join([idx_to_char[idx.item()] for idx in labels[i] if idx.item() != 0])\n",
    "                all_references.append(ref)\n",
    "                all_predictions.append(predictions[i])\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(all_predictions, all_references)\n",
    "\n",
    "    # Add some diagnostic info\n",
    "    print(f\"   Evaluated {metrics['num_samples']} samples\")\n",
    "\n",
    "    # Show cleaned examples\n",
    "    if len(all_predictions) > 0:\n",
    "        print(f\"\\n   Sample after cleaning:\")\n",
    "        pred_clean = clean_persian_text(all_predictions[0])\n",
    "        ref_clean = clean_persian_text(all_references[0])\n",
    "        print(f\"   Pred: {pred_clean[:80]}\")\n",
    "        print(f\"   Ref:  {ref_clean[:80]}\")\n",
    "\n",
    "    return metrics, all_predictions, all_references\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XZ7n7bLMK7Qx",
    "outputId": "fe82f570-0276-4128-a262-abeb81225916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing inference on one batch...\n",
      "\n",
      "Before training - Sample prediction:\n",
      "Predicted: بهمیتآن نامرانالیرگرت.ٌے\n",
      "Ground truth: اهمیت آن نامه را نادیده گرفت.\n",
      "✓ No English characters in prediction\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 10. TEST INFERENCE BEFORE TRAINING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nTesting inference on one batch...\")\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_features, test_labels, test_feat_len, test_lab_len = next(iter(val_loader))\n",
    "    test_features = test_features.to(device)\n",
    "    test_outputs = model(test_features)\n",
    "\n",
    "    # Decode predictions\n",
    "    predictions = decode_predictions(test_outputs, idx_to_char)\n",
    "\n",
    "    # Show first example\n",
    "    print(\"\\nBefore training - Sample prediction:\")\n",
    "    print(f\"Predicted: {predictions[0][:80]}\")\n",
    "\n",
    "    # Ground truth\n",
    "    ground_truth = ''.join([idx_to_char[idx.item()] for idx in test_labels[0] if idx.item() != 0])\n",
    "    print(f\"Ground truth: {ground_truth[:80]}\")\n",
    "\n",
    "    # Check for any garbage characters\n",
    "    non_persian = re.findall(r'[A-Za-z]', predictions[0])\n",
    "    if non_persian:\n",
    "        print(f\"WARNING: Found English characters in prediction: {non_persian}\")\n",
    "    else:\n",
    "        print(\"✓ No English characters in prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xSkbRVuhK_OH",
    "outputId": "0920dc8c-26fa-4471-8e44-314db772d260"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Epoch 1/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 1.0110\n",
      "Batch 50/3503, Loss: 0.8348\n",
      "Batch 100/3503, Loss: 1.3331\n",
      "Batch 150/3503, Loss: 0.8374\n",
      "Batch 200/3503, Loss: 0.9033\n",
      "Batch 250/3503, Loss: 0.9584\n",
      "Batch 300/3503, Loss: 0.9261\n",
      "Batch 350/3503, Loss: 0.9659\n",
      "Batch 400/3503, Loss: 1.0686\n",
      "Batch 450/3503, Loss: 0.9697\n",
      "Batch 500/3503, Loss: 0.9011\n",
      "Batch 550/3503, Loss: 0.8020\n",
      "Batch 600/3503, Loss: 1.2037\n",
      "Batch 650/3503, Loss: 1.1359\n",
      "Batch 700/3503, Loss: 1.2275\n",
      "Batch 750/3503, Loss: 0.8350\n",
      "Batch 800/3503, Loss: 0.8070\n",
      "Batch 850/3503, Loss: 0.8213\n",
      "Batch 900/3503, Loss: 0.8686\n",
      "Batch 950/3503, Loss: 0.9465\n",
      "Batch 1000/3503, Loss: 0.9270\n",
      "Batch 1050/3503, Loss: 1.2729\n",
      "Batch 1100/3503, Loss: 1.1114\n",
      "Batch 1150/3503, Loss: 1.3103\n",
      "Batch 1200/3503, Loss: 0.9529\n",
      "Batch 1250/3503, Loss: 0.8968\n",
      "Batch 1300/3503, Loss: 1.2003\n",
      "Batch 1350/3503, Loss: 0.8671\n",
      "Batch 1400/3503, Loss: 0.7322\n",
      "Batch 1450/3503, Loss: 1.1522\n",
      "Batch 1500/3503, Loss: 0.9929\n",
      "Batch 1550/3503, Loss: 0.7417\n",
      "Batch 1600/3503, Loss: 0.8759\n",
      "Batch 1650/3503, Loss: 1.1983\n",
      "Batch 1700/3503, Loss: 0.8823\n",
      "Batch 1750/3503, Loss: 1.0995\n",
      "Batch 1800/3503, Loss: 1.1414\n",
      "Batch 1850/3503, Loss: 1.0932\n",
      "Batch 1900/3503, Loss: 0.8875\n",
      "Batch 1950/3503, Loss: 0.9875\n",
      "Batch 2000/3503, Loss: 1.0618\n",
      "Batch 2050/3503, Loss: 1.0287\n",
      "Batch 2100/3503, Loss: 0.8332\n",
      "Batch 2150/3503, Loss: 0.7027\n",
      "Batch 2200/3503, Loss: 0.9208\n",
      "Batch 2250/3503, Loss: 1.1058\n",
      "Batch 2300/3503, Loss: 1.1009\n",
      "Batch 2350/3503, Loss: 1.2489\n",
      "Batch 2400/3503, Loss: 0.7917\n",
      "Batch 2450/3503, Loss: 0.8914\n",
      "Batch 2500/3503, Loss: 0.6949\n",
      "Batch 2550/3503, Loss: 0.8916\n",
      "Batch 2600/3503, Loss: 0.8022\n",
      "Batch 2650/3503, Loss: 1.0011\n",
      "Batch 2700/3503, Loss: 0.9319\n",
      "Batch 2750/3503, Loss: 1.1655\n",
      "Batch 2800/3503, Loss: 1.0118\n",
      "Batch 2850/3503, Loss: 1.0115\n",
      "Batch 2900/3503, Loss: 0.8078\n",
      "Batch 2950/3503, Loss: 1.1883\n",
      "Batch 3000/3503, Loss: 0.8316\n",
      "Batch 3050/3503, Loss: 0.9544\n",
      "Batch 3100/3503, Loss: 1.2179\n",
      "Batch 3150/3503, Loss: 0.9601\n",
      "Batch 3200/3503, Loss: 0.8013\n",
      "Batch 3250/3503, Loss: 0.6699\n",
      "Batch 3300/3503, Loss: 0.9440\n",
      "Batch 3350/3503, Loss: 0.7594\n",
      "Batch 3400/3503, Loss: 0.6637\n",
      "Batch 3450/3503, Loss: 0.9323\n",
      "Batch 3500/3503, Loss: 1.0467\n",
      "\n",
      "✓ Train Loss: 0.9432\n",
      "✓ Validation Loss: 1.3415\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: بهمیتآنامرانا گرتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 93.58%\n",
      "✓ CER: 40.91%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: بهمیتآنامرانا گرت..ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: آیر مآل مخلیبشای ران شروبدهد کلاسب ه میخارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: بیستپندمینسالیرنروسیدان مبارک.ے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 14/58\n",
      "   Blank token ratio: 64.7%\n",
      "\n",
      "💾 Saved best model! (CER: 40.91%)\n",
      "\n",
      "============================================================\n",
      "Epoch 2/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.5855\n",
      "Batch 50/3503, Loss: 0.7142\n",
      "Batch 100/3503, Loss: 0.8036\n",
      "Batch 150/3503, Loss: 0.7908\n",
      "Batch 200/3503, Loss: 0.7455\n",
      "Batch 250/3503, Loss: 0.7205\n",
      "Batch 300/3503, Loss: 0.7096\n",
      "Batch 350/3503, Loss: 0.9716\n",
      "Batch 400/3503, Loss: 0.7304\n",
      "Batch 450/3503, Loss: 0.5983\n",
      "Batch 500/3503, Loss: 0.8543\n",
      "Batch 550/3503, Loss: 1.0832\n",
      "Batch 600/3503, Loss: 0.9408\n",
      "Batch 650/3503, Loss: 0.8042\n",
      "Batch 700/3503, Loss: 1.1477\n",
      "Batch 750/3503, Loss: 0.7378\n",
      "Batch 800/3503, Loss: 0.7590\n",
      "Batch 850/3503, Loss: 0.5561\n",
      "Batch 900/3503, Loss: 0.8363\n",
      "Batch 950/3503, Loss: 0.8682\n",
      "Batch 1000/3503, Loss: 0.9567\n",
      "Batch 1050/3503, Loss: 0.9024\n",
      "Batch 1100/3503, Loss: 0.9862\n",
      "Batch 1150/3503, Loss: 0.8246\n",
      "Batch 1200/3503, Loss: 1.1252\n",
      "Batch 1250/3503, Loss: 0.8537\n",
      "Batch 1300/3503, Loss: 0.9547\n",
      "Batch 1350/3503, Loss: 0.6886\n",
      "Batch 1400/3503, Loss: 0.9932\n",
      "Batch 1450/3503, Loss: 0.8806\n",
      "Batch 1500/3503, Loss: 0.7883\n",
      "Batch 1550/3503, Loss: 0.6394\n",
      "Batch 1600/3503, Loss: 0.8545\n",
      "Batch 1650/3503, Loss: 0.7780\n",
      "Batch 1700/3503, Loss: 1.0906\n",
      "Batch 1750/3503, Loss: 0.6239\n",
      "Batch 1800/3503, Loss: 0.7589\n",
      "Batch 1850/3503, Loss: 0.7855\n",
      "Batch 1900/3503, Loss: 0.6468\n",
      "Batch 1950/3503, Loss: 0.9899\n",
      "Batch 2000/3503, Loss: 0.7843\n",
      "Batch 2050/3503, Loss: 1.0061\n",
      "Batch 2100/3503, Loss: 0.8771\n",
      "Batch 2150/3503, Loss: 0.7783\n",
      "Batch 2200/3503, Loss: 0.9733\n",
      "Batch 2250/3503, Loss: 0.7140\n",
      "Batch 2300/3503, Loss: 0.6924\n",
      "Batch 2350/3503, Loss: 0.5095\n",
      "Batch 2400/3503, Loss: 0.5634\n",
      "Batch 2450/3503, Loss: 0.7568\n",
      "Batch 2500/3503, Loss: 0.6510\n",
      "Batch 2550/3503, Loss: 0.6981\n",
      "Batch 2600/3503, Loss: 0.6759\n",
      "Batch 2650/3503, Loss: 0.7731\n",
      "Batch 2700/3503, Loss: 0.8129\n",
      "Batch 2750/3503, Loss: 0.8130\n",
      "Batch 2800/3503, Loss: 0.7280\n",
      "Batch 2850/3503, Loss: 0.8603\n",
      "Batch 2900/3503, Loss: 0.6205\n",
      "Batch 2950/3503, Loss: 0.5888\n",
      "Batch 3000/3503, Loss: 0.5983\n",
      "Batch 3050/3503, Loss: 0.7880\n",
      "Batch 3100/3503, Loss: 0.9228\n",
      "Batch 3150/3503, Loss: 0.7476\n",
      "Batch 3200/3503, Loss: 0.5564\n",
      "Batch 3250/3503, Loss: 0.6695\n",
      "Batch 3300/3503, Loss: 0.7944\n",
      "Batch 3350/3503, Loss: 0.8226\n",
      "Batch 3400/3503, Loss: 0.8090\n",
      "Batch 3450/3503, Loss: 0.9827\n",
      "Batch 3500/3503, Loss: 0.8111\n",
      "\n",
      "✓ Train Loss: 0.8200\n",
      "✓ Validation Loss: 1.2865\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: آهمیتآ نامرانالد گرتیٌ\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 92.30%\n",
      "✓ CER: 38.73%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: آهمیتآ نامرانالد گرت.ےٌ\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: آگیر ملم خلی ب شای رانش روب دهد کلاسب هم میخوارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: بیستدوپندمینسالگرد اروسیدان مبارک.ٌ\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 16/58\n",
      "   Blank token ratio: 63.8%\n",
      "\n",
      "💾 Saved best model! (CER: 38.73%)\n",
      "\n",
      "============================================================\n",
      "Epoch 3/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.5727\n",
      "Batch 50/3503, Loss: 0.6124\n",
      "Batch 100/3503, Loss: 0.7962\n",
      "Batch 150/3503, Loss: 0.7446\n",
      "Batch 200/3503, Loss: 0.5993\n",
      "Batch 250/3503, Loss: 0.6127\n",
      "Batch 300/3503, Loss: 0.6442\n",
      "Batch 350/3503, Loss: 0.5626\n",
      "Batch 400/3503, Loss: 0.5246\n",
      "Batch 450/3503, Loss: 0.7306\n",
      "Batch 500/3503, Loss: 0.6226\n",
      "Batch 550/3503, Loss: 0.7752\n",
      "Batch 600/3503, Loss: 0.9104\n",
      "Batch 650/3503, Loss: 0.6520\n",
      "Batch 700/3503, Loss: 0.8018\n",
      "Batch 750/3503, Loss: 0.8602\n",
      "Batch 800/3503, Loss: 0.7243\n",
      "Batch 850/3503, Loss: 0.9077\n",
      "Batch 900/3503, Loss: 0.7176\n",
      "Batch 950/3503, Loss: 0.6088\n",
      "Batch 1000/3503, Loss: 0.5880\n",
      "Batch 1050/3503, Loss: 0.9541\n",
      "Batch 1100/3503, Loss: 0.4817\n",
      "Batch 1150/3503, Loss: 0.7315\n",
      "Batch 1200/3503, Loss: 0.6751\n",
      "Batch 1250/3503, Loss: 0.6383\n",
      "Batch 1300/3503, Loss: 0.7032\n",
      "Batch 1350/3503, Loss: 0.8062\n",
      "Batch 1400/3503, Loss: 0.8179\n",
      "Batch 1450/3503, Loss: 0.8187\n",
      "Batch 1500/3503, Loss: 0.3367\n",
      "Batch 1550/3503, Loss: 0.7941\n",
      "Batch 1600/3503, Loss: 0.6933\n",
      "Batch 1650/3503, Loss: 0.7364\n",
      "Batch 1700/3503, Loss: 0.6812\n",
      "Batch 1750/3503, Loss: 0.7503\n",
      "Batch 1800/3503, Loss: 0.6282\n",
      "Batch 1850/3503, Loss: 0.6945\n",
      "Batch 1900/3503, Loss: 0.8493\n",
      "Batch 1950/3503, Loss: 0.7017\n",
      "Batch 2000/3503, Loss: 0.6408\n",
      "Batch 2050/3503, Loss: 0.8487\n",
      "Batch 2100/3503, Loss: 0.9618\n",
      "Batch 2150/3503, Loss: 0.5276\n",
      "Batch 2200/3503, Loss: 0.7482\n",
      "Batch 2250/3503, Loss: 0.7923\n",
      "Batch 2300/3503, Loss: 0.8260\n",
      "Batch 2350/3503, Loss: 0.9582\n",
      "Batch 2400/3503, Loss: 0.8672\n",
      "Batch 2450/3503, Loss: 0.5613\n",
      "Batch 2500/3503, Loss: 0.7508\n",
      "Batch 2550/3503, Loss: 0.6717\n",
      "Batch 2600/3503, Loss: 0.6760\n",
      "Batch 2650/3503, Loss: 0.7656\n",
      "Batch 2700/3503, Loss: 0.7989\n",
      "Batch 2750/3503, Loss: 0.4649\n",
      "Batch 2800/3503, Loss: 0.7225\n",
      "Batch 2850/3503, Loss: 0.9235\n",
      "Batch 2900/3503, Loss: 0.9495\n",
      "Batch 2950/3503, Loss: 0.7136\n",
      "Batch 3000/3503, Loss: 1.0256\n",
      "Batch 3050/3503, Loss: 0.6900\n",
      "Batch 3100/3503, Loss: 0.7022\n",
      "Batch 3150/3503, Loss: 0.6198\n",
      "Batch 3200/3503, Loss: 0.4762\n",
      "Batch 3250/3503, Loss: 0.8906\n",
      "Batch 3300/3503, Loss: 0.5625\n",
      "Batch 3350/3503, Loss: 0.4543\n",
      "Batch 3400/3503, Loss: 0.6425\n",
      "Batch 3450/3503, Loss: 0.8112\n",
      "Batch 3500/3503, Loss: 0.5648\n",
      "\n",
      "✓ Train Loss: 0.7207\n",
      "✓ Validation Loss: 1.2664\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: بحمیت آ نامراناد گرتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 89.88%\n",
      "✓ CER: 37.04%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: بحمیت آ نامراناد گرت.ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: اگر مال م خلی ب شای رانشروب دهد کلاسب هم میخوارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: یستدوپنجمینسالرن عروسیتان مبارک.ے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 15/58\n",
      "   Blank token ratio: 64.6%\n",
      "\n",
      "💾 Saved best model! (CER: 37.04%)\n",
      "\n",
      "============================================================\n",
      "Epoch 4/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.7194\n",
      "Batch 50/3503, Loss: 0.6215\n",
      "Batch 100/3503, Loss: 0.4823\n",
      "Batch 150/3503, Loss: 0.4782\n",
      "Batch 200/3503, Loss: 0.6033\n",
      "Batch 250/3503, Loss: 0.6696\n",
      "Batch 300/3503, Loss: 0.6378\n",
      "Batch 350/3503, Loss: 0.8230\n",
      "Batch 400/3503, Loss: 0.6676\n",
      "Batch 450/3503, Loss: 0.5538\n",
      "Batch 500/3503, Loss: 0.6960\n",
      "Batch 550/3503, Loss: 0.7297\n",
      "Batch 600/3503, Loss: 0.7578\n",
      "Batch 650/3503, Loss: 0.7235\n",
      "Batch 700/3503, Loss: 0.5078\n",
      "Batch 750/3503, Loss: 0.4271\n",
      "Batch 800/3503, Loss: 0.6413\n",
      "Batch 850/3503, Loss: 0.5868\n",
      "Batch 900/3503, Loss: 0.4298\n",
      "Batch 950/3503, Loss: 0.7636\n",
      "Batch 1000/3503, Loss: 0.5979\n",
      "Batch 1050/3503, Loss: 0.5586\n",
      "Batch 1100/3503, Loss: 0.4848\n",
      "Batch 1150/3503, Loss: 0.7325\n",
      "Batch 1200/3503, Loss: 0.5805\n",
      "Batch 1250/3503, Loss: 0.6636\n",
      "Batch 1300/3503, Loss: 0.6305\n",
      "Batch 1350/3503, Loss: 0.7632\n",
      "Batch 1400/3503, Loss: 0.6384\n",
      "Batch 1450/3503, Loss: 0.8759\n",
      "Batch 1500/3503, Loss: 0.4586\n",
      "Batch 1550/3503, Loss: 0.4905\n",
      "Batch 1600/3503, Loss: 0.6746\n",
      "Batch 1650/3503, Loss: 0.9054\n",
      "Batch 1700/3503, Loss: 0.7525\n",
      "Batch 1750/3503, Loss: 0.6053\n",
      "Batch 1800/3503, Loss: 0.6764\n",
      "Batch 1850/3503, Loss: 0.8744\n",
      "Batch 1900/3503, Loss: 0.6499\n",
      "Batch 1950/3503, Loss: 0.7444\n",
      "Batch 2000/3503, Loss: 0.4995\n",
      "Batch 2050/3503, Loss: 0.5383\n",
      "Batch 2100/3503, Loss: 0.6648\n",
      "Batch 2150/3503, Loss: 0.6798\n",
      "Batch 2200/3503, Loss: 0.6715\n",
      "Batch 2250/3503, Loss: 0.8514\n",
      "Batch 2300/3503, Loss: 0.4985\n",
      "Batch 2350/3503, Loss: 0.6486\n",
      "Batch 2400/3503, Loss: 0.5564\n",
      "Batch 2450/3503, Loss: 0.4970\n",
      "Batch 2500/3503, Loss: 0.5923\n",
      "Batch 2550/3503, Loss: 0.6839\n",
      "Batch 2600/3503, Loss: 0.5852\n",
      "Batch 2650/3503, Loss: 0.4428\n",
      "Batch 2700/3503, Loss: 0.3635\n",
      "Batch 2750/3503, Loss: 0.4986\n",
      "Batch 2800/3503, Loss: 0.7576\n",
      "Batch 2850/3503, Loss: 0.6280\n",
      "Batch 2900/3503, Loss: 0.5009\n",
      "Batch 2950/3503, Loss: 0.4911\n",
      "Batch 3000/3503, Loss: 0.6066\n",
      "Batch 3050/3503, Loss: 0.5594\n",
      "Batch 3100/3503, Loss: 0.8646\n",
      "Batch 3150/3503, Loss: 0.5479\n",
      "Batch 3200/3503, Loss: 0.6105\n",
      "Batch 3250/3503, Loss: 0.5970\n",
      "Batch 3300/3503, Loss: 0.5535\n",
      "Batch 3350/3503, Loss: 0.7977\n",
      "Batch 3400/3503, Loss: 0.8035\n",
      "Batch 3450/3503, Loss: 0.6492\n",
      "Batch 3500/3503, Loss: 0.4337\n",
      "\n",
      "✓ Train Loss: 0.6367\n",
      "✓ Validation Loss: 1.2565\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: بهمیت آننامرانالیه گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 88.35%\n",
      "✓ CER: 36.56%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: بهمیت آننامرانالیه گرفت.ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: آگر معلم خلی ب شای راناشروب دهد کلاسب هم میخوارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: یسدوپنجمینساللرل عروسیتان مبارک!ٌے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 16/58\n",
      "   Blank token ratio: 63.5%\n",
      "\n",
      "💾 Saved best model! (CER: 36.56%)\n",
      "\n",
      "============================================================\n",
      "Epoch 5/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.7510\n",
      "Batch 50/3503, Loss: 0.7865\n",
      "Batch 100/3503, Loss: 0.5148\n",
      "Batch 150/3503, Loss: 0.5357\n",
      "Batch 200/3503, Loss: 0.5569\n",
      "Batch 250/3503, Loss: 0.5768\n",
      "Batch 300/3503, Loss: 0.3624\n",
      "Batch 350/3503, Loss: 0.5935\n",
      "Batch 400/3503, Loss: 0.4771\n",
      "Batch 450/3503, Loss: 0.6408\n",
      "Batch 500/3503, Loss: 0.5933\n",
      "Batch 550/3503, Loss: 0.5322\n",
      "Batch 600/3503, Loss: 0.5177\n",
      "Batch 650/3503, Loss: 0.4523\n",
      "Batch 700/3503, Loss: 0.3815\n",
      "Batch 750/3503, Loss: 0.6095\n",
      "Batch 800/3503, Loss: 0.4637\n",
      "Batch 850/3503, Loss: 0.5024\n",
      "Batch 900/3503, Loss: 0.5257\n",
      "Batch 950/3503, Loss: 0.6976\n",
      "Batch 1000/3503, Loss: 0.4703\n",
      "Batch 1050/3503, Loss: 0.6162\n",
      "Batch 1100/3503, Loss: 0.6734\n",
      "Batch 1150/3503, Loss: 0.3328\n",
      "Batch 1200/3503, Loss: 0.7113\n",
      "Batch 1250/3503, Loss: 0.3631\n",
      "Batch 1300/3503, Loss: 0.7735\n",
      "Batch 1350/3503, Loss: 0.6286\n",
      "Batch 1400/3503, Loss: 0.6374\n",
      "Batch 1450/3503, Loss: 0.5532\n",
      "Batch 1500/3503, Loss: 0.5365\n",
      "Batch 1550/3503, Loss: 0.2761\n",
      "Batch 1600/3503, Loss: 0.6317\n",
      "Batch 1650/3503, Loss: 0.5822\n",
      "Batch 1700/3503, Loss: 0.3570\n",
      "Batch 1750/3503, Loss: 0.4296\n",
      "Batch 1800/3503, Loss: 0.4843\n",
      "Batch 1850/3503, Loss: 0.6533\n",
      "Batch 1900/3503, Loss: 0.5826\n",
      "Batch 1950/3503, Loss: 0.6560\n",
      "Batch 2000/3503, Loss: 0.6910\n",
      "Batch 2050/3503, Loss: 0.3075\n",
      "Batch 2100/3503, Loss: 0.5302\n",
      "Batch 2150/3503, Loss: 0.7439\n",
      "Batch 2200/3503, Loss: 0.6501\n",
      "Batch 2250/3503, Loss: 0.5146\n",
      "Batch 2300/3503, Loss: 0.5239\n",
      "Batch 2350/3503, Loss: 0.5681\n",
      "Batch 2400/3503, Loss: 0.5752\n",
      "Batch 2450/3503, Loss: 0.6878\n",
      "Batch 2500/3503, Loss: 0.7092\n",
      "Batch 2550/3503, Loss: 0.4552\n",
      "Batch 2600/3503, Loss: 0.7196\n",
      "Batch 2650/3503, Loss: 0.3946\n",
      "Batch 2700/3503, Loss: 0.5072\n",
      "Batch 2750/3503, Loss: 0.6890\n",
      "Batch 2800/3503, Loss: 0.6782\n",
      "Batch 2850/3503, Loss: 0.6944\n",
      "Batch 2900/3503, Loss: 0.5490\n",
      "Batch 2950/3503, Loss: 0.3633\n",
      "Batch 3000/3503, Loss: 0.7669\n",
      "Batch 3050/3503, Loss: 0.4146\n",
      "Batch 3100/3503, Loss: 0.6027\n",
      "Batch 3150/3503, Loss: 0.6008\n",
      "Batch 3200/3503, Loss: 0.6785\n",
      "Batch 3250/3503, Loss: 0.3963\n",
      "Batch 3300/3503, Loss: 0.6639\n",
      "Batch 3350/3503, Loss: 0.6186\n",
      "Batch 3400/3503, Loss: 0.5142\n",
      "Batch 3450/3503, Loss: 0.5943\n",
      "Batch 3500/3503, Loss: 0.5752\n",
      "\n",
      "✓ Train Loss: 0.5586\n",
      "✓ Validation Loss: 1.2717\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: بهمیتآ نامرانادید گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 89.20%\n",
      "✓ CER: 35.79%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: بهمیتآ نامرانادید گرفت.؟ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: ایر مالو خیلی ب شای رانشرو بدهد کلاسب هم می خوارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: بیسدوپنجمینسالگرد عروسیدان مبارک؟ٌے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 17/58\n",
      "   Blank token ratio: 63.6%\n",
      "\n",
      "💾 Saved best model! (CER: 35.79%)\n",
      "\n",
      "============================================================\n",
      "Epoch 6/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.4039\n",
      "Batch 50/3503, Loss: 0.4480\n",
      "Batch 100/3503, Loss: 0.4863\n",
      "Batch 150/3503, Loss: 0.4843\n",
      "Batch 200/3503, Loss: 0.4125\n",
      "Batch 250/3503, Loss: 0.3693\n",
      "Batch 300/3503, Loss: 0.4975\n",
      "Batch 350/3503, Loss: 0.5376\n",
      "Batch 400/3503, Loss: 0.4534\n",
      "Batch 450/3503, Loss: 0.4385\n",
      "Batch 500/3503, Loss: 0.4477\n",
      "Batch 550/3503, Loss: 0.4982\n",
      "Batch 600/3503, Loss: 0.4080\n",
      "Batch 650/3503, Loss: 0.2989\n",
      "Batch 700/3503, Loss: 0.3341\n",
      "Batch 750/3503, Loss: 0.4783\n",
      "Batch 800/3503, Loss: 0.3822\n",
      "Batch 850/3503, Loss: 0.4756\n",
      "Batch 900/3503, Loss: 0.4750\n",
      "Batch 950/3503, Loss: 0.4958\n",
      "Batch 1000/3503, Loss: 0.5059\n",
      "Batch 1050/3503, Loss: 0.4586\n",
      "Batch 1100/3503, Loss: 0.5543\n",
      "Batch 1150/3503, Loss: 0.6819\n",
      "Batch 1200/3503, Loss: 0.4741\n",
      "Batch 1250/3503, Loss: 0.3314\n",
      "Batch 1300/3503, Loss: 0.3940\n",
      "Batch 1350/3503, Loss: 0.5192\n",
      "Batch 1400/3503, Loss: 0.4410\n",
      "Batch 1450/3503, Loss: 0.5739\n",
      "Batch 1500/3503, Loss: 0.3246\n",
      "Batch 1550/3503, Loss: 0.6638\n",
      "Batch 1600/3503, Loss: 0.4456\n",
      "Batch 1650/3503, Loss: 0.6236\n",
      "Batch 1700/3503, Loss: 0.5313\n",
      "Batch 1750/3503, Loss: 0.4104\n",
      "Batch 1800/3503, Loss: 0.4632\n",
      "Batch 1850/3503, Loss: 0.4787\n",
      "Batch 1900/3503, Loss: 0.5447\n",
      "Batch 1950/3503, Loss: 0.3988\n",
      "Batch 2000/3503, Loss: 0.5386\n",
      "Batch 2050/3503, Loss: 0.2869\n",
      "Batch 2100/3503, Loss: 0.3002\n",
      "Batch 2150/3503, Loss: 0.4611\n",
      "Batch 2200/3503, Loss: 0.2944\n",
      "Batch 2250/3503, Loss: 0.5191\n",
      "Batch 2300/3503, Loss: 0.5924\n",
      "Batch 2350/3503, Loss: 0.2900\n",
      "Batch 2400/3503, Loss: 0.3957\n",
      "Batch 2450/3503, Loss: 0.6801\n",
      "Batch 2500/3503, Loss: 0.5384\n",
      "Batch 2550/3503, Loss: 0.5748\n",
      "Batch 2600/3503, Loss: 0.7045\n",
      "Batch 2650/3503, Loss: 0.5971\n",
      "Batch 2700/3503, Loss: 0.5493\n",
      "Batch 2750/3503, Loss: 0.4236\n",
      "Batch 2800/3503, Loss: 0.3771\n",
      "Batch 2850/3503, Loss: 0.4983\n",
      "Batch 2900/3503, Loss: 0.3941\n",
      "Batch 2950/3503, Loss: 0.5272\n",
      "Batch 3000/3503, Loss: 0.4071\n",
      "Batch 3050/3503, Loss: 0.5295\n",
      "Batch 3100/3503, Loss: 0.4759\n",
      "Batch 3150/3503, Loss: 0.5982\n",
      "Batch 3200/3503, Loss: 0.4336\n",
      "Batch 3250/3503, Loss: 0.6060\n",
      "Batch 3300/3503, Loss: 0.5934\n",
      "Batch 3350/3503, Loss: 0.5230\n",
      "Batch 3400/3503, Loss: 0.8556\n",
      "Batch 3450/3503, Loss: 0.4600\n",
      "Batch 3500/3503, Loss: 0.4610\n",
      "\n",
      "✓ Train Loss: 0.4907\n",
      "✓ Validation Loss: 1.2966\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: بهمیت آنام رانادد گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 90.05%\n",
      "✓ CER: 35.55%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: بهمیت آنام رانادد گرفت..ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: آیر معل ب خیلی به شای رانش رو ب دهد کلاسب هم می خوارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: یستوپند مینساللگد عروسیدان مبارک!ٌے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 16/58\n",
      "   Blank token ratio: 63.8%\n",
      "\n",
      "💾 Saved best model! (CER: 35.55%)\n",
      "\n",
      "============================================================\n",
      "Epoch 7/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.3819\n",
      "Batch 50/3503, Loss: 0.4328\n",
      "Batch 100/3503, Loss: 0.3844\n",
      "Batch 150/3503, Loss: 0.2426\n",
      "Batch 200/3503, Loss: 0.3757\n",
      "Batch 250/3503, Loss: 0.4116\n",
      "Batch 300/3503, Loss: 0.3613\n",
      "Batch 350/3503, Loss: 0.4090\n",
      "Batch 400/3503, Loss: 0.3674\n",
      "Batch 450/3503, Loss: 0.3776\n",
      "Batch 500/3503, Loss: 0.5130\n",
      "Batch 550/3503, Loss: 0.2911\n",
      "Batch 600/3503, Loss: 0.3048\n",
      "Batch 650/3503, Loss: 0.3468\n",
      "Batch 700/3503, Loss: 0.3880\n",
      "Batch 750/3503, Loss: 0.5899\n",
      "Batch 800/3503, Loss: 0.3889\n",
      "Batch 850/3503, Loss: 0.3230\n",
      "Batch 900/3503, Loss: 0.2869\n",
      "Batch 950/3503, Loss: 0.3743\n",
      "Batch 1000/3503, Loss: 0.3206\n",
      "Batch 1050/3503, Loss: 0.3815\n",
      "Batch 1100/3503, Loss: 0.3977\n",
      "Batch 1150/3503, Loss: 0.6796\n",
      "Batch 1200/3503, Loss: 0.5725\n",
      "Batch 1250/3503, Loss: 0.5765\n",
      "Batch 1300/3503, Loss: 0.2429\n",
      "Batch 1350/3503, Loss: 0.3710\n",
      "Batch 1400/3503, Loss: 0.3754\n",
      "Batch 1450/3503, Loss: 0.4253\n",
      "Batch 1500/3503, Loss: 0.3853\n",
      "Batch 1550/3503, Loss: 0.4646\n",
      "Batch 1600/3503, Loss: 0.3676\n",
      "Batch 1650/3503, Loss: 0.4563\n",
      "Batch 1700/3503, Loss: 0.5944\n",
      "Batch 1750/3503, Loss: 0.5487\n",
      "Batch 1800/3503, Loss: 0.3005\n",
      "Batch 1850/3503, Loss: 0.3379\n",
      "Batch 1900/3503, Loss: 0.4193\n",
      "Batch 1950/3503, Loss: 0.4685\n",
      "Batch 2000/3503, Loss: 0.3861\n",
      "Batch 2050/3503, Loss: 0.4792\n",
      "Batch 2100/3503, Loss: 0.2798\n",
      "Batch 2150/3503, Loss: 0.3593\n",
      "Batch 2200/3503, Loss: 0.7056\n",
      "Batch 2250/3503, Loss: 0.3561\n",
      "Batch 2300/3503, Loss: 0.5341\n",
      "Batch 2350/3503, Loss: 0.5583\n",
      "Batch 2400/3503, Loss: 0.2848\n",
      "Batch 2450/3503, Loss: 0.4112\n",
      "Batch 2500/3503, Loss: 0.3982\n",
      "Batch 2550/3503, Loss: 0.3400\n",
      "Batch 2600/3503, Loss: 0.5190\n",
      "Batch 2650/3503, Loss: 0.4669\n",
      "Batch 2700/3503, Loss: 0.3865\n",
      "Batch 2750/3503, Loss: 0.5547\n",
      "Batch 2800/3503, Loss: 0.3814\n",
      "Batch 2850/3503, Loss: 0.1843\n",
      "Batch 2900/3503, Loss: 0.4515\n",
      "Batch 2950/3503, Loss: 0.3969\n",
      "Batch 3000/3503, Loss: 0.4705\n",
      "Batch 3050/3503, Loss: 0.3519\n",
      "Batch 3100/3503, Loss: 0.2852\n",
      "Batch 3150/3503, Loss: 0.5715\n",
      "Batch 3200/3503, Loss: 0.5076\n",
      "Batch 3250/3503, Loss: 0.5910\n",
      "Batch 3300/3503, Loss: 0.3725\n",
      "Batch 3350/3503, Loss: 0.4126\n",
      "Batch 3400/3503, Loss: 0.5514\n",
      "Batch 3450/3503, Loss: 0.3658\n",
      "Batch 3500/3503, Loss: 0.4062\n",
      "\n",
      "✓ Train Loss: 0.4293\n",
      "✓ Validation Loss: 1.3263\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: احمیت آ نامرانادد گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 87.37%\n",
      "✓ CER: 35.80%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: احمیت آ نامرانادد گرفت.ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: اگر مالو خیلی به شاگ رانشرو بدهد کلاسب هم می خواند.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: بیسدوپندمینساللگرد عرو صیدان مبارک!ے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 15/58\n",
      "   Blank token ratio: 63.3%\n",
      "\n",
      "============================================================\n",
      "Epoch 8/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.3056\n",
      "Batch 50/3503, Loss: 0.3112\n",
      "Batch 100/3503, Loss: 0.3301\n",
      "Batch 150/3503, Loss: 0.4346\n",
      "Batch 200/3503, Loss: 0.3703\n",
      "Batch 250/3503, Loss: 0.4342\n",
      "Batch 300/3503, Loss: 0.4231\n",
      "Batch 350/3503, Loss: 0.4757\n",
      "Batch 400/3503, Loss: 0.3170\n",
      "Batch 450/3503, Loss: 0.4486\n",
      "Batch 500/3503, Loss: 0.3917\n",
      "Batch 550/3503, Loss: 0.3234\n",
      "Batch 600/3503, Loss: 0.2620\n",
      "Batch 650/3503, Loss: 0.2286\n",
      "Batch 700/3503, Loss: 0.6115\n",
      "Batch 750/3503, Loss: 0.4105\n",
      "Batch 800/3503, Loss: 0.3871\n",
      "Batch 850/3503, Loss: 0.2781\n",
      "Batch 900/3503, Loss: 0.2894\n",
      "Batch 950/3503, Loss: 0.5768\n",
      "Batch 1000/3503, Loss: 0.2802\n",
      "Batch 1050/3503, Loss: 0.3427\n",
      "Batch 1100/3503, Loss: 0.3613\n",
      "Batch 1150/3503, Loss: 0.3749\n",
      "Batch 1200/3503, Loss: 0.2839\n",
      "Batch 1250/3503, Loss: 0.4915\n",
      "Batch 1300/3503, Loss: 0.2170\n",
      "Batch 1350/3503, Loss: 0.3174\n",
      "Batch 1400/3503, Loss: 0.4320\n",
      "Batch 1450/3503, Loss: 0.3052\n",
      "Batch 1500/3503, Loss: 0.3142\n",
      "Batch 1550/3503, Loss: 0.3225\n",
      "Batch 1600/3503, Loss: 0.4552\n",
      "Batch 1650/3503, Loss: 0.2960\n",
      "Batch 1700/3503, Loss: 0.3742\n",
      "Batch 1750/3503, Loss: 0.2772\n",
      "Batch 1800/3503, Loss: 0.3633\n",
      "Batch 1850/3503, Loss: 0.4652\n",
      "Batch 1900/3503, Loss: 0.4369\n",
      "Batch 1950/3503, Loss: 0.3533\n",
      "Batch 2000/3503, Loss: 0.3520\n",
      "Batch 2050/3503, Loss: 0.4497\n",
      "Batch 2100/3503, Loss: 0.2916\n",
      "Batch 2150/3503, Loss: 0.3376\n",
      "Batch 2200/3503, Loss: 0.3148\n",
      "Batch 2250/3503, Loss: 0.2998\n",
      "Batch 2300/3503, Loss: 0.4285\n",
      "Batch 2350/3503, Loss: 0.4567\n",
      "Batch 2400/3503, Loss: 0.4916\n",
      "Batch 2450/3503, Loss: 0.3298\n",
      "Batch 2500/3503, Loss: 0.3495\n",
      "Batch 2550/3503, Loss: 0.3505\n",
      "Batch 2600/3503, Loss: 0.4014\n",
      "Batch 2650/3503, Loss: 0.4814\n",
      "Batch 2700/3503, Loss: 0.4121\n",
      "Batch 2750/3503, Loss: 0.4529\n",
      "Batch 2800/3503, Loss: 0.4333\n",
      "Batch 2850/3503, Loss: 0.2626\n",
      "Batch 2900/3503, Loss: 0.4999\n",
      "Batch 2950/3503, Loss: 0.4548\n",
      "Batch 3000/3503, Loss: 0.4497\n",
      "Batch 3050/3503, Loss: 0.3038\n",
      "Batch 3100/3503, Loss: 0.2820\n",
      "Batch 3150/3503, Loss: 0.2575\n",
      "Batch 3200/3503, Loss: 0.4680\n",
      "Batch 3250/3503, Loss: 0.3233\n",
      "Batch 3300/3503, Loss: 0.3746\n",
      "Batch 3350/3503, Loss: 0.2486\n",
      "Batch 3400/3503, Loss: 0.5104\n",
      "Batch 3450/3503, Loss: 0.7871\n",
      "Batch 3500/3503, Loss: 0.3564\n",
      "\n",
      "✓ Train Loss: 0.3754\n",
      "✓ Validation Loss: 1.3651\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: اهمیات آ نامرانادی ن گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 87.80%\n",
      "✓ CER: 35.38%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: اهمیات آ نامرانادی ن گرفت.ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: آیر مالو خیلی بشای راناش رو ب دهد کلاسب هم می خوارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: بیست وپنجمینسالگرد عروزیدان مبارکَے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 15/58\n",
      "   Blank token ratio: 61.9%\n",
      "\n",
      "💾 Saved best model! (CER: 35.38%)\n",
      "\n",
      "============================================================\n",
      "Epoch 9/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.3053\n",
      "Batch 50/3503, Loss: 0.2944\n",
      "Batch 100/3503, Loss: 0.3601\n",
      "Batch 150/3503, Loss: 0.3362\n",
      "Batch 200/3503, Loss: 0.1719\n",
      "Batch 250/3503, Loss: 0.1690\n",
      "Batch 300/3503, Loss: 0.4978\n",
      "Batch 350/3503, Loss: 0.2860\n",
      "Batch 400/3503, Loss: 0.3242\n",
      "Batch 450/3503, Loss: 0.2544\n",
      "Batch 500/3503, Loss: 0.3741\n",
      "Batch 550/3503, Loss: 0.2892\n",
      "Batch 600/3503, Loss: 0.3760\n",
      "Batch 650/3503, Loss: 0.3775\n",
      "Batch 700/3503, Loss: 0.3945\n",
      "Batch 750/3503, Loss: 0.2421\n",
      "Batch 800/3503, Loss: 0.1764\n",
      "Batch 850/3503, Loss: 0.4429\n",
      "Batch 900/3503, Loss: 0.4243\n",
      "Batch 950/3503, Loss: 0.2932\n",
      "Batch 1000/3503, Loss: 0.1752\n",
      "Batch 1050/3503, Loss: 0.2853\n",
      "Batch 1100/3503, Loss: 0.3279\n",
      "Batch 1150/3503, Loss: 0.2750\n",
      "Batch 1200/3503, Loss: 0.2922\n",
      "Batch 1250/3503, Loss: 0.4750\n",
      "Batch 1300/3503, Loss: 0.2843\n",
      "Batch 1350/3503, Loss: 0.3918\n",
      "Batch 1400/3503, Loss: 0.4023\n",
      "Batch 1450/3503, Loss: 0.3834\n",
      "Batch 1500/3503, Loss: 0.4828\n",
      "Batch 1550/3503, Loss: 0.4062\n",
      "Batch 1600/3503, Loss: 0.3648\n",
      "Batch 1650/3503, Loss: 0.4146\n",
      "Batch 1700/3503, Loss: 0.2655\n",
      "Batch 1750/3503, Loss: 0.3713\n",
      "Batch 1800/3503, Loss: 0.4299\n",
      "Batch 1850/3503, Loss: 0.3091\n",
      "Batch 1900/3503, Loss: 0.3511\n",
      "Batch 1950/3503, Loss: 0.2960\n",
      "Batch 2000/3503, Loss: 0.6807\n",
      "Batch 2050/3503, Loss: 0.2437\n",
      "Batch 2100/3503, Loss: 0.4419\n",
      "Batch 2150/3503, Loss: 0.4673\n",
      "Batch 2200/3503, Loss: 0.4084\n",
      "Batch 2250/3503, Loss: 0.2149\n",
      "Batch 2300/3503, Loss: 0.6013\n",
      "Batch 2350/3503, Loss: 0.4700\n",
      "Batch 2400/3503, Loss: 0.4426\n",
      "Batch 2450/3503, Loss: 0.3181\n",
      "Batch 2500/3503, Loss: 0.2365\n",
      "Batch 2550/3503, Loss: 0.1637\n",
      "Batch 2600/3503, Loss: 0.4746\n",
      "Batch 2650/3503, Loss: 0.3041\n",
      "Batch 2700/3503, Loss: 0.2815\n",
      "Batch 2750/3503, Loss: 0.3516\n",
      "Batch 2800/3503, Loss: 0.2974\n",
      "Batch 2850/3503, Loss: 0.5445\n",
      "Batch 2900/3503, Loss: 0.2166\n",
      "Batch 2950/3503, Loss: 0.4137\n",
      "Batch 3000/3503, Loss: 0.2806\n",
      "Batch 3050/3503, Loss: 0.5314\n",
      "Batch 3100/3503, Loss: 0.3195\n",
      "Batch 3150/3503, Loss: 0.2335\n",
      "Batch 3200/3503, Loss: 0.2337\n",
      "Batch 3250/3503, Loss: 0.2771\n",
      "Batch 3300/3503, Loss: 0.2538\n",
      "Batch 3350/3503, Loss: 0.2540\n",
      "Batch 3400/3503, Loss: 0.3189\n",
      "Batch 3450/3503, Loss: 0.2428\n",
      "Batch 3500/3503, Loss: 0.2367\n",
      "\n",
      "✓ Train Loss: 0.3280\n",
      "✓ Validation Loss: 1.4332\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: بحمیت آنامرانادید گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 86.52%\n",
      "✓ CER: 35.53%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: بحمیت آنامرانادید گرفت.ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: آگر مححل وخیلی به شاگیدانش رو ب دهدکلاسب هم مهی خوارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: بیست وپنجامینساللگر د عروصیدان مبارکَے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 16/58\n",
      "   Blank token ratio: 62.4%\n",
      "\n",
      "============================================================\n",
      "Epoch 10/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.4906\n",
      "Batch 50/3503, Loss: 0.3280\n",
      "Batch 100/3503, Loss: 0.3225\n",
      "Batch 150/3503, Loss: 0.1655\n",
      "Batch 200/3503, Loss: 0.2629\n",
      "Batch 250/3503, Loss: 0.2284\n",
      "Batch 300/3503, Loss: 0.3981\n",
      "Batch 350/3503, Loss: 0.1713\n",
      "Batch 400/3503, Loss: 0.1259\n",
      "Batch 450/3503, Loss: 0.1923\n",
      "Batch 500/3503, Loss: 0.3054\n",
      "Batch 550/3503, Loss: 0.3732\n",
      "Batch 600/3503, Loss: 0.2485\n",
      "Batch 650/3503, Loss: 0.1687\n",
      "Batch 700/3503, Loss: 0.2521\n",
      "Batch 750/3503, Loss: 0.1996\n",
      "Batch 800/3503, Loss: 0.2580\n",
      "Batch 850/3503, Loss: 0.1717\n",
      "Batch 900/3503, Loss: 0.3600\n",
      "Batch 950/3503, Loss: 0.2917\n",
      "Batch 1000/3503, Loss: 0.2957\n",
      "Batch 1050/3503, Loss: 0.2865\n",
      "Batch 1100/3503, Loss: 0.2019\n",
      "Batch 1150/3503, Loss: 0.1949\n",
      "Batch 1200/3503, Loss: 0.2213\n",
      "Batch 1250/3503, Loss: 0.2806\n",
      "Batch 1300/3503, Loss: 0.2785\n",
      "Batch 1350/3503, Loss: 0.2146\n",
      "Batch 1400/3503, Loss: 0.1821\n",
      "Batch 1450/3503, Loss: 0.2795\n",
      "Batch 1500/3503, Loss: 0.2852\n",
      "Batch 1550/3503, Loss: 0.4096\n",
      "Batch 1600/3503, Loss: 0.1138\n",
      "Batch 1650/3503, Loss: 0.3750\n",
      "Batch 1700/3503, Loss: 0.3575\n",
      "Batch 1750/3503, Loss: 0.2229\n",
      "Batch 1800/3503, Loss: 0.2043\n",
      "Batch 1850/3503, Loss: 0.3398\n",
      "Batch 1900/3503, Loss: 0.2248\n",
      "Batch 1950/3503, Loss: 0.2092\n",
      "Batch 2000/3503, Loss: 0.2485\n",
      "Batch 2050/3503, Loss: 0.3580\n",
      "Batch 2100/3503, Loss: 0.3340\n",
      "Batch 2150/3503, Loss: 0.3535\n",
      "Batch 2200/3503, Loss: 0.2798\n",
      "Batch 2250/3503, Loss: 0.2435\n",
      "Batch 2300/3503, Loss: 0.2782\n",
      "Batch 2350/3503, Loss: 0.2918\n",
      "Batch 2400/3503, Loss: 0.2685\n",
      "Batch 2450/3503, Loss: 0.2538\n",
      "Batch 2500/3503, Loss: 0.2443\n",
      "Batch 2550/3503, Loss: 0.3052\n",
      "Batch 2600/3503, Loss: 0.2816\n",
      "Batch 2650/3503, Loss: 0.1907\n",
      "Batch 2700/3503, Loss: 0.2403\n",
      "Batch 2750/3503, Loss: 0.2774\n",
      "Batch 2800/3503, Loss: 0.1768\n",
      "Batch 2850/3503, Loss: 0.2223\n",
      "Batch 2900/3503, Loss: 0.1975\n",
      "Batch 2950/3503, Loss: 0.3453\n",
      "Batch 3000/3503, Loss: 0.2488\n",
      "Batch 3050/3503, Loss: 0.3764\n",
      "Batch 3100/3503, Loss: 0.2358\n",
      "Batch 3150/3503, Loss: 0.1529\n",
      "Batch 3200/3503, Loss: 0.2631\n",
      "Batch 3250/3503, Loss: 0.2171\n",
      "Batch 3300/3503, Loss: 0.1948\n",
      "Batch 3350/3503, Loss: 0.3261\n",
      "Batch 3400/3503, Loss: 0.5038\n",
      "Batch 3450/3503, Loss: 0.2394\n",
      "Batch 3500/3503, Loss: 0.3437\n",
      "\n",
      "✓ Train Loss: 0.2874\n",
      "✓ Validation Loss: 1.4762\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: احمیت آنامرانادیه گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 86.14%\n",
      "✓ CER: 34.97%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: احمیت آنامرانادیه گرفت.ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: آگر ماحل م خیلی به شاگرانش رو ب دهد کلاسبه هم میخوارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: بیستوپن جامینسالگرد عرو سیدان مبارک!ٌے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 16/58\n",
      "   Blank token ratio: 63.0%\n",
      "\n",
      "💾 Saved best model! (CER: 34.97%)\n",
      "\n",
      "============================================================\n",
      "Epoch 11/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.3028\n",
      "Batch 50/3503, Loss: 0.1055\n",
      "Batch 100/3503, Loss: 0.2586\n",
      "Batch 150/3503, Loss: 0.2270\n",
      "Batch 200/3503, Loss: 0.1232\n",
      "Batch 250/3503, Loss: 0.2041\n",
      "Batch 300/3503, Loss: 0.2325\n",
      "Batch 350/3503, Loss: 0.1643\n",
      "Batch 400/3503, Loss: 0.0957\n",
      "Batch 450/3503, Loss: 0.2166\n",
      "Batch 500/3503, Loss: 0.2160\n",
      "Batch 550/3503, Loss: 0.1953\n",
      "Batch 600/3503, Loss: 0.1668\n",
      "Batch 650/3503, Loss: 0.1460\n",
      "Batch 700/3503, Loss: 0.3771\n",
      "Batch 750/3503, Loss: 0.1809\n",
      "Batch 800/3503, Loss: 0.1710\n",
      "Batch 850/3503, Loss: 0.1944\n",
      "Batch 900/3503, Loss: 0.1957\n",
      "Batch 950/3503, Loss: 0.2614\n",
      "Batch 1000/3503, Loss: 0.3489\n",
      "Batch 1050/3503, Loss: 0.3524\n",
      "Batch 1100/3503, Loss: 0.2387\n",
      "Batch 1150/3503, Loss: 0.2801\n",
      "Batch 1200/3503, Loss: 0.3055\n",
      "Batch 1250/3503, Loss: 0.2643\n",
      "Batch 1300/3503, Loss: 0.2611\n",
      "Batch 1350/3503, Loss: 0.3633\n",
      "Batch 1400/3503, Loss: 0.2402\n",
      "Batch 1450/3503, Loss: 0.2405\n",
      "Batch 1500/3503, Loss: 0.1779\n",
      "Batch 1550/3503, Loss: 0.2920\n",
      "Batch 1600/3503, Loss: 0.1738\n",
      "Batch 1650/3503, Loss: 0.2577\n",
      "Batch 1700/3503, Loss: 0.2935\n",
      "Batch 1750/3503, Loss: 0.2752\n",
      "Batch 1800/3503, Loss: 0.2250\n",
      "Batch 1850/3503, Loss: 0.1784\n",
      "Batch 1900/3503, Loss: 0.2147\n",
      "Batch 1950/3503, Loss: 0.2831\n",
      "Batch 2000/3503, Loss: 0.3616\n",
      "Batch 2050/3503, Loss: 0.2099\n",
      "Batch 2100/3503, Loss: 0.2892\n",
      "Batch 2150/3503, Loss: 0.4493\n",
      "Batch 2200/3503, Loss: 0.2505\n",
      "Batch 2250/3503, Loss: 0.2549\n",
      "Batch 2300/3503, Loss: 0.2201\n",
      "Batch 2350/3503, Loss: 0.1907\n",
      "Batch 2400/3503, Loss: 0.1703\n",
      "Batch 2450/3503, Loss: 0.3668\n",
      "Batch 2500/3503, Loss: 0.3548\n",
      "Batch 2550/3503, Loss: 0.1872\n",
      "Batch 2600/3503, Loss: 0.2102\n",
      "Batch 2650/3503, Loss: 0.1566\n",
      "Batch 2700/3503, Loss: 0.4052\n",
      "Batch 2750/3503, Loss: 0.1143\n",
      "Batch 2800/3503, Loss: 0.3357\n",
      "Batch 2850/3503, Loss: 0.1951\n",
      "Batch 2900/3503, Loss: 0.1961\n",
      "Batch 2950/3503, Loss: 0.1673\n",
      "Batch 3000/3503, Loss: 0.2645\n",
      "Batch 3050/3503, Loss: 0.3230\n",
      "Batch 3100/3503, Loss: 0.1160\n",
      "Batch 3150/3503, Loss: 0.2456\n",
      "Batch 3200/3503, Loss: 0.3254\n",
      "Batch 3250/3503, Loss: 0.2667\n",
      "Batch 3300/3503, Loss: 0.3182\n",
      "Batch 3350/3503, Loss: 0.2176\n",
      "Batch 3400/3503, Loss: 0.1935\n",
      "Batch 3450/3503, Loss: 0.1332\n",
      "Batch 3500/3503, Loss: 0.1952\n",
      "\n",
      "✓ Train Loss: 0.2525\n",
      "✓ Validation Loss: 1.5372\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: احمیت آنام را نالید گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 86.69%\n",
      "✓ CER: 34.49%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: احمیت آنام را نالید گرفت.ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: اگر ماحل م خیلی ب شایرانش رو ب دهد کلاس ب هم می خوارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: بیست وپنجا مینسالگرد عروصیدان مبارک!ے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 16/58\n",
      "   Blank token ratio: 62.2%\n",
      "\n",
      "💾 Saved best model! (CER: 34.49%)\n",
      "\n",
      "============================================================\n",
      "Epoch 12/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.1934\n",
      "Batch 50/3503, Loss: 0.2707\n",
      "Batch 100/3503, Loss: 0.2582\n",
      "Batch 150/3503, Loss: 0.1520\n",
      "Batch 200/3503, Loss: 0.2720\n",
      "Batch 250/3503, Loss: 0.1945\n",
      "Batch 300/3503, Loss: 0.1780\n",
      "Batch 350/3503, Loss: 0.1935\n",
      "Batch 400/3503, Loss: 0.1056\n",
      "Batch 450/3503, Loss: 0.1779\n",
      "Batch 500/3503, Loss: 0.2167\n",
      "Batch 550/3503, Loss: 0.2124\n",
      "Batch 600/3503, Loss: 0.2704\n",
      "Batch 650/3503, Loss: 0.1577\n",
      "Batch 700/3503, Loss: 0.1321\n",
      "Batch 750/3503, Loss: 0.2930\n",
      "Batch 800/3503, Loss: 0.1557\n",
      "Batch 850/3503, Loss: 0.1995\n",
      "Batch 900/3503, Loss: 0.1934\n",
      "Batch 950/3503, Loss: 0.1625\n",
      "Batch 1000/3503, Loss: 0.1831\n",
      "Batch 1050/3503, Loss: 0.2213\n",
      "Batch 1100/3503, Loss: 0.1477\n",
      "Batch 1150/3503, Loss: 0.1587\n",
      "Batch 1200/3503, Loss: 0.2097\n",
      "Batch 1250/3503, Loss: 0.1652\n",
      "Batch 1300/3503, Loss: 0.1696\n",
      "Batch 1350/3503, Loss: 0.2432\n",
      "Batch 1400/3503, Loss: 0.1922\n",
      "Batch 1450/3503, Loss: 0.1739\n",
      "Batch 1500/3503, Loss: 0.1617\n",
      "Batch 1550/3503, Loss: 0.1621\n",
      "Batch 1600/3503, Loss: 0.2296\n",
      "Batch 1650/3503, Loss: 0.3645\n",
      "Batch 1700/3503, Loss: 0.3480\n",
      "Batch 1750/3503, Loss: 0.2382\n",
      "Batch 1800/3503, Loss: 0.3514\n",
      "Batch 1850/3503, Loss: 0.2687\n",
      "Batch 1900/3503, Loss: 0.2786\n",
      "Batch 1950/3503, Loss: 0.2144\n",
      "Batch 2000/3503, Loss: 0.1670\n",
      "Batch 2050/3503, Loss: 0.2614\n",
      "Batch 2100/3503, Loss: 0.2068\n",
      "Batch 2150/3503, Loss: 0.1987\n",
      "Batch 2200/3503, Loss: 0.2242\n",
      "Batch 2250/3503, Loss: 0.3511\n",
      "Batch 2300/3503, Loss: 0.3179\n",
      "Batch 2350/3503, Loss: 0.4686\n",
      "Batch 2400/3503, Loss: 0.1944\n",
      "Batch 2450/3503, Loss: 0.1731\n",
      "Batch 2500/3503, Loss: 0.4273\n",
      "Batch 2550/3503, Loss: 0.3723\n",
      "Batch 2600/3503, Loss: 0.3108\n",
      "Batch 2650/3503, Loss: 0.1848\n",
      "Batch 2700/3503, Loss: 0.1149\n",
      "Batch 2750/3503, Loss: 0.3057\n",
      "Batch 2800/3503, Loss: 0.2712\n",
      "Batch 2850/3503, Loss: 0.2028\n",
      "Batch 2900/3503, Loss: 0.1495\n",
      "Batch 2950/3503, Loss: 0.2444\n",
      "Batch 3000/3503, Loss: 0.2330\n",
      "Batch 3050/3503, Loss: 0.2184\n",
      "Batch 3100/3503, Loss: 0.1918\n",
      "Batch 3150/3503, Loss: 0.1839\n",
      "Batch 3200/3503, Loss: 0.3001\n",
      "Batch 3250/3503, Loss: 0.1220\n",
      "Batch 3300/3503, Loss: 0.1318\n",
      "Batch 3350/3503, Loss: 0.1966\n",
      "Batch 3400/3503, Loss: 0.2618\n",
      "Batch 3450/3503, Loss: 0.1932\n",
      "Batch 3500/3503, Loss: 0.2054\n",
      "\n",
      "✓ Train Loss: 0.2211\n",
      "✓ Validation Loss: 1.6045\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: اهمیت آنام رانادیدر گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 86.69%\n",
      "✓ CER: 33.61%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: اهمیت آنام رانادیدر گرفت.ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: آگر ماحل م خیلی ب شای رانش رو ب دهد کلاسبه هم می خوارد.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: بیست وپنجا مینسالگرد عروصیدان مبارک!ے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 15/58\n",
      "   Blank token ratio: 61.8%\n",
      "\n",
      "💾 Saved best model! (CER: 33.61%)\n",
      "\n",
      "============================================================\n",
      "Epoch 13/13\n",
      "============================================================\n",
      "Batch 0/3503, Loss: 0.1854\n",
      "Batch 50/3503, Loss: 0.2224\n",
      "Batch 100/3503, Loss: 0.1385\n",
      "Batch 150/3503, Loss: 0.2511\n",
      "Batch 200/3503, Loss: 0.1291\n",
      "Batch 250/3503, Loss: 0.1533\n",
      "Batch 300/3503, Loss: 0.1541\n",
      "Batch 350/3503, Loss: 0.1176\n",
      "Batch 400/3503, Loss: 0.2035\n",
      "Batch 450/3503, Loss: 0.2344\n",
      "Batch 500/3503, Loss: 0.1796\n",
      "Batch 550/3503, Loss: 0.2585\n",
      "Batch 600/3503, Loss: 0.1630\n",
      "Batch 650/3503, Loss: 0.1621\n",
      "Batch 700/3503, Loss: 0.1951\n",
      "Batch 750/3503, Loss: 0.2001\n",
      "Batch 800/3503, Loss: 0.2081\n",
      "Batch 850/3503, Loss: 0.2538\n",
      "Batch 900/3503, Loss: 0.2044\n",
      "Batch 950/3503, Loss: 0.2739\n",
      "Batch 1000/3503, Loss: 0.3039\n",
      "Batch 1050/3503, Loss: 0.2071\n",
      "Batch 1100/3503, Loss: 0.1894\n",
      "Batch 1150/3503, Loss: 0.1771\n",
      "Batch 1200/3503, Loss: 0.1785\n",
      "Batch 1250/3503, Loss: 0.0981\n",
      "Batch 1300/3503, Loss: 0.1871\n",
      "Batch 1350/3503, Loss: 0.1802\n",
      "Batch 1400/3503, Loss: 0.3136\n",
      "Batch 1450/3503, Loss: 0.4615\n",
      "Batch 1500/3503, Loss: 0.1009\n",
      "Batch 1550/3503, Loss: 0.2745\n",
      "Batch 1600/3503, Loss: 0.1098\n",
      "Batch 1650/3503, Loss: 0.1375\n",
      "Batch 1700/3503, Loss: 0.2910\n",
      "Batch 1750/3503, Loss: 0.0916\n",
      "Batch 1800/3503, Loss: 0.1833\n",
      "Batch 1850/3503, Loss: 0.2469\n",
      "Batch 1900/3503, Loss: 0.1815\n",
      "Batch 1950/3503, Loss: 0.1617\n",
      "Batch 2000/3503, Loss: 0.1463\n",
      "Batch 2050/3503, Loss: 0.1420\n",
      "Batch 2100/3503, Loss: 0.2626\n",
      "Batch 2150/3503, Loss: 0.1665\n",
      "Batch 2200/3503, Loss: 0.1665\n",
      "Batch 2250/3503, Loss: 0.1486\n",
      "Batch 2300/3503, Loss: 0.1921\n",
      "Batch 2350/3503, Loss: 0.1673\n",
      "Batch 2400/3503, Loss: 0.1182\n",
      "Batch 2450/3503, Loss: 0.1157\n",
      "Batch 2500/3503, Loss: 0.3845\n",
      "Batch 2550/3503, Loss: 0.1470\n",
      "Batch 2600/3503, Loss: 0.1126\n",
      "Batch 2650/3503, Loss: 0.2096\n",
      "Batch 2700/3503, Loss: 0.1881\n",
      "Batch 2750/3503, Loss: 0.2575\n",
      "Batch 2800/3503, Loss: 0.2053\n",
      "Batch 2850/3503, Loss: 0.1573\n",
      "Batch 2900/3503, Loss: 0.1560\n",
      "Batch 2950/3503, Loss: 0.2881\n",
      "Batch 3000/3503, Loss: 0.2590\n",
      "Batch 3050/3503, Loss: 0.1599\n",
      "Batch 3100/3503, Loss: 0.2053\n",
      "Batch 3150/3503, Loss: 0.2237\n",
      "Batch 3200/3503, Loss: 0.1777\n",
      "Batch 3250/3503, Loss: 0.2136\n",
      "Batch 3300/3503, Loss: 0.2891\n",
      "Batch 3350/3503, Loss: 0.2150\n",
      "Batch 3400/3503, Loss: 0.3215\n",
      "Batch 3450/3503, Loss: 0.1719\n",
      "Batch 3500/3503, Loss: 0.1555\n",
      "\n",
      "✓ Train Loss: 0.1961\n",
      "✓ Validation Loss: 1.6356\n",
      "\n",
      "Computing WER and CER...\n",
      "   Evaluated 400 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: احمیت آنامرانادیهر گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "✓ WER: 87.71%\n",
      "✓ CER: 34.87%\n",
      "\n",
      "Sample predictions (3 examples):\n",
      "\n",
      "[1]\n",
      "Pred: احمیت آنامرانادیهر گرفت.ے\n",
      "True: اهمیت آن نامه را نادیده گرفت.\n",
      "\n",
      "[2]\n",
      "Pred: آگر محلم خیلی ب شاگرانش رو ب دهدکلاس ب هم می خواند.\n",
      "True: اگر معلم خیلی به شاگردانش رو بدهد کلاس بههم میخورد.\n",
      "\n",
      "[3]\n",
      "Pred: بیست وپنجا مینساللگر د عروسیدان مبارک!ے\n",
      "True: بیست و پنجمین سالگرد عروسی تان مبارک!\n",
      "\n",
      "📊 Diagnostics:\n",
      "   Unique tokens in prediction: 16/58\n",
      "   Blank token ratio: 62.4%\n",
      "\n",
      "============================================================\n",
      "TRAINING COMPLETE!\n",
      "Best CER: 33.61%\n",
      "============================================================\n",
      "\n",
      "Final evaluation on full validation set...\n",
      "   Evaluated 10439 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: احمیت آنامرانادیهر گرفتی\n",
      "   Ref:  اهمیت آن نامه را نادیده گرفت\n",
      "Final WER: 86.06%\n",
      "Final CER: 35.27%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# 11. TRAINING LOOP - WITH BETTER MONITORING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "num_epochs = 13  # Increased to 15 since breakthrough often happens around epoch 8-12\n",
    "best_val_loss = float('inf')\n",
    "best_cer = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print('='*60)\n",
    "\n",
    "    # Train\n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    print(f\"\\n✓ Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # Validate\n",
    "    val_loss = validate(model, val_loader, criterion, device)\n",
    "    print(f\"✓ Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Compute WER and CER on validation set (using subset for speed)\n",
    "    print(\"\\nComputing WER and CER...\")\n",
    "    metrics, preds, refs = evaluate_model(model, val_loader, device, idx_to_char, num_batches=50)\n",
    "\n",
    "    print(f\"✓ WER: {metrics['wer']*100:.2f}%\")\n",
    "    print(f\"✓ CER: {metrics['cer']*100:.2f}%\")\n",
    "\n",
    "    # Show sample predictions\n",
    "    print(f\"\\nSample predictions (3 examples):\")\n",
    "    for i in range(min(3, len(preds))):\n",
    "        print(f\"\\n[{i+1}]\")\n",
    "        print(f\"Pred: {preds[i][:100]}\")\n",
    "        print(f\"True: {refs[i][:100]}\")\n",
    "\n",
    "        # Check for English characters in predictions\n",
    "        english_in_pred = re.findall(r'[A-Za-z]', preds[i])\n",
    "        if english_in_pred:\n",
    "            print(f\"⚠️  WARNING: English chars found: {english_in_pred}\")\n",
    "\n",
    "    # Diagnostic: Check what model is outputting\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_batch = next(iter(val_loader))\n",
    "        test_features = test_batch[0][:1].to(device)\n",
    "        test_output = model(test_features)\n",
    "        prediction_indices = test_output.argmax(dim=-1).squeeze()\n",
    "\n",
    "        unique_tokens = torch.unique(prediction_indices).tolist()\n",
    "        blank_ratio = (prediction_indices == 0).float().mean().item()\n",
    "\n",
    "        print(f\"\\n📊 Diagnostics:\")\n",
    "        print(f\"   Unique tokens in prediction: {len(unique_tokens)}/{len(char_to_idx)}\")\n",
    "        print(f\"   Blank token ratio: {blank_ratio*100:.1f}%\")\n",
    "        if blank_ratio > 0.95:\n",
    "            print(f\"   ⚠️  Model mostly producing blanks - learning in progress\")\n",
    "        elif blank_ratio < 0.50:\n",
    "            print(f\"   ✓ Model producing diverse tokens - breakthrough imminent!\")\n",
    "\n",
    "    # Save best model based on CER\n",
    "    if metrics['cer'] < best_cer:\n",
    "        best_cer = metrics['cer']\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'wer': metrics['wer'],\n",
    "            'cer': metrics['cer'],\n",
    "            'char_to_idx': char_to_idx,\n",
    "            'idx_to_char': idx_to_char\n",
    "        }, 'best_model.pt')\n",
    "        print(f\"\\n💾 Saved best model! (CER: {metrics['cer']*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(f\"Best CER: {best_cer*100:.2f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Final evaluation on full validation set\n",
    "print(\"\\nFinal evaluation on full validation set...\")\n",
    "final_metrics, _, _ = evaluate_model(model, val_loader, device, idx_to_char)\n",
    "print(f\"Final WER: {final_metrics['wer']*100:.2f}%\")\n",
    "print(f\"Final CER: {final_metrics['cer']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Egk5V2Gbw9Xz",
    "outputId": "ca7f881c-3f0d-4bef-f25c-5ffb870b6afc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FINAL EVALUATION ON TEST SET\n",
      "============================================================\n",
      "\n",
      "Creating test dataset...\n",
      "Test batches: 1305\n",
      "\n",
      "Loading best model...\n",
      "Best model from epoch 11\n",
      "Validation CER: 33.61%\n",
      "\n",
      "Evaluating on test set...\n",
      "   Evaluated 10440 samples\n",
      "\n",
      "   Sample after cleaning:\n",
      "   Pred: جاودتو حس م سافریی\n",
      "   Ref:  اتوبوس مسافری\n",
      "\n",
      "============================================================\n",
      "TEST SET RESULTS\n",
      "============================================================\n",
      "✓ WER: 90.77%\n",
      "✓ CER: 38.28%\n",
      "✓ Samples evaluated: 10440\n",
      "\n",
      "============================================================\n",
      "SAMPLE PREDICTIONS FROM TEST SET\n",
      "============================================================\n",
      "\n",
      "[1]\n",
      "Pred: جاودتو حس م سافری.؟ے\n",
      "True: اتوبوس مسافری\n",
      "Pred (clean): جاودتو حس م سافریی\n",
      "True (clean): اتوبوس مسافری\n",
      "\n",
      "[2]\n",
      "Pred: ام اکاس مسبت دهید و حتبا رراخشید.ے\n",
      "True: انعکاس مثبت دهید و اعتبار ببخشید\n",
      "Pred (clean): ام اکاس مسبت دهید و حتبا رراخشیدی\n",
      "True (clean): انعکاس مثبت دهید و اعتبار ببخشید\n",
      "\n",
      "[3]\n",
      "Pred: خراده؟ے\n",
      "True: جنگ افزارهای ساده\n",
      "Pred (clean): خرادهی\n",
      "True (clean): جنگ افزارهای ساده\n",
      "\n",
      "[4]\n",
      "Pred: آیا حز رتعالی مرا س دازقید.؟ے\n",
      "True: آیا حضرت عالی مرا صدا زدید؟\n",
      "Pred (clean): آیا حز رتعالی مرا س دازقیدی\n",
      "True (clean): آیا حضرت عالی مرا صدا زدید\n",
      "\n",
      "[5]\n",
      "Pred: بی اچحرفت بزنم.ے\n",
      "True: باید باهاش حرف بزنم\n",
      "Pred (clean): بی اچحرفت بزنمی\n",
      "True (clean): باید باهاش حرف بزنم\n",
      "\n",
      "[6]\n",
      "Pred: اردیی لنفاوی من وریم کریدند.ے\n",
      "True: غدد لنفاوی من ورم کرده اند.\n",
      "Pred (clean): اردیی لنفاوی من وریم کریدندی\n",
      "True (clean): غدد لنفاوی من ورم کرده اند\n",
      "\n",
      "[7]\n",
      "Pred: توین دویایکیی دلی رز گیمی.ے\n",
      "True: تو این دنیا یکی دلیل زندگیمه\n",
      "Pred (clean): توین دویایکیی دلی رز گیمیی\n",
      "True (clean): تو این دنیا یکی دلیل زندگیمه\n",
      "\n",
      "[8]\n",
      "Pred: مخل بتو ملا ق دارام وعضت و میکوام کهی شما ه منو میپسندیی و ب م علا قدرین بی شراش نابشیم.\n",
      "True: من خیلی بهتون علاقه دارم و ازتون میخوام که اگه شما هم منو میپسندین و بهم علاقه دارین بیشتر اشنا بشیم\n",
      "Pred (clean): مخل بتو ملا ق دارام وعضت و میکوام کهی شما ه منو میپسندیی و ب م علا قدرین بی شراش نابشیم\n",
      "True (clean): من خیلی بهتون علاقه دارم و ازتون میخوام که اگه شما هم منو میپسندین و بهم علاقه دارین بیشتر اشنا بشیم\n",
      "\n",
      "[9]\n",
      "Pred: بو خیللی آمهدشودد.؟ے\n",
      "True: و خیلیها مادر شدند وقتی خودشان هنوز بچه بودند\n",
      "Pred (clean): بو خیللی آمهدشوددی\n",
      "True (clean): و خیلیها مادر شدند وقتی خودشان هنوز بچه بودند\n",
      "\n",
      "[10]\n",
      "Pred: مخاممچ دار بشم.ے\n",
      "True: می خوام بچه دار بشم\n",
      "Pred (clean): مخاممچ دار بشمی\n",
      "True (clean): می خوام بچه دار بشم\n",
      "\n",
      "============================================================\n",
      "EVALUATION COMPLETE!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# After training completes, evaluate on test set\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL EVALUATION ON TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create test dataset and loader\n",
    "print(\"\\nCreating test dataset...\")\n",
    "test_dataset = ASRDataset(ds['test'], char_to_idx)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "print(f\"Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Load best model\n",
    "print(\"\\nLoading best model...\")\n",
    "checkpoint = torch.load('best_model.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Best model from epoch {checkpoint['epoch']}\")\n",
    "print(f\"Validation CER: {checkpoint['cer']*100:.2f}%\")\n",
    "\n",
    "# Evaluate on full test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "test_metrics, test_preds, test_refs = evaluate_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    device,\n",
    "    idx_to_char,\n",
    "    num_batches=None  # Evaluate on full test set\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ WER: {test_metrics['wer']*100:.2f}%\")\n",
    "print(f\"✓ CER: {test_metrics['cer']*100:.2f}%\")\n",
    "print(f\"✓ Samples evaluated: {test_metrics['num_samples']}\")\n",
    "\n",
    "# Show some example predictions\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAMPLE PREDICTIONS FROM TEST SET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"\\n[{i+1}]\")\n",
    "    print(f\"Pred: {test_preds[i]}\")\n",
    "    print(f\"True: {test_refs[i]}\")\n",
    "    print(f\"Pred (clean): {clean_persian_text(test_preds[i])}\")\n",
    "    print(f\"True (clean): {clean_persian_text(test_refs[i])}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION COMPLETE!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
