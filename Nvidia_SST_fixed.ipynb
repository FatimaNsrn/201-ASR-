{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xb0oq0lMKPE8",
    "outputId": "d83b89f2-a7f8-47af-f938-e91262e11fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchcodec\n",
      "  Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Downloading torchcodec-0.9.1-cp312-cp312-manylinux_2_28_x86_64.whl (2.1 MB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchcodec\n",
      "Successfully installed torchcodec-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchcodec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "SA8psz3CKcKE",
    "outputId": "c033b0ed-f819-4168-f600-b915ff01ca98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: Cython in /usr/local/lib/python3.12/dist-packages (3.0.12)\n",
      "Collecting nemo_toolkit[asr]\n",
      "  Downloading nemo_toolkit-2.6.0-py3-none-any.whl.metadata (82 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec==2024.12.0 (from nemo_toolkit[asr])\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.24 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.36.0)\n",
      "Collecting numba-cuda>=0.20.0 (from numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr])\n",
      "  Downloading numba_cuda-0.22.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.7 kB)\n",
      "Collecting numexpr<2.14.0 (from nemo_toolkit[asr])\n",
      "  Downloading numexpr-2.13.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.0.2)\n",
      "Collecting onnx>=1.7.0 (from nemo_toolkit[asr])\n",
      "  Downloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: protobuf~=5.29.5 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (5.29.5)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.9.0.post0)\n",
      "Collecting ruamel.yaml (from nemo_toolkit[asr])\n",
      "  Downloading ruamel.yaml-0.18.16-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.6.1)\n",
      "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (75.2.0)\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.19.0)\n",
      "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.3)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.9.0+cu126)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (4.67.1)\n",
      "Collecting wget (from nemo_toolkit[asr])\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.0.1)\n",
      "Collecting braceexpand (from nemo_toolkit[asr])\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting ctc_segmentation==1.7.4 (from nemo_toolkit[asr])\n",
      "  Downloading ctc_segmentation-1.7.4.tar.gz (73 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: editdistance in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.8.1)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.8.1)\n",
      "Collecting jiwer<4.0.0,>=3.1.0 (from nemo_toolkit[asr])\n",
      "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting kaldi-python-io (from nemo_toolkit[asr])\n",
      "  Downloading kaldi-python-io-1.2.2.tar.gz (8.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting lhotse>=1.31.1 (from nemo_toolkit[asr])\n",
      "  Downloading lhotse-1.32.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: librosa>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.11.0)\n",
      "Collecting marshmallow (from nemo_toolkit[asr])\n",
      "  Downloading marshmallow-4.1.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Collecting optuna (from nemo_toolkit[asr])\n",
      "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (25.0)\n",
      "Collecting pyannote.core (from nemo_toolkit[asr])\n",
      "  Downloading pyannote_core-6.0.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting pyannote.metrics (from nemo_toolkit[asr])\n",
      "  Downloading pyannote_metrics-4.0.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.25.1)\n",
      "Collecting pyloudnorm (from nemo_toolkit[asr])\n",
      "  Downloading pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting resampy (from nemo_toolkit[asr])\n",
      "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (1.16.3)\n",
      "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.13.1)\n",
      "Collecting sox<=1.5.0 (from nemo_toolkit[asr])\n",
      "  Downloading sox-1.5.0.tar.gz (63 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting kaldialign<=0.9.1 (from nemo_toolkit[asr])\n",
      "  Downloading kaldialign-0.9.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting whisper_normalizer (from nemo_toolkit[asr])\n",
      "  Downloading whisper_normalizer-0.1.12-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting num2words (from nemo_toolkit[asr])\n",
      "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.22 (from nemo_toolkit[asr])\n",
      "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (4.0.0)\n",
      "Requirement already satisfied: inflect in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (7.5.0)\n",
      "Collecting mediapy==1.1.6 (from nemo_toolkit[asr])\n",
      "  Downloading mediapy-1.1.6-py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.2.2)\n",
      "Collecting sacremoses>=0.0.43 (from nemo_toolkit[asr])\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: sentencepiece<1.0.0 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.2.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (3.1.2)\n",
      "Collecting fiddle (from nemo_toolkit[asr])\n",
      "  Downloading fiddle-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting hydra-core<=1.3.2,>1.3 (from nemo_toolkit[asr])\n",
      "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting lightning<=2.4.0,>2.2.1 (from nemo_toolkit[asr])\n",
      "  Downloading lightning-2.4.0-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: omegaconf<=2.3 in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (2.3.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.18.0)\n",
      "Collecting torchmetrics>=0.11.0 (from nemo_toolkit[asr])\n",
      "  Downloading torchmetrics-1.8.2-py3-none-any.whl.metadata (22 kB)\n",
      "Collecting transformers~=4.53.0 (from nemo_toolkit[asr])\n",
      "  Downloading transformers-4.53.3-py3-none-any.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (from nemo_toolkit[asr]) (0.23.1)\n",
      "Collecting webdataset>=0.2.86 (from nemo_toolkit[asr])\n",
      "  Downloading webdataset-1.0.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nv_one_logger_core>=2.3.1 (from nemo_toolkit[asr])\n",
      "  Downloading nv_one_logger_core-2.3.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nv_one_logger_training_telemetry>=2.3.1 (from nemo_toolkit[asr])\n",
      "  Downloading nv_one_logger_training_telemetry-2.3.1-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting nv_one_logger_pytorch_lightning_integration>=2.3.1 (from nemo_toolkit[asr])\n",
      "  Downloading nv_one_logger_pytorch_lightning_integration-2.3.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: Cython in /usr/local/lib/python3.12/dist-packages (from ctc_segmentation==1.7.4->nemo_toolkit[asr]) (3.0.12)\n",
      "Requirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from mediapy==1.1.6->nemo_toolkit[asr]) (7.34.0)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapy==1.1.6->nemo_toolkit[asr]) (3.10.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from mediapy==1.1.6->nemo_toolkit[asr]) (11.3.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.24->nemo_toolkit[asr]) (3.20.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.24->nemo_toolkit[asr]) (6.0.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.24->nemo_toolkit[asr]) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.24->nemo_toolkit[asr]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.24->nemo_toolkit[asr]) (1.2.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core<=1.3.2,>1.3->nemo_toolkit[asr]) (4.9.3)\n",
      "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer<4.0.0,>=3.1.0->nemo_toolkit[asr]) (8.3.1)\n",
      "Collecting rapidfuzz>=3.9.7 (from jiwer<4.0.0,>=3.1.0->nemo_toolkit[asr])\n",
      "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.12/dist-packages (from lhotse>=1.31.1->nemo_toolkit[asr]) (3.1.0)\n",
      "Collecting cytoolz>=0.10.1 (from lhotse>=1.31.1->nemo_toolkit[asr])\n",
      "  Downloading cytoolz-1.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting intervaltree>=3.1.0 (from lhotse>=1.31.1->nemo_toolkit[asr])\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: tabulate>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from lhotse>=1.31.1->nemo_toolkit[asr]) (0.9.0)\n",
      "Collecting lilcom>=1.1.0 (from lhotse>=1.31.1->nemo_toolkit[asr])\n",
      "  Downloading lilcom-1.8.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->nemo_toolkit[asr]) (0.60.0)\n",
      "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->nemo_toolkit[asr]) (1.5.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->nemo_toolkit[asr]) (4.4.2)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->nemo_toolkit[asr]) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->nemo_toolkit[asr]) (1.0.0)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->nemo_toolkit[asr]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.12/dist-packages (from librosa>=0.10.1->nemo_toolkit[asr]) (1.1.2)\n",
      "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<=2.4.0,>2.2.1->nemo_toolkit[asr])\n",
      "  Downloading lightning_utilities-0.15.2-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting packaging (from nemo_toolkit[asr])\n",
      "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pytorch-lightning (from lightning<=2.4.0,>2.2.1->nemo_toolkit[asr])\n",
      "  Downloading pytorch_lightning-2.6.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: cuda-bindings<14.0.0,>=12.9.1 in /usr/local/lib/python3.12/dist-packages (from numba-cuda>=0.20.0->numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr]) (12.9.4)\n",
      "Requirement already satisfied: cuda-core<1.0.0,>=0.3.2 in /usr/local/lib/python3.12/dist-packages (from numba-cuda>=0.20.0->numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr]) (0.3.2)\n",
      "Collecting cuda-bindings<14.0.0,>=12.9.1 (from numba-cuda>=0.20.0->numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr])\n",
      "  Downloading cuda_bindings-13.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting cuda-python==13.* (from numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr])\n",
      "  Downloading cuda_python-13.1.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting nvidia-nvvm==13.* (from numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr])\n",
      "  Downloading nvidia_nvvm-13.1.80-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime==13.* (from numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr])\n",
      "  Downloading nvidia_cuda_runtime-13.1.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-nvrtc==13.* (from numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr])\n",
      "  Downloading nvidia_cuda_nvrtc-13.1.80-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink==13.* (from numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr])\n",
      "  Downloading nvidia_nvjitlink-13.1.80-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cccl==13.* (from numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr])\n",
      "  Downloading nvidia_cuda_cccl-13.1.78-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings<14.0.0,>=12.9.1->numba-cuda>=0.20.0->numba-cuda[cu13]>=0.20.0; platform_system != \"Darwin\"->nemo_toolkit[asr]) (1.3.3)\n",
      "Collecting StrEnum<0.5.0,>=0.4.0 (from nv_one_logger_core>=2.3.1->nemo_toolkit[asr])\n",
      "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: overrides<8.0.0,>=7.7.0 in /usr/local/lib/python3.12/dist-packages (from nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (7.7.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.10.6 in /usr/local/lib/python3.12/dist-packages (from nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (2.12.3)\n",
      "Requirement already satisfied: toml<0.11.0,>=0.10.2 in /usr/local/lib/python3.12/dist-packages (from nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (0.10.2)\n",
      "Collecting setuptools>=70.0.0 (from nemo_toolkit[asr])\n",
      "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.7.0->nemo_toolkit[asr]) (0.5.4)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacremoses>=0.0.43->nemo_toolkit[asr]) (2025.11.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->nemo_toolkit[asr]) (3.6.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile->nemo_toolkit[asr]) (2.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->nemo_toolkit[asr]) (3.5.0)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers~=4.53.0->nemo_toolkit[asr])\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers~=4.53.0->nemo_toolkit[asr]) (0.7.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets->nemo_toolkit[asr]) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets->nemo_toolkit[asr]) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets->nemo_toolkit[asr]) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets->nemo_toolkit[asr]) (0.70.16)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from fiddle->nemo_toolkit[asr]) (1.4.0)\n",
      "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from fiddle->nemo_toolkit[asr]) (0.21)\n",
      "Collecting libcst (from fiddle->nemo_toolkit[asr])\n",
      "  Downloading libcst-1.8.6-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.12/dist-packages (from inflect->nemo_toolkit[asr]) (10.8.0)\n",
      "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from inflect->nemo_toolkit[asr]) (4.4.4)\n",
      "Collecting docopt>=0.6.2 (from num2words->nemo_toolkit[asr])\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna->nemo_toolkit[asr]) (1.17.2)\n",
      "Collecting colorlog (from optuna->nemo_toolkit[asr])\n",
      "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna->nemo_toolkit[asr]) (2.0.45)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->nemo_toolkit[asr]) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->nemo_toolkit[asr]) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->nemo_toolkit[asr]) (1.17.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft->nemo_toolkit[asr]) (5.9.5)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft->nemo_toolkit[asr]) (1.12.0)\n",
      "INFO: pip is looking at multiple versions of pyannote-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyannote.core (from nemo_toolkit[asr])\n",
      "  Downloading pyannote_core-6.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.12/dist-packages (from pyannote.core->nemo_toolkit[asr]) (2.4.0)\n",
      "INFO: pip is looking at multiple versions of pyannote-metrics to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pyannote.metrics (from nemo_toolkit[asr])\n",
      "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pyannote.database>=4.0.1 (from pyannote.metrics->nemo_toolkit[asr])\n",
      "  Downloading pyannote_database-6.1.1-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from pyloudnorm->nemo_toolkit[asr]) (1.0.0)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->nemo_toolkit[asr])\n",
      "  Downloading ruamel_yaml_clib-0.2.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard->nemo_toolkit[asr]) (1.76.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard->nemo_toolkit[asr]) (3.10)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard->nemo_toolkit[asr]) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard->nemo_toolkit[asr]) (3.1.4)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->nemo_toolkit[asr]) (3.1.45)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb->nemo_toolkit[asr]) (4.5.1)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb->nemo_toolkit[asr]) (2.47.0)\n",
      "Collecting indic-numtowords (from whisper_normalizer->nemo_toolkit[asr])\n",
      "  Downloading indic_numtowords-1.1.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna->nemo_toolkit[asr]) (1.3.10)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile->nemo_toolkit[asr]) (2.23)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from cytoolz>=0.10.1->lhotse>=1.31.1->nemo_toolkit[asr]) (0.12.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (3.13.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[asr]) (4.0.12)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy==1.1.6->nemo_toolkit[asr]) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy==1.1.6->nemo_toolkit[asr]) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy==1.1.6->nemo_toolkit[asr]) (4.61.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy==1.1.6->nemo_toolkit[asr]) (1.4.9)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapy==1.1.6->nemo_toolkit[asr]) (3.2.5)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.51.0->librosa>=0.10.1->nemo_toolkit[asr]) (0.43.0)\n",
      "Collecting pandas (from nemo_toolkit[asr])\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pandas-2.3.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pandas-2.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hINFO: pip is still looking at multiple versions of pyannote-core to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading pandas-2.3.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading pandas-2.2.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyannote.database>=4.0.1 (from pyannote.metrics->nemo_toolkit[asr])\n",
      "  Downloading pyannote_database-6.1.0-py3-none-any.whl.metadata (30 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading pyannote_database-6.0.0-py3-none-any.whl.metadata (30 kB)\n",
      "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]) (0.20.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.10.6->nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.10.6->nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.10.6->nv_one_logger_core>=2.3.1->nemo_toolkit[asr]) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.24->nemo_toolkit[asr]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.24->nemo_toolkit[asr]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.24->nemo_toolkit[asr]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface_hub>=0.24->nemo_toolkit[asr]) (2025.11.12)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna->nemo_toolkit[asr]) (3.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->nemo_toolkit[asr]) (1.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard->nemo_toolkit[asr]) (3.0.3)\n",
      "Collecting jedi>=0.16 (from ipython->mediapy==1.1.6->nemo_toolkit[asr])\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (5.7.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (3.0.52)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (2.19.2)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython->mediapy==1.1.6->nemo_toolkit[asr]) (4.9.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning<=2.4.0,>2.2.1->nemo_toolkit[asr]) (1.22.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[asr]) (5.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->mediapy==1.1.6->nemo_toolkit[asr]) (0.2.14)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]) (13.9.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit[asr]) (0.1.2)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading mediapy-1.1.6-py3-none-any.whl (24 kB)\n",
      "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jiwer-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading kaldialign-0.9.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (91 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lhotse-1.32.1-py3-none-any.whl (893 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m893.1/893.1 kB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning-2.4.0-py3-none-any.whl (810 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m811.0/811.0 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numba_cuda-0.22.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cuda_bindings-13.1.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (16.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.1/16.1 MB\u001b[0m \u001b[31m131.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cuda_python-13.1.1-py3-none-any.whl (8.0 kB)\n",
      "Downloading nvidia_cuda_cccl-13.1.78-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc-13.1.80-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (46.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.6/46.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime-13.1.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink-13.1.80-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (40.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvvm-13.1.80-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (63.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numexpr-2.13.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (443 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.1/443.1 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nv_one_logger_core-2.3.1-py3-none-any.whl (63 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nv_one_logger_pytorch_lightning_integration-2.3.1-py3-none-any.whl (9.8 kB)\n",
      "Downloading nv_one_logger_training_telemetry-2.3.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.20.0-cp312-abi3-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (18.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading torchmetrics-1.8.2-py3-none-any.whl (983 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.53.3-py3-none-any.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading webdataset-1.0.2-py3-none-any.whl (74 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Downloading fiddle-0.3.0-py3-none-any.whl (419 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.8/419.8 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-4.1.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nemo_toolkit-2.6.0-py3-none-any.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.5/58.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
      "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruamel.yaml-0.18.16-py3-none-any.whl (119 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading whisper_normalizer-0.1.12-py3-none-any.whl (36 kB)\n",
      "Downloading cytoolz-1.1.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lightning_utilities-0.15.2-py3-none-any.whl (29 kB)\n",
      "Downloading lilcom-1.8.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.1/48.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ruamel_yaml_clib-0.2.15-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (788 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m788.2/788.2 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
      "Downloading indic_numtowords-1.1.0-py3-none-any.whl (71 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libcst-1.8.6-cp312-cp312-manylinux_2_28_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytorch_lightning-2.6.0-py3-none-any.whl (849 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m84.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: ctc_segmentation, sox, kaldi-python-io, wget, docopt, intervaltree\n",
      "  Building wheel for ctc_segmentation (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ctc_segmentation: filename=ctc_segmentation-1.7.4-cp312-cp312-linux_x86_64.whl size=166578 sha256=085169f90cbd4d7934751029c56ae8131f40064c8b0e8b9de566451181127fd4\n",
      "  Stored in directory: /root/.cache/pip/wheels/f2/f6/15/e25bbeafff87e3b13abe5f15ba3bd74846cd0752b26d31180e\n",
      "  Building wheel for sox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sox: filename=sox-1.5.0-py3-none-any.whl size=40036 sha256=f89b1cca2c53086a64b28137dbe73340bc4a4b62769b8f9940c4d29aada46203\n",
      "  Stored in directory: /root/.cache/pip/wheels/8c/c7/e7/baea1f7e79b9eb53addc81cc9b827424f4a7d8c9cc18c03659\n",
      "  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for kaldi-python-io: filename=kaldi_python_io-1.2.2-py3-none-any.whl size=8953 sha256=a8bb1cc4319d2d55be2f09724e1d834de18a275ee0920df21fa9dd327394b6bc\n",
      "  Stored in directory: /root/.cache/pip/wheels/8b/a9/7b/af1bff74047bf7dfde7040b8fbe968ebb2f68eeed71249a14c\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=6bc0f42816e974fdf4f121ebf7d77f4bdb5e5aa98996130b8cfa6f2bc98e0874\n",
      "  Stored in directory: /root/.cache/pip/wheels/01/46/3b/e29ffbe4ebe614ff224bad40fc6a5773a67a163251585a13a9\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=b61a7fee6aefcb05c92af13710cd5c5bee9446586c81a52446c8134eb18236ec\n",
      "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=def9fb397ffb3d96894c7a37a886c267d3a6f1a7a4f749d32207b346b1626821\n",
      "  Stored in directory: /root/.cache/pip/wheels/65/c3/c3/238bf93c243597857edd94ddb0577faa74a8e16e9585896e83\n",
      "Successfully built ctc_segmentation sox kaldi-python-io wget docopt intervaltree\n",
      "Installing collected packages: wget, StrEnum, kaldialign, docopt, braceexpand, setuptools, sacremoses, ruamel.yaml.clib, rapidfuzz, packaging, nvidia-nvvm, nvidia-nvjitlink, nvidia-cuda-runtime, nvidia-cuda-nvrtc, nvidia-cuda-cccl, numpy, num2words, marshmallow, libcst, jedi, intervaltree, indic-numtowords, fsspec, cytoolz, cuda-bindings, colorlog, whisper_normalizer, webdataset, sox, ruamel.yaml, numexpr, lilcom, lightning-utilities, kaldi-python-io, jiwer, hydra-core, fiddle, cuda-python, ctc_segmentation, tokenizers, resampy, pyloudnorm, pyannote.core, optuna, onnx, nv_one_logger_core, numba-cuda, transformers, torchmetrics, pyannote.database, nv_one_logger_training_telemetry, mediapy, lhotse, pytorch-lightning, pyannote.metrics, nemo_toolkit, lightning, nv_one_logger_pytorch_lightning_integration\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 75.2.0\n",
      "    Uninstalling setuptools-75.2.0:\n",
      "      Successfully uninstalled setuptools-75.2.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 25.0\n",
      "    Uninstalling packaging-25.0:\n",
      "      Successfully uninstalled packaging-25.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "  Attempting uninstall: cuda-bindings\n",
      "    Found existing installation: cuda-bindings 12.9.4\n",
      "    Uninstalling cuda-bindings-12.9.4:\n",
      "      Successfully uninstalled cuda-bindings-12.9.4\n",
      "  Attempting uninstall: numexpr\n",
      "    Found existing installation: numexpr 2.14.1\n",
      "    Uninstalling numexpr-2.14.1:\n",
      "      Successfully uninstalled numexpr-2.14.1\n",
      "  Attempting uninstall: cuda-python\n",
      "    Found existing installation: cuda-python 12.9.4\n",
      "    Uninstalling cuda-python-12.9.4:\n",
      "      Successfully uninstalled cuda-python-12.9.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.22.1\n",
      "    Uninstalling tokenizers-0.22.1:\n",
      "      Successfully uninstalled tokenizers-0.22.1\n",
      "  Attempting uninstall: numba-cuda\n",
      "    Found existing installation: numba-cuda 0.19.1\n",
      "    Uninstalling numba-cuda-0.19.1:\n",
      "      Successfully uninstalled numba-cuda-0.19.1\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.3\n",
      "    Uninstalling transformers-4.57.3:\n",
      "      Successfully uninstalled transformers-4.57.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "dask-cuda 25.10.0 requires numba-cuda<0.20.0a0,>=0.19.1, but you have numba-cuda 0.22.1 which is incompatible.\n",
      "cudf-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "cudf-cu12 25.10.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.22.1 which is incompatible.\n",
      "blosc2 3.12.2 requires numexpr>=2.14.1; platform_machine != \"wasm32\", but you have numexpr 2.13.1 which is incompatible.\n",
      "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
      "cuml-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "cuml-cu12 25.10.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.22.1 which is incompatible.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "pylibraft-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "rmm-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "ucxx-cu12 0.46.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.22.1 which is incompatible.\n",
      "pylibcudf-cu12 25.10.0 requires cuda-python<13.0a0,>=12.9.2, but you have cuda-python 13.1.1 which is incompatible.\n",
      "distributed-ucxx-cu12 0.46.0 requires numba-cuda[cu12]<0.20.0a0,>=0.19.1, but you have numba-cuda 0.22.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed StrEnum-0.4.15 braceexpand-0.1.7 colorlog-6.10.1 ctc_segmentation-1.7.4 cuda-bindings-13.1.1 cuda-python-13.1.1 cytoolz-1.1.0 docopt-0.6.2 fiddle-0.3.0 fsspec-2024.12.0 hydra-core-1.3.2 indic-numtowords-1.1.0 intervaltree-3.1.0 jedi-0.19.2 jiwer-3.1.0 kaldi-python-io-1.2.2 kaldialign-0.9.1 lhotse-1.32.1 libcst-1.8.6 lightning-2.4.0 lightning-utilities-0.15.2 lilcom-1.8.1 marshmallow-4.1.1 mediapy-1.1.6 nemo_toolkit-2.6.0 num2words-0.5.14 numba-cuda-0.22.1 numexpr-2.13.1 numpy-1.26.4 nv_one_logger_core-2.3.1 nv_one_logger_pytorch_lightning_integration-2.3.1 nv_one_logger_training_telemetry-2.3.1 nvidia-cuda-cccl-13.1.78 nvidia-cuda-nvrtc-13.1.80 nvidia-cuda-runtime-13.1.80 nvidia-nvjitlink-13.1.80 nvidia-nvvm-13.1.80 onnx-1.20.0 optuna-4.6.0 packaging-24.2 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pyloudnorm-0.1.1 pytorch-lightning-2.6.0 rapidfuzz-3.14.3 resampy-0.4.3 ruamel.yaml-0.18.16 ruamel.yaml.clib-0.2.15 sacremoses-0.1.1 setuptools-80.9.0 sox-1.5.0 tokenizers-0.21.4 torchmetrics-1.8.2 transformers-4.53.3 webdataset-1.0.2 wget-3.2 whisper_normalizer-0.1.12\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "481a2befb67f49f7b068f54c1912c8b0",
       "pip_warning": {
        "packages": [
         "_distutils_hack",
         "cuda",
         "numpy",
         "packaging"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install Cython\n",
    "!pip install nemo_toolkit[asr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hIRdEAPZKmFd",
    "outputId": "f8682fd8-cc89-439f-8db8-382458766d77"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-14 09:23:28 nemo_logging:405] Megatron num_microbatches_calculator not found, using Apex version.\n",
      "WARNING:nv_one_logger.api.config:OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.\n",
      "WARNING:nv_one_logger.training_telemetry.api.training_telemetry_provider:No exporters were provided. This means that no telemetry data will be collected.\n",
      "[NeMo W 2025-12-14 09:23:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:300: SyntaxWarning: invalid escape sequence '\\('\n",
      "      m = re.match('([su]([0-9]{1,2})p?) \\(([0-9]{1,2}) bit\\)$', token)\n",
      "    \n",
      "[NeMo W 2025-12-14 09:23:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:301: SyntaxWarning: invalid escape sequence '\\('\n",
      "      m2 = re.match('([su]([0-9]{1,2})p?)( \\(default\\))?$', token)\n",
      "    \n",
      "[NeMo W 2025-12-14 09:23:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:310: SyntaxWarning: invalid escape sequence '\\('\n",
      "      elif re.match('(flt)p?( \\(default\\))?$', token):\n",
      "    \n",
      "[NeMo W 2025-12-14 09:23:30 nemo_logging:405] /usr/local/lib/python3.12/dist-packages/pydub/utils.py:314: SyntaxWarning: invalid escape sequence '\\('\n",
      "      elif re.match('(dbl)p?( \\(default\\))?$', token):\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "from datasets import load_dataset\n",
    "import jiwer\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "548e3e83da224d28a9f9b704d06ea2ce",
      "51b8c9180c284380bdb871937aa259f4",
      "5638fda25e9d418793ce4332ecd983db",
      "32a4e805ae184ef7b4e4e86ca0386969",
      "e9404c8f71a3448da321f1843e22f627",
      "2827dfb93d1a4317a7ed4f195a7ff117",
      "ba3b7d82285c467bb824621b0c4de5b0",
      "2d7a1e3941a54bbd800d001f2857067d",
      "8f650a39ceda4d038867dcc2b6ae226e",
      "085bb08d46fc426a94151765394527f5",
      "1037f1d6d7f940a1a266f050ace1fb9d"
     ]
    },
    "id": "K9lo_GrlKoTV",
    "outputId": "60c50c6a-6bb5-4686-8b33-b2a203e126a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading pre-trained Persian FastConformer model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548e3e83da224d28a9f9b704d06ea2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "stt_fa_fastconformer_hybrid_large.nemo:   0%|          | 0.00/459M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-14 09:24:01 nemo_logging:393] Tokenizer SentencePieceTokenizer initialized with 1024 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-14 09:24:02 nemo_logging:405] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    manifest_filepath: dummy\n",
      "    sample_rate: 16000\n",
      "    batch_size: 1\n",
      "    shuffle: true\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    max_duration: 10\n",
      "    min_duration: 0.5\n",
      "    is_tarred: false\n",
      "    tarred_audio_filepaths: null\n",
      "    shuffle_n: 2048\n",
      "    bucketing_strategy: fully_randomized\n",
      "    bucketing_batch_size: null\n",
      "    use_lhotse: true\n",
      "    lhotse:\n",
      "      shar_path: /data_artifacts/data/shar/train\n",
      "      batch_duration: 1200\n",
      "      quadratic_duration: 15\n",
      "      num_buckets: 10\n",
      "      num_cuts_for_bins_estimate: 10000\n",
      "      buffer_size: 10000\n",
      "      shuffle_buffer_size: 10000\n",
      "    \n",
      "[NeMo W 2025-12-14 09:24:02 nemo_logging:405] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
      "    Validation config : \n",
      "    manifest_filepath: /data_artifacts/data/nemo/dev_decoded_exprunner.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 512\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n",
      "[NeMo W 2025-12-14 09:24:02 nemo_logging:405] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
      "    Test config : \n",
      "    manifest_filepath: /data_artifacts/data/nemo/test_decoded_exprunner.json\n",
      "    sample_rate: 16000\n",
      "    batch_size: 512\n",
      "    shuffle: false\n",
      "    use_start_end_token: false\n",
      "    num_workers: 8\n",
      "    pin_memory: true\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-14 09:24:02 nemo_logging:393] PADDING: 0\n",
      "[NeMo I 2025-12-14 09:24:03 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
      "[NeMo I 2025-12-14 09:24:03 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-14 09:24:03 nemo_logging:405] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.4, but the driver needs to support at least 12,6. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-14 09:24:04 nemo_logging:393] Using RNNT Loss : warprnnt_numba\n",
      "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-14 09:24:04 nemo_logging:405] No conditional node support for Cuda.\n",
      "    Cuda graphs with while loops are disabled, decoding speed will be slower\n",
      "    Reason: Driver supports cuda toolkit version 12.4, but the driver needs to support at least 12,6. Please update your cuda driver.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-12-14 09:24:05 nemo_logging:393] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /root/.cache/huggingface/hub/models--nvidia--stt_fa_fastconformer_hybrid_large/snapshots/249cf5bf70dda7220a60ddeeecff2f6aad8e1784/stt_fa_fastconformer_hybrid_large.nemo.\n",
      "✅ Model loaded!\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading pre-trained Persian FastConformer model...\")\n",
    "\n",
    "model = nemo_asr.models.EncDecHybridRNNTCTCBPEModel.from_pretrained(\n",
    "    model_name=\"nvidia/stt_fa_fastconformer_hybrid_large\"\n",
    ")\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "print(\"✅ Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437,
     "referenced_widgets": [
      "8b2eb59e669c45c582b01bbca5e97b56",
      "b7dfc83acf5d4c68bcd1ca306b036dae",
      "b8fccb714a1047b7b8b297f7a880f381",
      "34b0f95ff6c34f7bae7ad1fe31d382ab",
      "2411f1216e1f44f2ab237f2d37eccfb5",
      "ea5f14f5b5924fb69c169702b8b43052",
      "50c0e7ae3d614688aea3d0a185c34d4b",
      "a723dfeb1a5343688776405fac8a8644",
      "39d11dca5801453f9e62e0b5111d110d",
      "e83a80c595624e9cb5eb3c3c061f49f2",
      "e02662f59995466e88878563047df006",
      "f895f024ce404d369cfdffc440e17774",
      "3931c5b850b347219c8e76411f5a6f52",
      "d8a34e7cbe5f43cea11f028e17930864",
      "01736e280ed64143b7d9b2bebb06c8a9",
      "825abe4d0ff9475f91bd5ae86fe93988",
      "12d4135f69654defb4c4aa49dc05ae75",
      "0ca1b161050745b5aece8f639aadf3fa",
      "edd12bd77c054e1cb05b363962c08aef",
      "23f508dd83aa4c64833c8e1fa08fc78d",
      "ea993353c1694d1780d172ab3651b1ea",
      "e3bd640c8abd42da8453b0cfbbfd1e7d",
      "fe25d9597fb6447781593a5187b9c0b1",
      "42e6e49af0b247e8a3116d652e51ec74",
      "95e7fd867db94a9f9847aafece3aa211",
      "0d22f55ae0dc4e7a8acda5254a4306e7",
      "c28e5decd73f4fb98c940986388631d5",
      "3871b8af18224c9ba9dea872f94052a2",
      "d43c069896534bdc8dbb4f324084a531",
      "44df0dfb887e489c9f0bf8152457538d",
      "346c8a810607441d8162dc70eaea22c3",
      "6ddac3a763824c2982ded4b69dc74931",
      "1e959ea1504f4958804d16e89389d3b0",
      "09747c65d68e4bd792e9bb7693c082ef",
      "57f01e6e0552480890bac2d7ae97e7f9",
      "60971821d2b94d8dbb3b85e252cfcb9a",
      "0dc0e7f4600f4cf9a1773131bd9ef757",
      "9013d14a81204f19a39b36fdf1f67523",
      "b04f9a57d22149a9b89232db26f808fb",
      "ff29f5b878284d05a220a4cf7ff38744",
      "f434294a36a14ab68dfe9f66c2b654c6",
      "409cdec2a7554063bcaee083ee84ae53",
      "88d9e70f9cba42c7998d97b227ded8b0",
      "ab7e3cd8e5ec4cbbbe5bd08b8d94aee9",
      "42eba442a2d2467a8b938d4d1925b666",
      "cd2082919388423588625883d4b44095",
      "aad7aa7fe0c34833b24b373c93db6bf9",
      "b74262b7e4da42a3a0bf6233f98563ec",
      "d8b0ba7d95994d1a994986300aecd97b",
      "eb1b624a39424996a8073c85e4f65d67",
      "28168a5f65844506a8677fe2d13a25cd",
      "0faa32077a034e2697ed762e1b4e6333",
      "a271ea80d4764db8a5b86730fb6ae3a0",
      "bf18ef65d9bd4e01ade4117544465795",
      "0fa90d4d995d4f5297fb72dd5c2e9a88",
      "379a7b68b019486482cd04e4f0e69c70",
      "c2c019f235b5468a85b6ac83c70ec189",
      "d1591fcab3fa47efa44d7f4c8f6516bb",
      "29c625bb12854b7086754c0b04244e05",
      "2eaff647893c4519b35f6dbf613261af",
      "d6f8ebcab0e14b9997322408fe00d963",
      "2be365983c104bc08cc115c8b2e05776",
      "da53472532964fc18a7283ef74ca2be4",
      "901b1b70bbd14447a484ab97eba680d0",
      "d9a63bc419a54c05975d63c829573d0f",
      "98a7064edb98405cac8a5cd6decb22d5",
      "c0791966044f40eaa84e67ad9d4cde42",
      "b5cd329130844aac8c2c6179b79945fe",
      "40a59383638c419bb6315c000ed8d375",
      "a24a70bc32a440f9bf0a3d5df92068dc",
      "dfa1f75450dd435abb3ae7a13b22cf19",
      "b348c182184d47d9925062e259328318",
      "aa40d69255f64e73bb07f00ce5c44716",
      "51a5b5998adf4e8db1ba171cbf9ff0a9",
      "3b66e2b6af2f47baafc5c4bc4d23312d",
      "e1bbe313c77c4366954c486588a6b456",
      "0066e452c03c4511a06f98bf0d4d62c4",
      "0c9f4d106d73458a932bc8de804edf42",
      "48c06c1588774a20b037e576b4965973",
      "cc9a6fd1fca04fe7a0baef83ee4cdecb",
      "d67283da296d410690c9684ad6b9afa1",
      "7ef2ae70b407433f8db1bf47144877fa",
      "45d86a33cbf04774bbfd213749fe82a2",
      "dfbcee644e054ac6916610ef30de5a84",
      "a1d13509bfbd4902bc45b208224ddca0",
      "d5ca419ad7394ea6aee72d52a98845eb",
      "e1ce7dfd69914e978bb495f412fd8682",
      "7336231e961f4c7b810b653bd24d9529"
     ]
    },
    "id": "zvSFy7oOKsEu",
    "outputId": "c22de477-9731-46d7-b063-0891a0f79b75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading Persian Common Voice test dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b2eb59e669c45c582b01bbca5e97b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f895f024ce404d369cfdffc440e17774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00002.parquet:   0%|          | 0.00/328M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe25d9597fb6447781593a5187b9c0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00001-of-00002.parquet:   0%|          | 0.00/329M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09747c65d68e4bd792e9bb7693c082ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/303M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42eba442a2d2467a8b938d4d1925b666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/383M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379a7b68b019486482cd04e4f0e69c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/28024 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0791966044f40eaa84e67ad9d4cde42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9f4d106d73458a932bc8de804edf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/10440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test samples: 10440\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading Persian Common Voice test dataset...\")\n",
    "\n",
    "ds = load_dataset(\"hezarai/common-voice-13-fa\")\n",
    "test_dataset = ds[\"test\"]\n",
    "\n",
    "print(f\"Test samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Z5HwsfAkKwSW",
    "outputId": "d649ea54-385e-4702-fba9-25c1dcf90d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running inference on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10440 [00:00<?, ?it/s][NeMo W 2025-12-14 09:25:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:25:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:04,  4.74s/it]\n",
      "  0%|          | 1/10440 [00:05<16:57:56,  5.85s/it][NeMo W 2025-12-14 09:25:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:25:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "  Reference:  اتوبوس مسافری\n",
      "  Prediction: اتوبوس مسافر\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Transcribing: 1it [00:00, 11.74it/s]\n",
      "  0%|          | 2/10440 [00:06<7:16:20,  2.51s/it] [NeMo W 2025-12-14 09:25:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 2:\n",
      "  Reference:  انعکاس مثبت دهید و اعتبار ببخشید\n",
      "  Prediction: انعکاس مثبت دهید و اعتبار ببخشید\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-12-14 09:25:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.30it/s]\n",
      "  0%|          | 3/10440 [00:06<4:06:07,  1.41s/it][NeMo W 2025-12-14 09:25:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:25:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 3:\n",
      "  Reference:  جنگ افزارهای ساده\n",
      "  Prediction: جنگ افزارهای ساده\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "\n",
      "Transcribing: 1it [00:00, 14.36it/s]\n",
      " 88%|████████▊ | 9196/10440 [20:54<02:36,  7.94it/s][NeMo W 2025-12-14 09:46:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.37it/s]\n",
      " 88%|████████▊ | 9197/10440 [20:54<02:36,  7.93it/s][NeMo W 2025-12-14 09:46:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.83it/s]\n",
      " 88%|████████▊ | 9198/10440 [20:54<02:33,  8.08it/s][NeMo W 2025-12-14 09:46:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.97it/s]\n",
      " 88%|████████▊ | 9199/10440 [20:54<02:40,  7.74it/s][NeMo W 2025-12-14 09:46:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.99it/s]\n",
      " 88%|████████▊ | 9200/10440 [20:54<02:33,  8.07it/s][NeMo W 2025-12-14 09:46:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.00it/s]\n",
      " 88%|████████▊ | 9201/10440 [20:54<02:29,  8.27it/s][NeMo W 2025-12-14 09:46:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.66it/s]\n",
      " 88%|████████▊ | 9202/10440 [20:54<02:28,  8.34it/s][NeMo W 2025-12-14 09:46:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.17it/s]\n",
      " 88%|████████▊ | 9203/10440 [20:55<02:28,  8.31it/s][NeMo W 2025-12-14 09:46:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.31it/s]\n",
      " 88%|████████▊ | 9204/10440 [20:55<02:30,  8.20it/s][NeMo W 2025-12-14 09:46:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.28it/s]\n",
      " 88%|████████▊ | 9205/10440 [20:55<02:47,  7.36it/s][NeMo W 2025-12-14 09:46:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.73it/s]\n",
      " 88%|████████▊ | 9206/10440 [20:55<02:47,  7.36it/s][NeMo W 2025-12-14 09:46:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.25it/s]\n",
      " 88%|████████▊ | 9207/10440 [20:55<02:50,  7.22it/s][NeMo W 2025-12-14 09:46:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.01it/s]\n",
      " 88%|████████▊ | 9208/10440 [20:55<02:46,  7.40it/s][NeMo W 2025-12-14 09:46:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.54it/s]\n",
      " 88%|████████▊ | 9209/10440 [20:55<02:50,  7.21it/s][NeMo W 2025-12-14 09:46:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.12it/s]\n",
      " 88%|████████▊ | 9210/10440 [20:55<02:40,  7.68it/s][NeMo W 2025-12-14 09:46:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.98it/s]\n",
      " 88%|████████▊ | 9211/10440 [20:56<02:34,  7.97it/s][NeMo W 2025-12-14 09:46:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.38it/s]\n",
      " 88%|████████▊ | 9212/10440 [20:56<02:36,  7.84it/s][NeMo W 2025-12-14 09:46:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.68it/s]\n",
      " 88%|████████▊ | 9213/10440 [20:56<02:41,  7.59it/s][NeMo W 2025-12-14 09:46:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.85it/s]\n",
      " 88%|████████▊ | 9214/10440 [20:56<02:38,  7.75it/s][NeMo W 2025-12-14 09:46:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.25it/s]\n",
      " 88%|████████▊ | 9215/10440 [20:56<02:44,  7.45it/s][NeMo W 2025-12-14 09:46:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.02it/s]\n",
      " 88%|████████▊ | 9216/10440 [20:56<02:34,  7.91it/s][NeMo W 2025-12-14 09:46:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.12it/s]\n",
      " 88%|████████▊ | 9217/10440 [20:56<02:40,  7.60it/s][NeMo W 2025-12-14 09:46:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.83it/s]\n",
      " 88%|████████▊ | 9218/10440 [20:57<02:39,  7.64it/s][NeMo W 2025-12-14 09:46:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.83it/s]\n",
      " 88%|████████▊ | 9219/10440 [20:57<02:41,  7.55it/s][NeMo W 2025-12-14 09:46:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.18it/s]\n",
      " 88%|████████▊ | 9220/10440 [20:57<02:53,  7.01it/s][NeMo W 2025-12-14 09:46:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.48it/s]\n",
      " 88%|████████▊ | 9221/10440 [20:57<02:53,  7.01it/s][NeMo W 2025-12-14 09:46:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.85it/s]\n",
      " 88%|████████▊ | 9222/10440 [20:57<02:47,  7.28it/s][NeMo W 2025-12-14 09:46:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.99it/s]\n",
      " 88%|████████▊ | 9223/10440 [20:57<02:40,  7.59it/s][NeMo W 2025-12-14 09:46:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.17it/s]\n",
      " 88%|████████▊ | 9224/10440 [20:57<02:39,  7.64it/s][NeMo W 2025-12-14 09:46:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.31it/s]\n",
      " 88%|████████▊ | 9225/10440 [20:57<02:38,  7.65it/s][NeMo W 2025-12-14 09:46:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.92it/s]\n",
      " 88%|████████▊ | 9226/10440 [20:58<02:33,  7.93it/s][NeMo W 2025-12-14 09:46:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.41it/s]\n",
      " 88%|████████▊ | 9227/10440 [20:58<02:30,  8.05it/s][NeMo W 2025-12-14 09:46:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.77it/s]\n",
      " 88%|████████▊ | 9228/10440 [20:58<02:35,  7.82it/s][NeMo W 2025-12-14 09:46:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.79it/s]\n",
      " 88%|████████▊ | 9229/10440 [20:58<02:35,  7.79it/s][NeMo W 2025-12-14 09:46:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.21it/s]\n",
      " 88%|████████▊ | 9230/10440 [20:58<02:29,  8.08it/s][NeMo W 2025-12-14 09:46:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.21it/s]\n",
      " 88%|████████▊ | 9231/10440 [20:58<02:27,  8.19it/s][NeMo W 2025-12-14 09:46:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.52it/s]\n",
      " 88%|████████▊ | 9232/10440 [20:58<02:34,  7.83it/s][NeMo W 2025-12-14 09:46:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.83it/s]\n",
      " 88%|████████▊ | 9233/10440 [20:58<02:38,  7.62it/s][NeMo W 2025-12-14 09:46:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.33it/s]\n",
      " 88%|████████▊ | 9234/10440 [20:59<02:53,  6.93it/s][NeMo W 2025-12-14 09:46:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.64it/s]\n",
      " 88%|████████▊ | 9235/10440 [20:59<02:51,  7.02it/s][NeMo W 2025-12-14 09:46:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.02it/s]\n",
      " 88%|████████▊ | 9236/10440 [20:59<03:07,  6.41it/s][NeMo W 2025-12-14 09:46:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.47it/s]\n",
      " 88%|████████▊ | 9237/10440 [20:59<03:09,  6.34it/s][NeMo W 2025-12-14 09:46:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.65it/s]\n",
      " 88%|████████▊ | 9238/10440 [20:59<03:10,  6.31it/s][NeMo W 2025-12-14 09:46:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.04it/s]\n",
      " 88%|████████▊ | 9239/10440 [21:00<03:20,  5.99it/s][NeMo W 2025-12-14 09:46:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.56it/s]\n",
      " 89%|████████▊ | 9240/10440 [21:00<03:20,  5.99it/s][NeMo W 2025-12-14 09:46:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.27it/s]\n",
      " 89%|████████▊ | 9241/10440 [21:00<03:29,  5.74it/s][NeMo W 2025-12-14 09:46:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.62it/s]\n",
      " 89%|████████▊ | 9242/10440 [21:00<03:27,  5.77it/s][NeMo W 2025-12-14 09:46:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.08it/s]\n",
      " 89%|████████▊ | 9243/10440 [21:00<03:31,  5.66it/s][NeMo W 2025-12-14 09:46:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.17it/s]\n",
      " 89%|████████▊ | 9244/10440 [21:00<03:37,  5.49it/s][NeMo W 2025-12-14 09:46:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.34it/s]\n",
      " 89%|████████▊ | 9245/10440 [21:01<03:23,  5.87it/s][NeMo W 2025-12-14 09:46:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.15it/s]\n",
      " 89%|████████▊ | 9246/10440 [21:01<03:15,  6.10it/s][NeMo W 2025-12-14 09:46:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.52it/s]\n",
      " 89%|████████▊ | 9247/10440 [21:01<03:07,  6.37it/s][NeMo W 2025-12-14 09:46:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.47it/s]\n",
      " 89%|████████▊ | 9248/10440 [21:01<03:00,  6.61it/s][NeMo W 2025-12-14 09:46:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.12it/s]\n",
      " 89%|████████▊ | 9249/10440 [21:01<03:08,  6.33it/s][NeMo W 2025-12-14 09:46:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.59it/s]\n",
      " 89%|████████▊ | 9250/10440 [21:01<03:13,  6.16it/s][NeMo W 2025-12-14 09:46:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.27it/s]\n",
      " 89%|████████▊ | 9251/10440 [21:01<03:05,  6.40it/s][NeMo W 2025-12-14 09:46:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.73it/s]\n",
      " 89%|████████▊ | 9252/10440 [21:02<02:55,  6.75it/s][NeMo W 2025-12-14 09:46:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.64it/s]\n",
      " 89%|████████▊ | 9253/10440 [21:02<02:48,  7.03it/s][NeMo W 2025-12-14 09:46:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.75it/s]\n",
      " 89%|████████▊ | 9254/10440 [21:02<02:52,  6.86it/s][NeMo W 2025-12-14 09:46:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.05it/s]\n",
      " 89%|████████▊ | 9255/10440 [21:02<02:55,  6.74it/s][NeMo W 2025-12-14 09:46:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.96it/s]\n",
      " 89%|████████▊ | 9256/10440 [21:02<02:54,  6.79it/s][NeMo W 2025-12-14 09:46:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.83it/s]\n",
      " 89%|████████▊ | 9257/10440 [21:02<02:52,  6.86it/s][NeMo W 2025-12-14 09:46:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.31it/s]\n",
      " 89%|████████▊ | 9258/10440 [21:02<02:51,  6.90it/s][NeMo W 2025-12-14 09:46:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.93it/s]\n",
      " 89%|████████▊ | 9259/10440 [21:03<02:52,  6.86it/s][NeMo W 2025-12-14 09:46:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.55it/s]\n",
      " 89%|████████▊ | 9260/10440 [21:03<02:55,  6.72it/s][NeMo W 2025-12-14 09:46:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.12it/s]\n",
      " 89%|████████▊ | 9261/10440 [21:03<02:52,  6.84it/s][NeMo W 2025-12-14 09:46:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.44it/s]\n",
      " 89%|████████▊ | 9262/10440 [21:03<02:41,  7.28it/s][NeMo W 2025-12-14 09:46:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.40it/s]\n",
      " 89%|████████▊ | 9263/10440 [21:03<02:34,  7.61it/s][NeMo W 2025-12-14 09:46:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.55it/s]\n",
      " 89%|████████▊ | 9264/10440 [21:03<02:35,  7.55it/s][NeMo W 2025-12-14 09:46:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.19it/s]\n",
      " 89%|████████▊ | 9265/10440 [21:03<02:46,  7.07it/s][NeMo W 2025-12-14 09:46:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.39it/s]\n",
      " 89%|████████▉ | 9266/10440 [21:04<02:43,  7.17it/s][NeMo W 2025-12-14 09:46:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.43it/s]\n",
      " 89%|████████▉ | 9267/10440 [21:04<02:37,  7.46it/s][NeMo W 2025-12-14 09:46:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.28it/s]\n",
      " 89%|████████▉ | 9268/10440 [21:04<02:39,  7.34it/s][NeMo W 2025-12-14 09:46:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.04it/s]\n",
      " 89%|████████▉ | 9269/10440 [21:04<02:39,  7.36it/s][NeMo W 2025-12-14 09:46:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.32it/s]\n",
      " 89%|████████▉ | 9270/10440 [21:04<02:34,  7.59it/s][NeMo W 2025-12-14 09:46:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.94it/s]\n",
      " 89%|████████▉ | 9271/10440 [21:04<02:30,  7.77it/s][NeMo W 2025-12-14 09:46:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.99it/s]\n",
      " 89%|████████▉ | 9272/10440 [21:04<02:24,  8.06it/s][NeMo W 2025-12-14 09:46:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.86it/s]\n",
      " 89%|████████▉ | 9273/10440 [21:05<02:40,  7.29it/s][NeMo W 2025-12-14 09:46:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.00it/s]\n",
      " 89%|████████▉ | 9274/10440 [21:05<02:45,  7.06it/s][NeMo W 2025-12-14 09:46:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.53it/s]\n",
      " 89%|████████▉ | 9275/10440 [21:05<02:36,  7.43it/s][NeMo W 2025-12-14 09:46:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.74it/s]\n",
      " 89%|████████▉ | 9276/10440 [21:05<02:34,  7.52it/s][NeMo W 2025-12-14 09:46:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.30it/s]\n",
      " 89%|████████▉ | 9277/10440 [21:05<02:36,  7.43it/s][NeMo W 2025-12-14 09:46:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.61it/s]\n",
      " 89%|████████▉ | 9278/10440 [21:05<02:40,  7.26it/s][NeMo W 2025-12-14 09:46:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.84it/s]\n",
      " 89%|████████▉ | 9279/10440 [21:05<02:41,  7.17it/s][NeMo W 2025-12-14 09:46:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.79it/s]\n",
      " 89%|████████▉ | 9280/10440 [21:05<02:39,  7.28it/s][NeMo W 2025-12-14 09:46:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.57it/s]\n",
      " 89%|████████▉ | 9281/10440 [21:06<02:45,  6.99it/s][NeMo W 2025-12-14 09:46:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.97it/s]\n",
      " 89%|████████▉ | 9282/10440 [21:06<02:44,  7.06it/s][NeMo W 2025-12-14 09:46:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.62it/s]\n",
      " 89%|████████▉ | 9283/10440 [21:06<02:44,  7.03it/s][NeMo W 2025-12-14 09:46:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.43it/s]\n",
      " 89%|████████▉ | 9284/10440 [21:06<02:38,  7.27it/s][NeMo W 2025-12-14 09:46:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.76it/s]\n",
      " 89%|████████▉ | 9285/10440 [21:06<02:37,  7.35it/s][NeMo W 2025-12-14 09:46:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.94it/s]\n",
      " 89%|████████▉ | 9286/10440 [21:06<02:40,  7.18it/s][NeMo W 2025-12-14 09:46:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.54it/s]\n",
      " 89%|████████▉ | 9287/10440 [21:06<02:35,  7.44it/s][NeMo W 2025-12-14 09:46:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.89it/s]\n",
      " 89%|████████▉ | 9288/10440 [21:07<02:39,  7.24it/s][NeMo W 2025-12-14 09:46:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.77it/s]\n",
      " 89%|████████▉ | 9289/10440 [21:07<02:39,  7.21it/s][NeMo W 2025-12-14 09:46:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.78it/s]\n",
      " 89%|████████▉ | 9290/10440 [21:07<02:40,  7.17it/s][NeMo W 2025-12-14 09:46:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.10it/s]\n",
      " 89%|████████▉ | 9291/10440 [21:07<02:36,  7.34it/s][NeMo W 2025-12-14 09:46:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.74it/s]\n",
      " 89%|████████▉ | 9292/10440 [21:07<02:34,  7.43it/s][NeMo W 2025-12-14 09:46:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.57it/s]\n",
      " 89%|████████▉ | 9293/10440 [21:07<02:33,  7.47it/s][NeMo W 2025-12-14 09:46:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.15it/s]\n",
      " 89%|████████▉ | 9294/10440 [21:07<02:38,  7.25it/s][NeMo W 2025-12-14 09:46:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.85it/s]\n",
      " 89%|████████▉ | 9295/10440 [21:08<02:34,  7.40it/s][NeMo W 2025-12-14 09:46:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.35it/s]\n",
      " 89%|████████▉ | 9296/10440 [21:08<02:35,  7.37it/s][NeMo W 2025-12-14 09:46:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.81it/s]\n",
      " 89%|████████▉ | 9297/10440 [21:08<02:32,  7.48it/s][NeMo W 2025-12-14 09:46:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.62it/s]\n",
      " 89%|████████▉ | 9298/10440 [21:08<02:32,  7.47it/s][NeMo W 2025-12-14 09:46:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.69it/s]\n",
      " 89%|████████▉ | 9299/10440 [21:08<02:25,  7.82it/s][NeMo W 2025-12-14 09:46:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.18it/s]\n",
      " 89%|████████▉ | 9300/10440 [21:08<02:23,  7.96it/s][NeMo W 2025-12-14 09:46:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.57it/s]\n",
      " 89%|████████▉ | 9301/10440 [21:08<02:28,  7.68it/s][NeMo W 2025-12-14 09:46:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.48it/s]\n",
      " 89%|████████▉ | 9302/10440 [21:08<02:35,  7.30it/s][NeMo W 2025-12-14 09:46:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.23it/s]\n",
      " 89%|████████▉ | 9303/10440 [21:09<02:36,  7.28it/s][NeMo W 2025-12-14 09:46:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.96it/s]\n",
      " 89%|████████▉ | 9304/10440 [21:09<02:32,  7.43it/s][NeMo W 2025-12-14 09:46:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.50it/s]\n",
      " 89%|████████▉ | 9305/10440 [21:09<02:34,  7.37it/s][NeMo W 2025-12-14 09:46:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.34it/s]\n",
      " 89%|████████▉ | 9306/10440 [21:09<02:30,  7.54it/s][NeMo W 2025-12-14 09:46:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.12it/s]\n",
      " 89%|████████▉ | 9307/10440 [21:09<02:24,  7.82it/s][NeMo W 2025-12-14 09:46:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.15it/s]\n",
      " 89%|████████▉ | 9308/10440 [21:09<02:23,  7.86it/s][NeMo W 2025-12-14 09:46:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.00it/s]\n",
      " 89%|████████▉ | 9309/10440 [21:09<02:22,  7.95it/s][NeMo W 2025-12-14 09:46:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.22it/s]\n",
      " 89%|████████▉ | 9310/10440 [21:09<02:21,  7.99it/s][NeMo W 2025-12-14 09:46:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.33it/s]\n",
      " 89%|████████▉ | 9311/10440 [21:10<02:26,  7.72it/s][NeMo W 2025-12-14 09:46:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.91it/s]\n",
      " 89%|████████▉ | 9312/10440 [21:10<02:28,  7.62it/s][NeMo W 2025-12-14 09:46:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.88it/s]\n",
      " 89%|████████▉ | 9313/10440 [21:10<02:29,  7.52it/s][NeMo W 2025-12-14 09:46:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.92it/s]\n",
      " 89%|████████▉ | 9314/10440 [21:10<02:28,  7.60it/s][NeMo W 2025-12-14 09:46:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.18it/s]\n",
      " 89%|████████▉ | 9315/10440 [21:10<02:28,  7.60it/s][NeMo W 2025-12-14 09:46:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.77it/s]\n",
      " 89%|████████▉ | 9316/10440 [21:10<02:26,  7.69it/s][NeMo W 2025-12-14 09:46:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.21it/s]\n",
      " 89%|████████▉ | 9317/10440 [21:10<02:29,  7.51it/s][NeMo W 2025-12-14 09:46:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.74it/s]\n",
      " 89%|████████▉ | 9318/10440 [21:11<02:23,  7.81it/s][NeMo W 2025-12-14 09:46:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.60it/s]\n",
      " 89%|████████▉ | 9319/10440 [21:11<02:36,  7.18it/s][NeMo W 2025-12-14 09:46:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.53it/s]\n",
      " 89%|████████▉ | 9320/10440 [21:11<02:29,  7.48it/s][NeMo W 2025-12-14 09:46:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.95it/s]\n",
      " 89%|████████▉ | 9321/10440 [21:11<02:25,  7.70it/s][NeMo W 2025-12-14 09:46:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.78it/s]\n",
      " 89%|████████▉ | 9322/10440 [21:11<02:31,  7.40it/s][NeMo W 2025-12-14 09:46:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.43it/s]\n",
      " 89%|████████▉ | 9323/10440 [21:11<02:28,  7.54it/s][NeMo W 2025-12-14 09:46:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.24it/s]\n",
      " 89%|████████▉ | 9324/10440 [21:11<02:25,  7.68it/s][NeMo W 2025-12-14 09:46:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.06it/s]\n",
      " 89%|████████▉ | 9325/10440 [21:11<02:23,  7.77it/s][NeMo W 2025-12-14 09:46:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.40it/s]\n",
      " 89%|████████▉ | 9326/10440 [21:12<02:20,  7.95it/s][NeMo W 2025-12-14 09:46:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.86it/s]\n",
      " 89%|████████▉ | 9327/10440 [21:12<02:35,  7.17it/s][NeMo W 2025-12-14 09:46:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.23it/s]\n",
      " 89%|████████▉ | 9328/10440 [21:12<02:28,  7.48it/s][NeMo W 2025-12-14 09:46:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.55it/s]\n",
      " 89%|████████▉ | 9329/10440 [21:12<02:33,  7.23it/s][NeMo W 2025-12-14 09:46:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.80it/s]\n",
      " 89%|████████▉ | 9330/10440 [21:12<02:26,  7.58it/s][NeMo W 2025-12-14 09:46:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.51it/s]\n",
      " 89%|████████▉ | 9331/10440 [21:12<02:23,  7.73it/s][NeMo W 2025-12-14 09:46:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.66it/s]\n",
      " 89%|████████▉ | 9332/10440 [21:12<02:21,  7.81it/s][NeMo W 2025-12-14 09:46:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.94it/s]\n",
      " 89%|████████▉ | 9333/10440 [21:13<02:31,  7.30it/s][NeMo W 2025-12-14 09:46:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.98it/s]\n",
      " 89%|████████▉ | 9334/10440 [21:13<02:31,  7.32it/s][NeMo W 2025-12-14 09:46:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.61it/s]\n",
      " 89%|████████▉ | 9335/10440 [21:13<02:38,  6.97it/s][NeMo W 2025-12-14 09:46:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.93it/s]\n",
      " 89%|████████▉ | 9336/10440 [21:13<02:41,  6.82it/s][NeMo W 2025-12-14 09:46:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.24it/s]\n",
      " 89%|████████▉ | 9337/10440 [21:13<02:55,  6.27it/s][NeMo W 2025-12-14 09:46:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.36it/s]\n",
      " 89%|████████▉ | 9338/10440 [21:13<03:10,  5.77it/s][NeMo W 2025-12-14 09:46:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.60it/s]\n",
      " 89%|████████▉ | 9339/10440 [21:14<03:16,  5.60it/s][NeMo W 2025-12-14 09:46:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.14it/s]\n",
      " 89%|████████▉ | 9340/10440 [21:14<03:15,  5.62it/s][NeMo W 2025-12-14 09:46:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.43it/s]\n",
      " 89%|████████▉ | 9341/10440 [21:14<03:11,  5.73it/s][NeMo W 2025-12-14 09:46:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.24it/s]\n",
      " 89%|████████▉ | 9342/10440 [21:14<03:05,  5.92it/s][NeMo W 2025-12-14 09:46:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.73it/s]\n",
      " 89%|████████▉ | 9343/10440 [21:14<03:02,  6.00it/s][NeMo W 2025-12-14 09:46:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.71it/s]\n",
      " 90%|████████▉ | 9344/10440 [21:14<03:07,  5.86it/s][NeMo W 2025-12-14 09:46:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.22it/s]\n",
      " 90%|████████▉ | 9345/10440 [21:15<03:02,  5.99it/s][NeMo W 2025-12-14 09:46:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.20it/s]\n",
      " 90%|████████▉ | 9346/10440 [21:15<02:54,  6.25it/s][NeMo W 2025-12-14 09:46:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.58it/s]\n",
      " 90%|████████▉ | 9347/10440 [21:15<02:49,  6.47it/s][NeMo W 2025-12-14 09:46:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.91it/s]\n",
      " 90%|████████▉ | 9348/10440 [21:15<02:50,  6.42it/s][NeMo W 2025-12-14 09:46:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.33it/s]\n",
      " 90%|████████▉ | 9349/10440 [21:15<02:53,  6.30it/s][NeMo W 2025-12-14 09:46:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.74it/s]\n",
      " 90%|████████▉ | 9350/10440 [21:15<02:51,  6.36it/s][NeMo W 2025-12-14 09:46:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.01it/s]\n",
      " 90%|████████▉ | 9351/10440 [21:15<02:46,  6.55it/s][NeMo W 2025-12-14 09:46:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.76it/s]\n",
      " 90%|████████▉ | 9352/10440 [21:16<02:47,  6.49it/s][NeMo W 2025-12-14 09:46:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.47it/s]\n",
      " 90%|████████▉ | 9353/10440 [21:16<02:50,  6.39it/s][NeMo W 2025-12-14 09:46:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.57it/s]\n",
      " 90%|████████▉ | 9354/10440 [21:16<02:48,  6.44it/s][NeMo W 2025-12-14 09:46:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.52it/s]\n",
      " 90%|████████▉ | 9355/10440 [21:16<02:51,  6.32it/s][NeMo W 2025-12-14 09:46:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.49it/s]\n",
      " 90%|████████▉ | 9356/10440 [21:16<02:57,  6.11it/s][NeMo W 2025-12-14 09:46:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.56it/s]\n",
      " 90%|████████▉ | 9357/10440 [21:16<02:48,  6.42it/s][NeMo W 2025-12-14 09:46:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.25it/s]\n",
      " 90%|████████▉ | 9358/10440 [21:17<02:44,  6.56it/s][NeMo W 2025-12-14 09:46:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.94it/s]\n",
      " 90%|████████▉ | 9359/10440 [21:17<02:35,  6.95it/s][NeMo W 2025-12-14 09:46:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.93it/s]\n",
      " 90%|████████▉ | 9360/10440 [21:17<02:31,  7.15it/s][NeMo W 2025-12-14 09:46:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.07it/s]\n",
      " 90%|████████▉ | 9361/10440 [21:17<02:28,  7.27it/s][NeMo W 2025-12-14 09:46:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.67it/s]\n",
      " 90%|████████▉ | 9362/10440 [21:17<02:26,  7.36it/s][NeMo W 2025-12-14 09:46:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.68it/s]\n",
      " 90%|████████▉ | 9363/10440 [21:17<02:37,  6.84it/s][NeMo W 2025-12-14 09:46:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.98it/s]\n",
      " 90%|████████▉ | 9364/10440 [21:17<02:34,  6.97it/s][NeMo W 2025-12-14 09:46:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.92it/s]\n",
      " 90%|████████▉ | 9365/10440 [21:18<02:31,  7.11it/s][NeMo W 2025-12-14 09:46:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.74it/s]\n",
      " 90%|████████▉ | 9366/10440 [21:18<02:28,  7.26it/s][NeMo W 2025-12-14 09:46:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.64it/s]\n",
      " 90%|████████▉ | 9367/10440 [21:18<02:25,  7.39it/s][NeMo W 2025-12-14 09:46:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.92it/s]\n",
      " 90%|████████▉ | 9368/10440 [21:18<02:21,  7.59it/s][NeMo W 2025-12-14 09:46:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.42it/s]\n",
      " 90%|████████▉ | 9369/10440 [21:18<02:23,  7.48it/s][NeMo W 2025-12-14 09:46:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.87it/s]\n",
      " 90%|████████▉ | 9370/10440 [21:18<02:19,  7.68it/s][NeMo W 2025-12-14 09:46:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.92it/s]\n",
      " 90%|████████▉ | 9371/10440 [21:18<02:19,  7.65it/s][NeMo W 2025-12-14 09:46:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.71it/s]\n",
      " 90%|████████▉ | 9372/10440 [21:18<02:18,  7.73it/s][NeMo W 2025-12-14 09:46:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.23it/s]\n",
      " 90%|████████▉ | 9373/10440 [21:19<02:15,  7.88it/s][NeMo W 2025-12-14 09:46:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.56it/s]\n",
      " 90%|████████▉ | 9374/10440 [21:19<02:19,  7.64it/s][NeMo W 2025-12-14 09:46:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.23it/s]\n",
      " 90%|████████▉ | 9375/10440 [21:19<02:26,  7.27it/s][NeMo W 2025-12-14 09:46:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.61it/s]\n",
      " 90%|████████▉ | 9376/10440 [21:19<02:19,  7.64it/s][NeMo W 2025-12-14 09:46:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.20it/s]\n",
      " 90%|████████▉ | 9377/10440 [21:19<02:22,  7.47it/s][NeMo W 2025-12-14 09:46:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.66it/s]\n",
      " 90%|████████▉ | 9378/10440 [21:19<02:24,  7.34it/s][NeMo W 2025-12-14 09:46:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.46it/s]\n",
      " 90%|████████▉ | 9379/10440 [21:19<02:24,  7.32it/s][NeMo W 2025-12-14 09:46:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.74it/s]\n",
      " 90%|████████▉ | 9380/10440 [21:20<02:19,  7.61it/s][NeMo W 2025-12-14 09:46:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.53it/s]\n",
      " 90%|████████▉ | 9381/10440 [21:20<02:17,  7.69it/s][NeMo W 2025-12-14 09:46:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.27it/s]\n",
      " 90%|████████▉ | 9382/10440 [21:20<02:19,  7.59it/s][NeMo W 2025-12-14 09:46:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.77it/s]\n",
      " 90%|████████▉ | 9383/10440 [21:20<02:17,  7.70it/s][NeMo W 2025-12-14 09:46:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.56it/s]\n",
      " 90%|████████▉ | 9384/10440 [21:20<02:24,  7.33it/s][NeMo W 2025-12-14 09:46:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.78it/s]\n",
      " 90%|████████▉ | 9385/10440 [21:20<02:21,  7.43it/s][NeMo W 2025-12-14 09:46:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.22it/s]\n",
      " 90%|████████▉ | 9386/10440 [21:20<02:18,  7.62it/s][NeMo W 2025-12-14 09:46:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.07it/s]\n",
      " 90%|████████▉ | 9387/10440 [21:20<02:18,  7.60it/s][NeMo W 2025-12-14 09:46:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.03it/s]\n",
      " 90%|████████▉ | 9388/10440 [21:21<02:23,  7.35it/s][NeMo W 2025-12-14 09:46:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.66it/s]\n",
      " 90%|████████▉ | 9389/10440 [21:21<02:16,  7.72it/s][NeMo W 2025-12-14 09:46:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.54it/s]\n",
      " 90%|████████▉ | 9390/10440 [21:21<02:12,  7.93it/s][NeMo W 2025-12-14 09:46:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.87it/s]\n",
      " 90%|████████▉ | 9391/10440 [21:21<02:17,  7.65it/s][NeMo W 2025-12-14 09:46:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.44it/s]\n",
      " 90%|████████▉ | 9392/10440 [21:21<02:12,  7.92it/s][NeMo W 2025-12-14 09:46:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.22it/s]\n",
      " 90%|████████▉ | 9393/10440 [21:21<02:15,  7.74it/s][NeMo W 2025-12-14 09:46:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.56it/s]\n",
      " 90%|████████▉ | 9394/10440 [21:21<02:16,  7.67it/s][NeMo W 2025-12-14 09:46:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.75it/s]\n",
      " 90%|████████▉ | 9395/10440 [21:21<02:15,  7.69it/s][NeMo W 2025-12-14 09:46:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.46it/s]\n",
      " 90%|█████████ | 9396/10440 [21:22<02:14,  7.75it/s][NeMo W 2025-12-14 09:46:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.80it/s]\n",
      " 90%|█████████ | 9397/10440 [21:22<02:20,  7.44it/s][NeMo W 2025-12-14 09:46:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.27it/s]\n",
      " 90%|█████████ | 9398/10440 [21:22<02:15,  7.68it/s][NeMo W 2025-12-14 09:46:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.98it/s]\n",
      " 90%|█████████ | 9399/10440 [21:22<02:19,  7.48it/s][NeMo W 2025-12-14 09:46:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.93it/s]\n",
      " 90%|█████████ | 9400/10440 [21:22<02:18,  7.50it/s][NeMo W 2025-12-14 09:46:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.56it/s]\n",
      " 90%|█████████ | 9401/10440 [21:22<02:28,  7.01it/s][NeMo W 2025-12-14 09:46:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.57it/s]\n",
      " 90%|█████████ | 9402/10440 [21:22<02:21,  7.33it/s][NeMo W 2025-12-14 09:46:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.95it/s]\n",
      " 90%|█████████ | 9403/10440 [21:23<02:30,  6.91it/s][NeMo W 2025-12-14 09:46:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.21it/s]\n",
      " 90%|█████████ | 9404/10440 [21:23<02:26,  7.09it/s][NeMo W 2025-12-14 09:46:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.80it/s]\n",
      " 90%|█████████ | 9405/10440 [21:23<02:21,  7.34it/s][NeMo W 2025-12-14 09:46:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.65it/s]\n",
      " 90%|█████████ | 9406/10440 [21:23<02:21,  7.32it/s][NeMo W 2025-12-14 09:46:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.79it/s]\n",
      " 90%|█████████ | 9407/10440 [21:23<02:15,  7.64it/s][NeMo W 2025-12-14 09:46:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.79it/s]\n",
      " 90%|█████████ | 9408/10440 [21:23<02:18,  7.44it/s][NeMo W 2025-12-14 09:46:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.49it/s]\n",
      " 90%|█████████ | 9409/10440 [21:23<02:15,  7.60it/s][NeMo W 2025-12-14 09:46:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.57it/s]\n",
      " 90%|█████████ | 9410/10440 [21:23<02:14,  7.64it/s][NeMo W 2025-12-14 09:46:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.61it/s]\n",
      " 90%|█████████ | 9411/10440 [21:24<02:12,  7.76it/s][NeMo W 2025-12-14 09:46:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.59it/s]\n",
      " 90%|█████████ | 9412/10440 [21:24<02:19,  7.37it/s][NeMo W 2025-12-14 09:46:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.67it/s]\n",
      " 90%|█████████ | 9413/10440 [21:24<02:24,  7.10it/s][NeMo W 2025-12-14 09:46:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.48it/s]\n",
      " 90%|█████████ | 9414/10440 [21:24<02:24,  7.08it/s][NeMo W 2025-12-14 09:46:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.86it/s]\n",
      " 90%|█████████ | 9415/10440 [21:24<02:18,  7.40it/s][NeMo W 2025-12-14 09:46:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.77it/s]\n",
      " 90%|█████████ | 9416/10440 [21:24<02:18,  7.41it/s][NeMo W 2025-12-14 09:46:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.01it/s]\n",
      " 90%|█████████ | 9417/10440 [21:24<02:12,  7.71it/s][NeMo W 2025-12-14 09:46:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.08it/s]\n",
      " 90%|█████████ | 9418/10440 [21:25<02:09,  7.91it/s][NeMo W 2025-12-14 09:46:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.94it/s]\n",
      " 90%|█████████ | 9419/10440 [21:25<02:08,  7.97it/s][NeMo W 2025-12-14 09:46:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.88it/s]\n",
      " 90%|█████████ | 9420/10440 [21:25<02:17,  7.42it/s][NeMo W 2025-12-14 09:46:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.38it/s]\n",
      " 90%|█████████ | 9421/10440 [21:25<02:11,  7.76it/s][NeMo W 2025-12-14 09:46:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.50it/s]\n",
      " 90%|█████████ | 9422/10440 [21:25<02:05,  8.10it/s][NeMo W 2025-12-14 09:46:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.04it/s]\n",
      " 90%|█████████ | 9423/10440 [21:25<02:01,  8.39it/s][NeMo W 2025-12-14 09:46:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.90it/s]\n",
      " 90%|█████████ | 9424/10440 [21:25<02:03,  8.19it/s][NeMo W 2025-12-14 09:46:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.23it/s]\n",
      " 90%|█████████ | 9425/10440 [21:25<02:18,  7.31it/s][NeMo W 2025-12-14 09:46:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.76it/s]\n",
      " 90%|█████████ | 9426/10440 [21:26<02:18,  7.30it/s][NeMo W 2025-12-14 09:46:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.01it/s]\n",
      " 90%|█████████ | 9427/10440 [21:26<02:15,  7.48it/s][NeMo W 2025-12-14 09:46:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.30it/s]\n",
      " 90%|█████████ | 9428/10440 [21:26<02:11,  7.67it/s][NeMo W 2025-12-14 09:46:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.84it/s]\n",
      " 90%|█████████ | 9429/10440 [21:26<02:11,  7.68it/s][NeMo W 2025-12-14 09:46:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.81it/s]\n",
      " 90%|█████████ | 9430/10440 [21:26<02:09,  7.79it/s][NeMo W 2025-12-14 09:46:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.02it/s]\n",
      " 90%|█████████ | 9431/10440 [21:26<02:08,  7.87it/s][NeMo W 2025-12-14 09:46:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.75it/s]\n",
      " 90%|█████████ | 9432/10440 [21:26<02:13,  7.53it/s][NeMo W 2025-12-14 09:46:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  8.54it/s]\n",
      " 90%|█████████ | 9433/10440 [21:27<02:35,  6.49it/s][NeMo W 2025-12-14 09:46:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.54it/s]\n",
      " 90%|█████████ | 9434/10440 [21:27<02:39,  6.30it/s][NeMo W 2025-12-14 09:46:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.13it/s]\n",
      " 90%|█████████ | 9435/10440 [21:27<02:38,  6.32it/s][NeMo W 2025-12-14 09:46:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.77it/s]\n",
      " 90%|█████████ | 9436/10440 [21:27<02:36,  6.43it/s][NeMo W 2025-12-14 09:46:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.75it/s]\n",
      " 90%|█████████ | 9437/10440 [21:27<02:37,  6.36it/s][NeMo W 2025-12-14 09:46:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.25it/s]\n",
      " 90%|█████████ | 9438/10440 [21:27<02:46,  6.03it/s][NeMo W 2025-12-14 09:46:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.13it/s]\n",
      " 90%|█████████ | 9439/10440 [21:28<02:44,  6.09it/s][NeMo W 2025-12-14 09:46:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.43it/s]\n",
      " 90%|█████████ | 9440/10440 [21:28<02:42,  6.15it/s][NeMo W 2025-12-14 09:46:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.47it/s]\n",
      " 90%|█████████ | 9441/10440 [21:28<02:38,  6.29it/s][NeMo W 2025-12-14 09:46:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.96it/s]\n",
      " 90%|█████████ | 9442/10440 [21:28<02:28,  6.72it/s][NeMo W 2025-12-14 09:46:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.11it/s]\n",
      " 90%|█████████ | 9443/10440 [21:28<02:18,  7.19it/s][NeMo W 2025-12-14 09:46:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.07it/s]\n",
      " 90%|█████████ | 9444/10440 [21:28<02:28,  6.70it/s][NeMo W 2025-12-14 09:46:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.63it/s]\n",
      " 90%|█████████ | 9445/10440 [21:28<02:25,  6.84it/s][NeMo W 2025-12-14 09:46:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.34it/s]\n",
      " 90%|█████████ | 9446/10440 [21:29<02:24,  6.87it/s][NeMo W 2025-12-14 09:46:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.38it/s]\n",
      " 90%|█████████ | 9447/10440 [21:29<02:34,  6.41it/s][NeMo W 2025-12-14 09:46:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.43it/s]\n",
      " 90%|█████████ | 9448/10440 [21:29<02:32,  6.50it/s][NeMo W 2025-12-14 09:46:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.96it/s]\n",
      " 91%|█████████ | 9449/10440 [21:29<02:33,  6.48it/s][NeMo W 2025-12-14 09:46:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.08it/s]\n",
      " 91%|█████████ | 9450/10440 [21:29<02:34,  6.39it/s][NeMo W 2025-12-14 09:46:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.97it/s]\n",
      " 91%|█████████ | 9451/10440 [21:29<02:35,  6.35it/s][NeMo W 2025-12-14 09:46:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.21it/s]\n",
      " 91%|█████████ | 9452/10440 [21:30<02:32,  6.49it/s][NeMo W 2025-12-14 09:46:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.28it/s]\n",
      " 91%|█████████ | 9453/10440 [21:30<02:34,  6.41it/s][NeMo W 2025-12-14 09:46:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.44it/s]\n",
      " 91%|█████████ | 9454/10440 [21:30<02:35,  6.35it/s][NeMo W 2025-12-14 09:46:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.15it/s]\n",
      " 91%|█████████ | 9455/10440 [21:30<02:29,  6.61it/s][NeMo W 2025-12-14 09:46:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.02it/s]\n",
      " 91%|█████████ | 9456/10440 [21:30<02:31,  6.48it/s][NeMo W 2025-12-14 09:46:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.08it/s]\n",
      " 91%|█████████ | 9457/10440 [21:30<02:27,  6.68it/s][NeMo W 2025-12-14 09:46:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.33it/s]\n",
      " 91%|█████████ | 9458/10440 [21:30<02:25,  6.77it/s][NeMo W 2025-12-14 09:46:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.93it/s]\n",
      " 91%|█████████ | 9459/10440 [21:31<02:19,  7.04it/s][NeMo W 2025-12-14 09:46:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.96it/s]\n",
      " 91%|█████████ | 9460/10440 [21:31<02:15,  7.25it/s][NeMo W 2025-12-14 09:46:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.03it/s]\n",
      " 91%|█████████ | 9461/10440 [21:31<02:11,  7.46it/s][NeMo W 2025-12-14 09:46:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.33it/s]\n",
      " 91%|█████████ | 9462/10440 [21:31<02:08,  7.60it/s][NeMo W 2025-12-14 09:46:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.67it/s]\n",
      " 91%|█████████ | 9463/10440 [21:31<02:11,  7.42it/s][NeMo W 2025-12-14 09:46:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.81it/s]\n",
      " 91%|█████████ | 9464/10440 [21:31<02:10,  7.51it/s][NeMo W 2025-12-14 09:46:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.22it/s]\n",
      " 91%|█████████ | 9465/10440 [21:31<02:11,  7.44it/s][NeMo W 2025-12-14 09:46:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.37it/s]\n",
      " 91%|█████████ | 9466/10440 [21:31<02:07,  7.63it/s][NeMo W 2025-12-14 09:46:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.88it/s]\n",
      " 91%|█████████ | 9467/10440 [21:32<02:19,  6.96it/s][NeMo W 2025-12-14 09:46:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.29it/s]\n",
      " 91%|█████████ | 9468/10440 [21:32<02:15,  7.17it/s][NeMo W 2025-12-14 09:46:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.32it/s]\n",
      " 91%|█████████ | 9469/10440 [21:32<02:14,  7.21it/s][NeMo W 2025-12-14 09:46:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.75it/s]\n",
      " 91%|█████████ | 9470/10440 [21:32<02:15,  7.17it/s][NeMo W 2025-12-14 09:46:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.51it/s]\n",
      " 91%|█████████ | 9471/10440 [21:32<02:15,  7.17it/s][NeMo W 2025-12-14 09:46:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.95it/s]\n",
      " 91%|█████████ | 9472/10440 [21:32<02:16,  7.11it/s][NeMo W 2025-12-14 09:46:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.61it/s]\n",
      " 91%|█████████ | 9473/10440 [21:32<02:15,  7.14it/s][NeMo W 2025-12-14 09:46:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.96it/s]\n",
      " 91%|█████████ | 9474/10440 [21:33<02:25,  6.65it/s][NeMo W 2025-12-14 09:46:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.42it/s]\n",
      " 91%|█████████ | 9475/10440 [21:33<02:28,  6.52it/s][NeMo W 2025-12-14 09:46:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.34it/s]\n",
      " 91%|█████████ | 9476/10440 [21:33<02:23,  6.71it/s][NeMo W 2025-12-14 09:46:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.34it/s]\n",
      " 91%|█████████ | 9477/10440 [21:33<02:21,  6.82it/s][NeMo W 2025-12-14 09:46:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.38it/s]\n",
      " 91%|█████████ | 9478/10440 [21:33<02:11,  7.33it/s][NeMo W 2025-12-14 09:46:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.39it/s]\n",
      " 91%|█████████ | 9479/10440 [21:33<02:06,  7.57it/s][NeMo W 2025-12-14 09:46:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.42it/s]\n",
      " 91%|█████████ | 9480/10440 [21:33<02:05,  7.63it/s][NeMo W 2025-12-14 09:46:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.03it/s]\n",
      " 91%|█████████ | 9481/10440 [21:34<02:10,  7.35it/s][NeMo W 2025-12-14 09:46:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.51it/s]\n",
      " 91%|█████████ | 9482/10440 [21:34<02:11,  7.29it/s][NeMo W 2025-12-14 09:46:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.21it/s]\n",
      " 91%|█████████ | 9483/10440 [21:34<02:21,  6.77it/s][NeMo W 2025-12-14 09:46:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.45it/s]\n",
      " 91%|█████████ | 9484/10440 [21:34<02:13,  7.19it/s][NeMo W 2025-12-14 09:46:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.97it/s]\n",
      " 91%|█████████ | 9485/10440 [21:34<02:14,  7.08it/s][NeMo W 2025-12-14 09:46:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.30it/s]\n",
      " 91%|█████████ | 9486/10440 [21:34<02:09,  7.38it/s][NeMo W 2025-12-14 09:46:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.80it/s]\n",
      " 91%|█████████ | 9487/10440 [21:34<02:09,  7.38it/s][NeMo W 2025-12-14 09:46:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.53it/s]\n",
      " 91%|█████████ | 9488/10440 [21:35<02:08,  7.44it/s][NeMo W 2025-12-14 09:46:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.82it/s]\n",
      " 91%|█████████ | 9489/10440 [21:35<02:05,  7.55it/s][NeMo W 2025-12-14 09:46:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.68it/s]\n",
      " 91%|█████████ | 9490/10440 [21:35<02:03,  7.68it/s][NeMo W 2025-12-14 09:46:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.24it/s]\n",
      " 91%|█████████ | 9491/10440 [21:35<02:06,  7.51it/s][NeMo W 2025-12-14 09:46:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.17it/s]\n",
      " 91%|█████████ | 9492/10440 [21:35<02:03,  7.70it/s][NeMo W 2025-12-14 09:46:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.62it/s]\n",
      " 91%|█████████ | 9493/10440 [21:35<02:08,  7.35it/s][NeMo W 2025-12-14 09:46:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.98it/s]\n",
      " 91%|█████████ | 9494/10440 [21:35<02:06,  7.50it/s][NeMo W 2025-12-14 09:46:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.89it/s]\n",
      " 91%|█████████ | 9495/10440 [21:35<02:02,  7.71it/s][NeMo W 2025-12-14 09:46:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.10it/s]\n",
      " 91%|█████████ | 9496/10440 [21:36<01:59,  7.90it/s][NeMo W 2025-12-14 09:46:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.82it/s]\n",
      " 91%|█████████ | 9497/10440 [21:36<02:07,  7.37it/s][NeMo W 2025-12-14 09:46:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.45it/s]\n",
      " 91%|█████████ | 9498/10440 [21:36<02:09,  7.25it/s][NeMo W 2025-12-14 09:46:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.30it/s]\n",
      " 91%|█████████ | 9499/10440 [21:36<02:06,  7.44it/s][NeMo W 2025-12-14 09:46:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.93it/s]\n",
      " 91%|█████████ | 9500/10440 [21:36<02:01,  7.76it/s][NeMo W 2025-12-14 09:46:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.14it/s]\n",
      " 91%|█████████ | 9501/10440 [21:36<02:05,  7.51it/s][NeMo W 2025-12-14 09:46:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.12it/s]\n",
      " 91%|█████████ | 9502/10440 [21:36<02:04,  7.53it/s][NeMo W 2025-12-14 09:46:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.06it/s]\n",
      " 91%|█████████ | 9503/10440 [21:37<02:03,  7.59it/s][NeMo W 2025-12-14 09:46:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.14it/s]\n",
      " 91%|█████████ | 9504/10440 [21:37<02:09,  7.23it/s][NeMo W 2025-12-14 09:46:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.42it/s]\n",
      " 91%|█████████ | 9505/10440 [21:37<02:05,  7.44it/s][NeMo W 2025-12-14 09:46:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.90it/s]\n",
      " 91%|█████████ | 9506/10440 [21:37<02:04,  7.53it/s][NeMo W 2025-12-14 09:46:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.57it/s]\n",
      " 91%|█████████ | 9507/10440 [21:37<02:00,  7.73it/s][NeMo W 2025-12-14 09:46:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.21it/s]\n",
      " 91%|█████████ | 9508/10440 [21:37<02:00,  7.76it/s][NeMo W 2025-12-14 09:46:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.35it/s]\n",
      " 91%|█████████ | 9509/10440 [21:37<01:59,  7.79it/s][NeMo W 2025-12-14 09:46:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.56it/s]\n",
      " 91%|█████████ | 9510/10440 [21:37<01:58,  7.83it/s][NeMo W 2025-12-14 09:46:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.64it/s]\n",
      " 91%|█████████ | 9511/10440 [21:38<02:01,  7.66it/s][NeMo W 2025-12-14 09:46:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.57it/s]\n",
      " 91%|█████████ | 9512/10440 [21:38<01:57,  7.90it/s][NeMo W 2025-12-14 09:46:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.09it/s]\n",
      " 91%|█████████ | 9513/10440 [21:38<02:08,  7.19it/s][NeMo W 2025-12-14 09:46:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.25it/s]\n",
      " 91%|█████████ | 9514/10440 [21:38<02:10,  7.08it/s][NeMo W 2025-12-14 09:46:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.33it/s]\n",
      " 91%|█████████ | 9515/10440 [21:38<02:07,  7.27it/s][NeMo W 2025-12-14 09:46:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.91it/s]\n",
      " 91%|█████████ | 9516/10440 [21:38<01:58,  7.78it/s][NeMo W 2025-12-14 09:46:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.67it/s]\n",
      " 91%|█████████ | 9517/10440 [21:38<02:05,  7.38it/s][NeMo W 2025-12-14 09:46:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.52it/s]\n",
      " 91%|█████████ | 9518/10440 [21:39<02:06,  7.30it/s][NeMo W 2025-12-14 09:46:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.03it/s]\n",
      " 91%|█████████ | 9519/10440 [21:39<02:02,  7.49it/s][NeMo W 2025-12-14 09:46:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.61it/s]\n",
      " 91%|█████████ | 9520/10440 [21:39<02:08,  7.16it/s][NeMo W 2025-12-14 09:46:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.50it/s]\n",
      " 91%|█████████ | 9521/10440 [21:39<02:15,  6.79it/s][NeMo W 2025-12-14 09:46:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.11it/s]\n",
      " 91%|█████████ | 9522/10440 [21:39<02:08,  7.16it/s][NeMo W 2025-12-14 09:46:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.75it/s]\n",
      " 91%|█████████ | 9523/10440 [21:39<02:04,  7.38it/s][NeMo W 2025-12-14 09:46:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.92it/s]\n",
      " 91%|█████████ | 9524/10440 [21:39<01:59,  7.66it/s][NeMo W 2025-12-14 09:46:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.59it/s]\n",
      " 91%|█████████ | 9525/10440 [21:39<01:59,  7.63it/s][NeMo W 2025-12-14 09:46:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.80it/s]\n",
      " 91%|█████████ | 9526/10440 [21:40<01:57,  7.76it/s][NeMo W 2025-12-14 09:46:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.31it/s]\n",
      " 91%|█████████▏| 9527/10440 [21:40<01:54,  7.98it/s][NeMo W 2025-12-14 09:46:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.55it/s]\n",
      " 91%|█████████▏| 9528/10440 [21:40<01:53,  8.04it/s][NeMo W 2025-12-14 09:46:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.72it/s]\n",
      " 91%|█████████▏| 9529/10440 [21:40<01:58,  7.67it/s][NeMo W 2025-12-14 09:46:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.35it/s]\n",
      " 91%|█████████▏| 9530/10440 [21:40<01:59,  7.63it/s][NeMo W 2025-12-14 09:46:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.83it/s]\n",
      " 91%|█████████▏| 9531/10440 [21:40<02:02,  7.42it/s][NeMo W 2025-12-14 09:46:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.63it/s]\n",
      " 91%|█████████▏| 9532/10440 [21:40<02:11,  6.89it/s][NeMo W 2025-12-14 09:46:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.76it/s]\n",
      " 91%|█████████▏| 9533/10440 [21:41<02:17,  6.60it/s][NeMo W 2025-12-14 09:46:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.05it/s]\n",
      " 91%|█████████▏| 9534/10440 [21:41<02:21,  6.40it/s][NeMo W 2025-12-14 09:46:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.94it/s]\n",
      " 91%|█████████▏| 9535/10440 [21:41<02:24,  6.28it/s][NeMo W 2025-12-14 09:46:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.07it/s]\n",
      " 91%|█████████▏| 9536/10440 [21:41<02:19,  6.47it/s][NeMo W 2025-12-14 09:46:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.08it/s]\n",
      " 91%|█████████▏| 9537/10440 [21:41<02:17,  6.57it/s][NeMo W 2025-12-14 09:46:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.77it/s]\n",
      " 91%|█████████▏| 9538/10440 [21:41<02:17,  6.56it/s][NeMo W 2025-12-14 09:46:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.74it/s]\n",
      " 91%|█████████▏| 9539/10440 [21:42<02:20,  6.43it/s][NeMo W 2025-12-14 09:46:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.76it/s]\n",
      " 91%|█████████▏| 9540/10440 [21:42<02:12,  6.78it/s][NeMo W 2025-12-14 09:46:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.28it/s]\n",
      " 91%|█████████▏| 9541/10440 [21:42<02:13,  6.72it/s][NeMo W 2025-12-14 09:46:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.26it/s]\n",
      " 91%|█████████▏| 9542/10440 [21:42<02:14,  6.67it/s][NeMo W 2025-12-14 09:46:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.05it/s]\n",
      " 91%|█████████▏| 9543/10440 [21:42<02:11,  6.81it/s][NeMo W 2025-12-14 09:46:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.00it/s]\n",
      " 91%|█████████▏| 9544/10440 [21:42<02:05,  7.16it/s][NeMo W 2025-12-14 09:46:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.63it/s]\n",
      " 91%|█████████▏| 9545/10440 [21:42<02:13,  6.70it/s][NeMo W 2025-12-14 09:46:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.01it/s]\n",
      " 91%|█████████▏| 9546/10440 [21:43<02:15,  6.59it/s][NeMo W 2025-12-14 09:46:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.91it/s]\n",
      " 91%|█████████▏| 9547/10440 [21:43<02:16,  6.55it/s][NeMo W 2025-12-14 09:46:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.48it/s]\n",
      " 91%|█████████▏| 9548/10440 [21:43<02:13,  6.67it/s][NeMo W 2025-12-14 09:46:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.84it/s]\n",
      " 91%|█████████▏| 9549/10440 [21:43<02:08,  6.91it/s][NeMo W 2025-12-14 09:46:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.96it/s]\n",
      " 91%|█████████▏| 9550/10440 [21:43<02:19,  6.37it/s][NeMo W 2025-12-14 09:46:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.30it/s]\n",
      " 91%|█████████▏| 9551/10440 [21:43<02:24,  6.17it/s][NeMo W 2025-12-14 09:46:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.09it/s]\n",
      " 91%|█████████▏| 9552/10440 [21:44<02:17,  6.44it/s][NeMo W 2025-12-14 09:46:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.06it/s]\n",
      " 92%|█████████▏| 9553/10440 [21:44<02:17,  6.45it/s][NeMo W 2025-12-14 09:46:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.31it/s]\n",
      " 92%|█████████▏| 9554/10440 [21:44<02:17,  6.43it/s][NeMo W 2025-12-14 09:46:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.43it/s]\n",
      " 92%|█████████▏| 9555/10440 [21:44<02:19,  6.35it/s][NeMo W 2025-12-14 09:46:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.83it/s]\n",
      " 92%|█████████▏| 9556/10440 [21:44<02:17,  6.42it/s][NeMo W 2025-12-14 09:46:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.00it/s]\n",
      " 92%|█████████▏| 9557/10440 [21:44<02:15,  6.52it/s][NeMo W 2025-12-14 09:46:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.98it/s]\n",
      " 92%|█████████▏| 9558/10440 [21:44<02:15,  6.53it/s][NeMo W 2025-12-14 09:46:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.81it/s]\n",
      " 92%|█████████▏| 9559/10440 [21:45<02:08,  6.85it/s][NeMo W 2025-12-14 09:46:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.89it/s]\n",
      " 92%|█████████▏| 9560/10440 [21:45<02:00,  7.31it/s][NeMo W 2025-12-14 09:46:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.34it/s]\n",
      " 92%|█████████▏| 9561/10440 [21:45<02:00,  7.28it/s][NeMo W 2025-12-14 09:46:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.88it/s]\n",
      " 92%|█████████▏| 9562/10440 [21:45<02:00,  7.29it/s][NeMo W 2025-12-14 09:46:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.32it/s]\n",
      " 92%|█████████▏| 9563/10440 [21:45<01:54,  7.65it/s][NeMo W 2025-12-14 09:46:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.54it/s]\n",
      " 92%|█████████▏| 9564/10440 [21:45<01:56,  7.50it/s][NeMo W 2025-12-14 09:46:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.27it/s]\n",
      " 92%|█████████▏| 9565/10440 [21:45<01:55,  7.58it/s][NeMo W 2025-12-14 09:46:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.61it/s]\n",
      " 92%|█████████▏| 9566/10440 [21:45<01:53,  7.73it/s][NeMo W 2025-12-14 09:46:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.56it/s]\n",
      " 92%|█████████▏| 9567/10440 [21:46<01:50,  7.93it/s][NeMo W 2025-12-14 09:46:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.48it/s]\n",
      " 92%|█████████▏| 9568/10440 [21:46<01:54,  7.63it/s][NeMo W 2025-12-14 09:46:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.34it/s]\n",
      " 92%|█████████▏| 9569/10440 [21:46<01:54,  7.58it/s][NeMo W 2025-12-14 09:46:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.47it/s]\n",
      " 92%|█████████▏| 9570/10440 [21:46<01:53,  7.63it/s][NeMo W 2025-12-14 09:46:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.44it/s]\n",
      " 92%|█████████▏| 9571/10440 [21:46<01:50,  7.84it/s][NeMo W 2025-12-14 09:46:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.69it/s]\n",
      " 92%|█████████▏| 9572/10440 [21:46<01:46,  8.15it/s][NeMo W 2025-12-14 09:46:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.85it/s]\n",
      " 92%|█████████▏| 9573/10440 [21:46<01:45,  8.24it/s][NeMo W 2025-12-14 09:46:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.61it/s]\n",
      " 92%|█████████▏| 9574/10440 [21:46<01:42,  8.42it/s][NeMo W 2025-12-14 09:46:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.87it/s]\n",
      " 92%|█████████▏| 9575/10440 [21:47<01:42,  8.44it/s][NeMo W 2025-12-14 09:46:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.99it/s]\n",
      " 92%|█████████▏| 9576/10440 [21:47<01:51,  7.78it/s][NeMo W 2025-12-14 09:46:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.19it/s]\n",
      " 92%|█████████▏| 9577/10440 [21:47<01:56,  7.41it/s][NeMo W 2025-12-14 09:46:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.09it/s]\n",
      " 92%|█████████▏| 9578/10440 [21:47<01:56,  7.40it/s][NeMo W 2025-12-14 09:46:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.13it/s]\n",
      " 92%|█████████▏| 9579/10440 [21:47<01:52,  7.66it/s][NeMo W 2025-12-14 09:46:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.10it/s]\n",
      " 92%|█████████▏| 9580/10440 [21:47<01:51,  7.73it/s][NeMo W 2025-12-14 09:46:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.38it/s]\n",
      " 92%|█████████▏| 9581/10440 [21:47<01:51,  7.67it/s][NeMo W 2025-12-14 09:46:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.76it/s]\n",
      " 92%|█████████▏| 9582/10440 [21:48<01:57,  7.28it/s][NeMo W 2025-12-14 09:46:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.81it/s]\n",
      " 92%|█████████▏| 9583/10440 [21:48<02:04,  6.87it/s][NeMo W 2025-12-14 09:46:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.70it/s]\n",
      " 92%|█████████▏| 9584/10440 [21:48<02:03,  6.95it/s][NeMo W 2025-12-14 09:46:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.47it/s]\n",
      " 92%|█████████▏| 9585/10440 [21:48<01:55,  7.42it/s][NeMo W 2025-12-14 09:46:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.17it/s]\n",
      " 92%|█████████▏| 9586/10440 [21:48<01:55,  7.41it/s][NeMo W 2025-12-14 09:46:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.15it/s]\n",
      " 92%|█████████▏| 9587/10440 [21:48<01:56,  7.34it/s][NeMo W 2025-12-14 09:46:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.78it/s]\n",
      " 92%|█████████▏| 9588/10440 [21:48<01:54,  7.43it/s][NeMo W 2025-12-14 09:46:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.08it/s]\n",
      " 92%|█████████▏| 9589/10440 [21:48<01:54,  7.44it/s][NeMo W 2025-12-14 09:46:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  7.90it/s]\n",
      " 92%|█████████▏| 9590/10440 [21:49<02:12,  6.42it/s][NeMo W 2025-12-14 09:46:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.40it/s]\n",
      " 92%|█████████▏| 9591/10440 [21:49<02:10,  6.53it/s][NeMo W 2025-12-14 09:46:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.72it/s]\n",
      " 92%|█████████▏| 9592/10440 [21:49<02:00,  7.01it/s][NeMo W 2025-12-14 09:46:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.42it/s]\n",
      " 92%|█████████▏| 9593/10440 [21:49<01:56,  7.26it/s][NeMo W 2025-12-14 09:46:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.00it/s]\n",
      " 92%|█████████▏| 9594/10440 [21:49<01:49,  7.75it/s][NeMo W 2025-12-14 09:46:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.41it/s]\n",
      " 92%|█████████▏| 9595/10440 [21:49<01:46,  7.92it/s][NeMo W 2025-12-14 09:46:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.28it/s]\n",
      " 92%|█████████▏| 9596/10440 [21:49<01:46,  7.91it/s][NeMo W 2025-12-14 09:46:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.39it/s]\n",
      " 92%|█████████▏| 9597/10440 [21:50<01:44,  8.04it/s][NeMo W 2025-12-14 09:46:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.80it/s]\n",
      " 92%|█████████▏| 9598/10440 [21:50<01:42,  8.23it/s][NeMo W 2025-12-14 09:46:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.90it/s]\n",
      " 92%|█████████▏| 9599/10440 [21:50<01:43,  8.10it/s][NeMo W 2025-12-14 09:46:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 19.55it/s]\n",
      " 92%|█████████▏| 9600/10440 [21:50<01:39,  8.43it/s][NeMo W 2025-12-14 09:46:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.59it/s]\n",
      " 92%|█████████▏| 9601/10440 [21:50<01:43,  8.12it/s][NeMo W 2025-12-14 09:46:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.60it/s]\n",
      " 92%|█████████▏| 9602/10440 [21:50<01:51,  7.50it/s][NeMo W 2025-12-14 09:46:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.55it/s]\n",
      " 92%|█████████▏| 9603/10440 [21:50<01:50,  7.54it/s][NeMo W 2025-12-14 09:46:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.10it/s]\n",
      " 92%|█████████▏| 9604/10440 [21:50<01:52,  7.45it/s][NeMo W 2025-12-14 09:46:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.09it/s]\n",
      " 92%|█████████▏| 9605/10440 [21:51<01:53,  7.39it/s][NeMo W 2025-12-14 09:46:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.94it/s]\n",
      " 92%|█████████▏| 9606/10440 [21:51<01:51,  7.50it/s][NeMo W 2025-12-14 09:46:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.68it/s]\n",
      " 92%|█████████▏| 9607/10440 [21:51<01:46,  7.82it/s][NeMo W 2025-12-14 09:46:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.49it/s]\n",
      " 92%|█████████▏| 9608/10440 [21:51<01:41,  8.23it/s][NeMo W 2025-12-14 09:46:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:46:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.88it/s]\n",
      " 92%|█████████▏| 9609/10440 [21:51<01:41,  8.19it/s][NeMo W 2025-12-14 09:47:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.33it/s]\n",
      " 92%|█████████▏| 9610/10440 [21:51<01:46,  7.80it/s][NeMo W 2025-12-14 09:47:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.69it/s]\n",
      " 92%|█████████▏| 9611/10440 [21:51<01:45,  7.89it/s][NeMo W 2025-12-14 09:47:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.39it/s]\n",
      " 92%|█████████▏| 9612/10440 [21:51<01:47,  7.69it/s][NeMo W 2025-12-14 09:47:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.40it/s]\n",
      " 92%|█████████▏| 9613/10440 [21:52<01:46,  7.80it/s][NeMo W 2025-12-14 09:47:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.54it/s]\n",
      " 92%|█████████▏| 9614/10440 [21:52<01:44,  7.89it/s][NeMo W 2025-12-14 09:47:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.10it/s]\n",
      " 92%|█████████▏| 9615/10440 [21:52<01:43,  7.95it/s][NeMo W 2025-12-14 09:47:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.29it/s]\n",
      " 92%|█████████▏| 9616/10440 [21:52<01:44,  7.86it/s][NeMo W 2025-12-14 09:47:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.62it/s]\n",
      " 92%|█████████▏| 9617/10440 [21:52<01:47,  7.63it/s][NeMo W 2025-12-14 09:47:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.18it/s]\n",
      " 92%|█████████▏| 9618/10440 [21:52<01:46,  7.73it/s][NeMo W 2025-12-14 09:47:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.36it/s]\n",
      " 92%|█████████▏| 9619/10440 [21:52<01:43,  7.91it/s][NeMo W 2025-12-14 09:47:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.49it/s]\n",
      " 92%|█████████▏| 9620/10440 [21:52<01:41,  8.04it/s][NeMo W 2025-12-14 09:47:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.46it/s]\n",
      " 92%|█████████▏| 9621/10440 [21:53<01:37,  8.37it/s][NeMo W 2025-12-14 09:47:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.64it/s]\n",
      " 92%|█████████▏| 9622/10440 [21:53<01:44,  7.80it/s][NeMo W 2025-12-14 09:47:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.05it/s]\n",
      " 92%|█████████▏| 9623/10440 [21:53<01:46,  7.69it/s][NeMo W 2025-12-14 09:47:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.44it/s]\n",
      " 92%|█████████▏| 9624/10440 [21:53<01:44,  7.79it/s][NeMo W 2025-12-14 09:47:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.87it/s]\n",
      " 92%|█████████▏| 9625/10440 [21:53<01:55,  7.05it/s][NeMo W 2025-12-14 09:47:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.08it/s]\n",
      " 92%|█████████▏| 9626/10440 [21:53<01:52,  7.26it/s][NeMo W 2025-12-14 09:47:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.67it/s]\n",
      " 92%|█████████▏| 9627/10440 [21:53<01:53,  7.15it/s][NeMo W 2025-12-14 09:47:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.10it/s]\n",
      " 92%|█████████▏| 9628/10440 [21:54<01:48,  7.48it/s][NeMo W 2025-12-14 09:47:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.74it/s]\n",
      " 92%|█████████▏| 9629/10440 [21:54<01:46,  7.60it/s][NeMo W 2025-12-14 09:47:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.05it/s]\n",
      " 92%|█████████▏| 9630/10440 [21:54<01:43,  7.83it/s][NeMo W 2025-12-14 09:47:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.24it/s]\n",
      " 92%|█████████▏| 9631/10440 [21:54<01:43,  7.82it/s][NeMo W 2025-12-14 09:47:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.21it/s]\n",
      " 92%|█████████▏| 9632/10440 [21:54<01:46,  7.61it/s][NeMo W 2025-12-14 09:47:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.88it/s]\n",
      " 92%|█████████▏| 9633/10440 [21:54<01:42,  7.90it/s][NeMo W 2025-12-14 09:47:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.55it/s]\n",
      " 92%|█████████▏| 9634/10440 [21:54<01:48,  7.40it/s][NeMo W 2025-12-14 09:47:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.04it/s]\n",
      " 92%|█████████▏| 9635/10440 [21:55<01:52,  7.16it/s][NeMo W 2025-12-14 09:47:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.53it/s]\n",
      " 92%|█████████▏| 9636/10440 [21:55<01:56,  6.90it/s][NeMo W 2025-12-14 09:47:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.94it/s]\n",
      " 92%|█████████▏| 9637/10440 [21:55<02:02,  6.54it/s][NeMo W 2025-12-14 09:47:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.69it/s]\n",
      " 92%|█████████▏| 9638/10440 [21:55<01:59,  6.73it/s][NeMo W 2025-12-14 09:47:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.37it/s]\n",
      " 92%|█████████▏| 9639/10440 [21:55<02:01,  6.61it/s][NeMo W 2025-12-14 09:47:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  8.29it/s]\n",
      " 92%|█████████▏| 9640/10440 [21:55<02:13,  6.01it/s][NeMo W 2025-12-14 09:47:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.16it/s]\n",
      " 92%|█████████▏| 9641/10440 [21:56<02:18,  5.78it/s][NeMo W 2025-12-14 09:47:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.72it/s]\n",
      " 92%|█████████▏| 9642/10440 [21:56<02:10,  6.13it/s][NeMo W 2025-12-14 09:47:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.71it/s]\n",
      " 92%|█████████▏| 9643/10440 [21:56<02:04,  6.41it/s][NeMo W 2025-12-14 09:47:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.59it/s]\n",
      " 92%|█████████▏| 9644/10440 [21:56<01:56,  6.83it/s][NeMo W 2025-12-14 09:47:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.87it/s]\n",
      " 92%|█████████▏| 9645/10440 [21:56<02:00,  6.59it/s][NeMo W 2025-12-14 09:47:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.32it/s]\n",
      " 92%|█████████▏| 9646/10440 [21:56<02:07,  6.21it/s][NeMo W 2025-12-14 09:47:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.77it/s]\n",
      " 92%|█████████▏| 9647/10440 [21:56<02:15,  5.85it/s][NeMo W 2025-12-14 09:47:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.73it/s]\n",
      " 92%|█████████▏| 9648/10440 [21:57<02:09,  6.10it/s][NeMo W 2025-12-14 09:47:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.70it/s]\n",
      " 92%|█████████▏| 9649/10440 [21:57<02:08,  6.14it/s][NeMo W 2025-12-14 09:47:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.09it/s]\n",
      " 92%|█████████▏| 9650/10440 [21:57<02:09,  6.12it/s][NeMo W 2025-12-14 09:47:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.87it/s]\n",
      " 92%|█████████▏| 9651/10440 [21:57<02:07,  6.20it/s][NeMo W 2025-12-14 09:47:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.26it/s]\n",
      " 92%|█████████▏| 9652/10440 [21:57<02:16,  5.79it/s][NeMo W 2025-12-14 09:47:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.53it/s]\n",
      " 92%|█████████▏| 9653/10440 [21:57<02:14,  5.85it/s][NeMo W 2025-12-14 09:47:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.39it/s]\n",
      " 92%|█████████▏| 9654/10440 [21:58<02:16,  5.75it/s][NeMo W 2025-12-14 09:47:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.62it/s]\n",
      " 92%|█████████▏| 9655/10440 [21:58<02:11,  5.96it/s][NeMo W 2025-12-14 09:47:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.47it/s]\n",
      " 92%|█████████▏| 9656/10440 [21:58<02:05,  6.24it/s][NeMo W 2025-12-14 09:47:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.39it/s]\n",
      " 92%|█████████▎| 9657/10440 [21:58<02:05,  6.25it/s][NeMo W 2025-12-14 09:47:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.84it/s]\n",
      " 93%|█████████▎| 9658/10440 [21:58<01:56,  6.71it/s][NeMo W 2025-12-14 09:47:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.33it/s]\n",
      " 93%|█████████▎| 9659/10440 [21:58<01:55,  6.74it/s][NeMo W 2025-12-14 09:47:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.73it/s]\n",
      " 93%|█████████▎| 9660/10440 [21:59<01:51,  7.02it/s][NeMo W 2025-12-14 09:47:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.74it/s]\n",
      " 93%|█████████▎| 9661/10440 [21:59<01:47,  7.26it/s][NeMo W 2025-12-14 09:47:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.43it/s]\n",
      " 93%|█████████▎| 9662/10440 [21:59<01:55,  6.74it/s][NeMo W 2025-12-14 09:47:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.33it/s]\n",
      " 93%|█████████▎| 9663/10440 [21:59<01:49,  7.12it/s][NeMo W 2025-12-14 09:47:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.92it/s]\n",
      " 93%|█████████▎| 9664/10440 [21:59<01:49,  7.09it/s][NeMo W 2025-12-14 09:47:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.61it/s]\n",
      " 93%|█████████▎| 9665/10440 [21:59<01:45,  7.32it/s][NeMo W 2025-12-14 09:47:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.86it/s]\n",
      " 93%|█████████▎| 9666/10440 [21:59<01:48,  7.14it/s][NeMo W 2025-12-14 09:47:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.80it/s]\n",
      " 93%|█████████▎| 9667/10440 [21:59<01:49,  7.03it/s][NeMo W 2025-12-14 09:47:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.40it/s]\n",
      " 93%|█████████▎| 9668/10440 [22:00<01:45,  7.32it/s][NeMo W 2025-12-14 09:47:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.52it/s]\n",
      " 93%|█████████▎| 9669/10440 [22:00<01:49,  7.04it/s][NeMo W 2025-12-14 09:47:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.00it/s]\n",
      " 93%|█████████▎| 9670/10440 [22:00<01:49,  7.04it/s][NeMo W 2025-12-14 09:47:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.61it/s]\n",
      " 93%|█████████▎| 9671/10440 [22:00<01:44,  7.36it/s][NeMo W 2025-12-14 09:47:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.70it/s]\n",
      " 93%|█████████▎| 9672/10440 [22:00<01:41,  7.59it/s][NeMo W 2025-12-14 09:47:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.73it/s]\n",
      " 93%|█████████▎| 9673/10440 [22:00<01:41,  7.59it/s][NeMo W 2025-12-14 09:47:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.91it/s]\n",
      " 93%|█████████▎| 9674/10440 [22:00<01:43,  7.41it/s][NeMo W 2025-12-14 09:47:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.65it/s]\n",
      " 93%|█████████▎| 9675/10440 [22:01<01:41,  7.56it/s][NeMo W 2025-12-14 09:47:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.25it/s]\n",
      " 93%|█████████▎| 9676/10440 [22:01<01:36,  7.89it/s][NeMo W 2025-12-14 09:47:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.15it/s]\n",
      " 93%|█████████▎| 9677/10440 [22:01<01:34,  8.03it/s][NeMo W 2025-12-14 09:47:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.71it/s]\n",
      " 93%|█████████▎| 9678/10440 [22:01<01:40,  7.59it/s][NeMo W 2025-12-14 09:47:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.26it/s]\n",
      " 93%|█████████▎| 9679/10440 [22:01<01:38,  7.76it/s][NeMo W 2025-12-14 09:47:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.96it/s]\n",
      " 93%|█████████▎| 9680/10440 [22:01<01:36,  7.89it/s][NeMo W 2025-12-14 09:47:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.48it/s]\n",
      " 93%|█████████▎| 9681/10440 [22:01<01:36,  7.87it/s][NeMo W 2025-12-14 09:47:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.90it/s]\n",
      " 93%|█████████▎| 9682/10440 [22:01<01:36,  7.84it/s][NeMo W 2025-12-14 09:47:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.94it/s]\n",
      " 93%|█████████▎| 9683/10440 [22:02<01:35,  7.90it/s][NeMo W 2025-12-14 09:47:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.29it/s]\n",
      " 93%|█████████▎| 9684/10440 [22:02<01:36,  7.87it/s][NeMo W 2025-12-14 09:47:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.74it/s]\n",
      " 93%|█████████▎| 9685/10440 [22:02<01:34,  7.98it/s][NeMo W 2025-12-14 09:47:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.88it/s]\n",
      " 93%|█████████▎| 9686/10440 [22:02<01:37,  7.71it/s][NeMo W 2025-12-14 09:47:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.99it/s]\n",
      " 93%|█████████▎| 9687/10440 [22:02<01:37,  7.76it/s][NeMo W 2025-12-14 09:47:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.69it/s]\n",
      " 93%|█████████▎| 9688/10440 [22:02<01:33,  8.06it/s][NeMo W 2025-12-14 09:47:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.21it/s]\n",
      " 93%|█████████▎| 9689/10440 [22:02<01:34,  7.95it/s][NeMo W 2025-12-14 09:47:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.53it/s]\n",
      " 93%|█████████▎| 9690/10440 [22:02<01:31,  8.21it/s][NeMo W 2025-12-14 09:47:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.08it/s]\n",
      " 93%|█████████▎| 9691/10440 [22:03<01:32,  8.11it/s][NeMo W 2025-12-14 09:47:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.04it/s]\n",
      " 93%|█████████▎| 9692/10440 [22:03<01:37,  7.69it/s][NeMo W 2025-12-14 09:47:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.22it/s]\n",
      " 93%|█████████▎| 9693/10440 [22:03<01:34,  7.94it/s][NeMo W 2025-12-14 09:47:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.14it/s]\n",
      " 93%|█████████▎| 9694/10440 [22:03<01:40,  7.39it/s][NeMo W 2025-12-14 09:47:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.20it/s]\n",
      " 93%|█████████▎| 9695/10440 [22:03<01:39,  7.49it/s][NeMo W 2025-12-14 09:47:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.19it/s]\n",
      " 93%|█████████▎| 9696/10440 [22:03<01:38,  7.54it/s][NeMo W 2025-12-14 09:47:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.55it/s]\n",
      " 93%|█████████▎| 9697/10440 [22:03<01:37,  7.63it/s][NeMo W 2025-12-14 09:47:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.07it/s]\n",
      " 93%|█████████▎| 9698/10440 [22:04<01:41,  7.33it/s][NeMo W 2025-12-14 09:47:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.76it/s]\n",
      " 93%|█████████▎| 9699/10440 [22:04<01:38,  7.54it/s][NeMo W 2025-12-14 09:47:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.15it/s]\n",
      " 93%|█████████▎| 9700/10440 [22:04<01:38,  7.55it/s][NeMo W 2025-12-14 09:47:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.98it/s]\n",
      " 93%|█████████▎| 9701/10440 [22:04<01:42,  7.24it/s][NeMo W 2025-12-14 09:47:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.10it/s]\n",
      " 93%|█████████▎| 9702/10440 [22:04<01:43,  7.15it/s][NeMo W 2025-12-14 09:47:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.44it/s]\n",
      " 93%|█████████▎| 9703/10440 [22:04<01:46,  6.94it/s][NeMo W 2025-12-14 09:47:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.27it/s]\n",
      " 93%|█████████▎| 9704/10440 [22:04<01:41,  7.25it/s][NeMo W 2025-12-14 09:47:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.83it/s]\n",
      " 93%|█████████▎| 9705/10440 [22:04<01:37,  7.53it/s][NeMo W 2025-12-14 09:47:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.66it/s]\n",
      " 93%|█████████▎| 9706/10440 [22:05<01:39,  7.38it/s][NeMo W 2025-12-14 09:47:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.19it/s]\n",
      " 93%|█████████▎| 9707/10440 [22:05<01:37,  7.51it/s][NeMo W 2025-12-14 09:47:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.04it/s]\n",
      " 93%|█████████▎| 9708/10440 [22:05<01:38,  7.43it/s][NeMo W 2025-12-14 09:47:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.16it/s]\n",
      " 93%|█████████▎| 9709/10440 [22:05<01:40,  7.28it/s][NeMo W 2025-12-14 09:47:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.63it/s]\n",
      " 93%|█████████▎| 9710/10440 [22:05<01:48,  6.71it/s][NeMo W 2025-12-14 09:47:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.16it/s]\n",
      " 93%|█████████▎| 9711/10440 [22:05<01:43,  7.02it/s][NeMo W 2025-12-14 09:47:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.37it/s]\n",
      " 93%|█████████▎| 9712/10440 [22:05<01:39,  7.33it/s][NeMo W 2025-12-14 09:47:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.69it/s]\n",
      " 93%|█████████▎| 9713/10440 [22:06<01:34,  7.71it/s][NeMo W 2025-12-14 09:47:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.23it/s]\n",
      " 93%|█████████▎| 9714/10440 [22:06<01:31,  7.96it/s][NeMo W 2025-12-14 09:47:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.85it/s]\n",
      " 93%|█████████▎| 9715/10440 [22:06<01:29,  8.14it/s][NeMo W 2025-12-14 09:47:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.90it/s]\n",
      " 93%|█████████▎| 9716/10440 [22:06<01:32,  7.79it/s][NeMo W 2025-12-14 09:47:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.24it/s]\n",
      " 93%|█████████▎| 9717/10440 [22:06<01:31,  7.89it/s][NeMo W 2025-12-14 09:47:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.36it/s]\n",
      " 93%|█████████▎| 9718/10440 [22:06<01:30,  7.98it/s][NeMo W 2025-12-14 09:47:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.81it/s]\n",
      " 93%|█████████▎| 9719/10440 [22:06<01:37,  7.42it/s][NeMo W 2025-12-14 09:47:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.04it/s]\n",
      " 93%|█████████▎| 9720/10440 [22:06<01:34,  7.65it/s][NeMo W 2025-12-14 09:47:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.38it/s]\n",
      " 93%|█████████▎| 9721/10440 [22:07<01:33,  7.65it/s][NeMo W 2025-12-14 09:47:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.10it/s]\n",
      " 93%|█████████▎| 9722/10440 [22:07<01:32,  7.73it/s][NeMo W 2025-12-14 09:47:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.10it/s]\n",
      " 93%|█████████▎| 9723/10440 [22:07<01:31,  7.83it/s][NeMo W 2025-12-14 09:47:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.93it/s]\n",
      " 93%|█████████▎| 9724/10440 [22:07<01:29,  8.04it/s][NeMo W 2025-12-14 09:47:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.25it/s]\n",
      " 93%|█████████▎| 9725/10440 [22:07<01:25,  8.35it/s][NeMo W 2025-12-14 09:47:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.42it/s]\n",
      " 93%|█████████▎| 9726/10440 [22:07<01:26,  8.30it/s][NeMo W 2025-12-14 09:47:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.50it/s]\n",
      " 93%|█████████▎| 9727/10440 [22:07<01:39,  7.15it/s][NeMo W 2025-12-14 09:47:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.49it/s]\n",
      " 93%|█████████▎| 9728/10440 [22:07<01:38,  7.20it/s][NeMo W 2025-12-14 09:47:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.37it/s]\n",
      " 93%|█████████▎| 9729/10440 [22:08<01:36,  7.37it/s][NeMo W 2025-12-14 09:47:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.79it/s]\n",
      " 93%|█████████▎| 9730/10440 [22:08<01:34,  7.55it/s][NeMo W 2025-12-14 09:47:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.40it/s]\n",
      " 93%|█████████▎| 9731/10440 [22:08<01:31,  7.78it/s][NeMo W 2025-12-14 09:47:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.45it/s]\n",
      " 93%|█████████▎| 9732/10440 [22:08<01:29,  7.88it/s][NeMo W 2025-12-14 09:47:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.60it/s]\n",
      " 93%|█████████▎| 9733/10440 [22:08<01:35,  7.38it/s][NeMo W 2025-12-14 09:47:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.96it/s]\n",
      " 93%|█████████▎| 9734/10440 [22:08<01:38,  7.15it/s][NeMo W 2025-12-14 09:47:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.53it/s]\n",
      " 93%|█████████▎| 9735/10440 [22:08<01:41,  6.92it/s][NeMo W 2025-12-14 09:47:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.56it/s]\n",
      " 93%|█████████▎| 9736/10440 [22:09<01:43,  6.81it/s][NeMo W 2025-12-14 09:47:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.22it/s]\n",
      " 93%|█████████▎| 9737/10440 [22:09<01:56,  6.05it/s][NeMo W 2025-12-14 09:47:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.50it/s]\n",
      " 93%|█████████▎| 9738/10440 [22:09<02:04,  5.66it/s][NeMo W 2025-12-14 09:47:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.32it/s]\n",
      " 93%|█████████▎| 9739/10440 [22:09<01:58,  5.92it/s][NeMo W 2025-12-14 09:47:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.80it/s]\n",
      " 93%|█████████▎| 9740/10440 [22:09<01:55,  6.08it/s][NeMo W 2025-12-14 09:47:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.39it/s]\n",
      " 93%|█████████▎| 9741/10440 [22:09<01:49,  6.36it/s][NeMo W 2025-12-14 09:47:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.93it/s]\n",
      " 93%|█████████▎| 9742/10440 [22:10<01:49,  6.35it/s][NeMo W 2025-12-14 09:47:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.54it/s]\n",
      " 93%|█████████▎| 9743/10440 [22:10<01:52,  6.19it/s][NeMo W 2025-12-14 09:47:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.44it/s]\n",
      " 93%|█████████▎| 9744/10440 [22:10<01:54,  6.10it/s][NeMo W 2025-12-14 09:47:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.20it/s]\n",
      " 93%|█████████▎| 9745/10440 [22:10<01:52,  6.20it/s][NeMo W 2025-12-14 09:47:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.50it/s]\n",
      " 93%|█████████▎| 9746/10440 [22:10<01:49,  6.35it/s][NeMo W 2025-12-14 09:47:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.15it/s]\n",
      " 93%|█████████▎| 9747/10440 [22:10<01:46,  6.53it/s][NeMo W 2025-12-14 09:47:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.36it/s]\n",
      " 93%|█████████▎| 9748/10440 [22:11<01:42,  6.74it/s][NeMo W 2025-12-14 09:47:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.86it/s]\n",
      " 93%|█████████▎| 9749/10440 [22:11<01:40,  6.90it/s][NeMo W 2025-12-14 09:47:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.89it/s]\n",
      " 93%|█████████▎| 9750/10440 [22:11<01:38,  7.00it/s][NeMo W 2025-12-14 09:47:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.25it/s]\n",
      " 93%|█████████▎| 9751/10440 [22:11<01:43,  6.63it/s][NeMo W 2025-12-14 09:47:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.06it/s]\n",
      " 93%|█████████▎| 9752/10440 [22:11<01:44,  6.60it/s][NeMo W 2025-12-14 09:47:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.46it/s]\n",
      " 93%|█████████▎| 9753/10440 [22:11<01:47,  6.39it/s][NeMo W 2025-12-14 09:47:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.10it/s]\n",
      " 93%|█████████▎| 9754/10440 [22:12<01:54,  6.00it/s][NeMo W 2025-12-14 09:47:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  8.86it/s]\n",
      " 93%|█████████▎| 9755/10440 [22:12<02:03,  5.54it/s][NeMo W 2025-12-14 09:47:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.03it/s]\n",
      " 93%|█████████▎| 9756/10440 [22:12<01:59,  5.72it/s][NeMo W 2025-12-14 09:47:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.80it/s]\n",
      " 93%|█████████▎| 9757/10440 [22:12<01:51,  6.12it/s][NeMo W 2025-12-14 09:47:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.48it/s]\n",
      " 93%|█████████▎| 9758/10440 [22:12<01:53,  6.02it/s][NeMo W 2025-12-14 09:47:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.37it/s]\n",
      " 93%|█████████▎| 9759/10440 [22:12<01:45,  6.44it/s][NeMo W 2025-12-14 09:47:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.06it/s]\n",
      " 93%|█████████▎| 9760/10440 [22:12<01:37,  6.95it/s][NeMo W 2025-12-14 09:47:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.28it/s]\n",
      " 93%|█████████▎| 9761/10440 [22:13<01:31,  7.40it/s][NeMo W 2025-12-14 09:47:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.14it/s]\n",
      " 94%|█████████▎| 9762/10440 [22:13<01:26,  7.80it/s][NeMo W 2025-12-14 09:47:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.71it/s]\n",
      " 94%|█████████▎| 9763/10440 [22:13<01:27,  7.76it/s][NeMo W 2025-12-14 09:47:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.55it/s]\n",
      " 94%|█████████▎| 9764/10440 [22:13<01:27,  7.74it/s][NeMo W 2025-12-14 09:47:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.89it/s]\n",
      " 94%|█████████▎| 9765/10440 [22:13<01:23,  8.05it/s][NeMo W 2025-12-14 09:47:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.50it/s]\n",
      " 94%|█████████▎| 9766/10440 [22:13<01:20,  8.36it/s][NeMo W 2025-12-14 09:47:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.60it/s]\n",
      " 94%|█████████▎| 9767/10440 [22:13<01:26,  7.74it/s][NeMo W 2025-12-14 09:47:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.07it/s]\n",
      " 94%|█████████▎| 9768/10440 [22:13<01:27,  7.70it/s][NeMo W 2025-12-14 09:47:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.52it/s]\n",
      " 94%|█████████▎| 9769/10440 [22:14<01:24,  7.92it/s][NeMo W 2025-12-14 09:47:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.03it/s]\n",
      " 94%|█████████▎| 9770/10440 [22:14<01:22,  8.14it/s][NeMo W 2025-12-14 09:47:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.34it/s]\n",
      " 94%|█████████▎| 9771/10440 [22:14<01:26,  7.74it/s][NeMo W 2025-12-14 09:47:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.91it/s]\n",
      " 94%|█████████▎| 9772/10440 [22:14<01:28,  7.51it/s][NeMo W 2025-12-14 09:47:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.18it/s]\n",
      " 94%|█████████▎| 9773/10440 [22:14<01:25,  7.79it/s][NeMo W 2025-12-14 09:47:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.85it/s]\n",
      " 94%|█████████▎| 9774/10440 [22:14<01:25,  7.81it/s][NeMo W 2025-12-14 09:47:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.52it/s]\n",
      " 94%|█████████▎| 9775/10440 [22:14<01:32,  7.17it/s][NeMo W 2025-12-14 09:47:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.22it/s]\n",
      " 94%|█████████▎| 9776/10440 [22:14<01:28,  7.52it/s][NeMo W 2025-12-14 09:47:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.74it/s]\n",
      " 94%|█████████▎| 9777/10440 [22:15<01:24,  7.82it/s][NeMo W 2025-12-14 09:47:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.89it/s]\n",
      " 94%|█████████▎| 9778/10440 [22:15<01:23,  7.93it/s][NeMo W 2025-12-14 09:47:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.62it/s]\n",
      " 94%|█████████▎| 9779/10440 [22:15<01:20,  8.18it/s][NeMo W 2025-12-14 09:47:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.16it/s]\n",
      " 94%|█████████▎| 9780/10440 [22:15<01:18,  8.37it/s][NeMo W 2025-12-14 09:47:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.22it/s]\n",
      " 94%|█████████▎| 9781/10440 [22:15<01:21,  8.13it/s][NeMo W 2025-12-14 09:47:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.98it/s]\n",
      " 94%|█████████▎| 9782/10440 [22:15<01:20,  8.19it/s][NeMo W 2025-12-14 09:47:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.46it/s]\n",
      " 94%|█████████▎| 9783/10440 [22:15<01:24,  7.81it/s][NeMo W 2025-12-14 09:47:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.22it/s]\n",
      " 94%|█████████▎| 9784/10440 [22:15<01:28,  7.41it/s][NeMo W 2025-12-14 09:47:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.77it/s]\n",
      " 94%|█████████▎| 9785/10440 [22:16<01:31,  7.18it/s][NeMo W 2025-12-14 09:47:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.37it/s]\n",
      " 94%|█████████▎| 9786/10440 [22:16<01:30,  7.22it/s][NeMo W 2025-12-14 09:47:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.33it/s]\n",
      " 94%|█████████▎| 9787/10440 [22:16<01:34,  6.95it/s][NeMo W 2025-12-14 09:47:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.89it/s]\n",
      " 94%|█████████▍| 9788/10440 [22:16<01:28,  7.37it/s][NeMo W 2025-12-14 09:47:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.85it/s]\n",
      " 94%|█████████▍| 9789/10440 [22:16<01:30,  7.22it/s][NeMo W 2025-12-14 09:47:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.87it/s]\n",
      " 94%|█████████▍| 9790/10440 [22:16<01:26,  7.48it/s][NeMo W 2025-12-14 09:47:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.10it/s]\n",
      " 94%|█████████▍| 9791/10440 [22:16<01:23,  7.76it/s][NeMo W 2025-12-14 09:47:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.28it/s]\n",
      " 94%|█████████▍| 9792/10440 [22:17<01:26,  7.48it/s][NeMo W 2025-12-14 09:47:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.48it/s]\n",
      " 94%|█████████▍| 9793/10440 [22:17<01:27,  7.37it/s][NeMo W 2025-12-14 09:47:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.59it/s]\n",
      " 94%|█████████▍| 9794/10440 [22:17<01:29,  7.22it/s][NeMo W 2025-12-14 09:47:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.43it/s]\n",
      " 94%|█████████▍| 9795/10440 [22:17<01:24,  7.62it/s][NeMo W 2025-12-14 09:47:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.74it/s]\n",
      " 94%|█████████▍| 9796/10440 [22:17<01:22,  7.82it/s][NeMo W 2025-12-14 09:47:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.69it/s]\n",
      " 94%|█████████▍| 9797/10440 [22:17<01:27,  7.32it/s][NeMo W 2025-12-14 09:47:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.72it/s]\n",
      " 94%|█████████▍| 9798/10440 [22:17<01:24,  7.55it/s][NeMo W 2025-12-14 09:47:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.27it/s]\n",
      " 94%|█████████▍| 9799/10440 [22:17<01:22,  7.76it/s][NeMo W 2025-12-14 09:47:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.22it/s]\n",
      " 94%|█████████▍| 9800/10440 [22:18<01:23,  7.68it/s][NeMo W 2025-12-14 09:47:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.93it/s]\n",
      " 94%|█████████▍| 9801/10440 [22:18<01:21,  7.82it/s][NeMo W 2025-12-14 09:47:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.31it/s]\n",
      " 94%|█████████▍| 9802/10440 [22:18<01:22,  7.73it/s][NeMo W 2025-12-14 09:47:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.54it/s]\n",
      " 94%|█████████▍| 9803/10440 [22:18<01:23,  7.66it/s][NeMo W 2025-12-14 09:47:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.12it/s]\n",
      " 94%|█████████▍| 9804/10440 [22:18<01:25,  7.46it/s][NeMo W 2025-12-14 09:47:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.24it/s]\n",
      " 94%|█████████▍| 9805/10440 [22:18<01:25,  7.42it/s][NeMo W 2025-12-14 09:47:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.69it/s]\n",
      " 94%|█████████▍| 9806/10440 [22:18<01:23,  7.59it/s][NeMo W 2025-12-14 09:47:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.17it/s]\n",
      " 94%|█████████▍| 9807/10440 [22:19<01:26,  7.31it/s][NeMo W 2025-12-14 09:47:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.89it/s]\n",
      " 94%|█████████▍| 9808/10440 [22:19<01:26,  7.35it/s][NeMo W 2025-12-14 09:47:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.59it/s]\n",
      " 94%|█████████▍| 9809/10440 [22:19<01:23,  7.53it/s][NeMo W 2025-12-14 09:47:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.84it/s]\n",
      " 94%|█████████▍| 9810/10440 [22:19<01:19,  7.92it/s][NeMo W 2025-12-14 09:47:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.90it/s]\n",
      " 94%|█████████▍| 9811/10440 [22:19<01:25,  7.35it/s][NeMo W 2025-12-14 09:47:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.59it/s]\n",
      " 94%|█████████▍| 9812/10440 [22:19<01:27,  7.17it/s][NeMo W 2025-12-14 09:47:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.47it/s]\n",
      " 94%|█████████▍| 9813/10440 [22:19<01:26,  7.28it/s][NeMo W 2025-12-14 09:47:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.27it/s]\n",
      " 94%|█████████▍| 9814/10440 [22:20<01:27,  7.15it/s][NeMo W 2025-12-14 09:47:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.69it/s]\n",
      " 94%|█████████▍| 9815/10440 [22:20<01:25,  7.28it/s][NeMo W 2025-12-14 09:47:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.74it/s]\n",
      " 94%|█████████▍| 9816/10440 [22:20<01:22,  7.60it/s][NeMo W 2025-12-14 09:47:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.40it/s]\n",
      " 94%|█████████▍| 9817/10440 [22:20<01:22,  7.59it/s][NeMo W 2025-12-14 09:47:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.10it/s]\n",
      " 94%|█████████▍| 9818/10440 [22:20<01:19,  7.87it/s][NeMo W 2025-12-14 09:47:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.90it/s]\n",
      " 94%|█████████▍| 9819/10440 [22:20<01:16,  8.08it/s][NeMo W 2025-12-14 09:47:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.50it/s]\n",
      " 94%|█████████▍| 9820/10440 [22:20<01:19,  7.76it/s][NeMo W 2025-12-14 09:47:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.32it/s]\n",
      " 94%|█████████▍| 9821/10440 [22:20<01:25,  7.21it/s][NeMo W 2025-12-14 09:47:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.25it/s]\n",
      " 94%|█████████▍| 9822/10440 [22:21<01:23,  7.44it/s][NeMo W 2025-12-14 09:47:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.40it/s]\n",
      " 94%|█████████▍| 9823/10440 [22:21<01:21,  7.54it/s][NeMo W 2025-12-14 09:47:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.74it/s]\n",
      " 94%|█████████▍| 9824/10440 [22:21<01:20,  7.66it/s][NeMo W 2025-12-14 09:47:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.82it/s]\n",
      " 94%|█████████▍| 9825/10440 [22:21<01:17,  7.90it/s][NeMo W 2025-12-14 09:47:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.24it/s]\n",
      " 94%|█████████▍| 9826/10440 [22:21<01:14,  8.23it/s][NeMo W 2025-12-14 09:47:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.60it/s]\n",
      " 94%|█████████▍| 9827/10440 [22:21<01:15,  8.14it/s][NeMo W 2025-12-14 09:47:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.93it/s]\n",
      " 94%|█████████▍| 9828/10440 [22:21<01:19,  7.74it/s][NeMo W 2025-12-14 09:47:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.51it/s]\n",
      " 94%|█████████▍| 9829/10440 [22:21<01:19,  7.65it/s][NeMo W 2025-12-14 09:47:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.57it/s]\n",
      " 94%|█████████▍| 9830/10440 [22:22<01:21,  7.45it/s][NeMo W 2025-12-14 09:47:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.06it/s]\n",
      " 94%|█████████▍| 9831/10440 [22:22<01:19,  7.61it/s][NeMo W 2025-12-14 09:47:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.96it/s]\n",
      " 94%|█████████▍| 9832/10440 [22:22<01:18,  7.75it/s][NeMo W 2025-12-14 09:47:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.09it/s]\n",
      " 94%|█████████▍| 9833/10440 [22:22<01:24,  7.17it/s][NeMo W 2025-12-14 09:47:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.58it/s]\n",
      " 94%|█████████▍| 9834/10440 [22:22<01:30,  6.66it/s][NeMo W 2025-12-14 09:47:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.41it/s]\n",
      " 94%|█████████▍| 9835/10440 [22:22<01:31,  6.58it/s][NeMo W 2025-12-14 09:47:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.30it/s]\n",
      " 94%|█████████▍| 9836/10440 [22:22<01:32,  6.54it/s][NeMo W 2025-12-14 09:47:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.31it/s]\n",
      " 94%|█████████▍| 9837/10440 [22:23<01:32,  6.50it/s][NeMo W 2025-12-14 09:47:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.63it/s]\n",
      " 94%|█████████▍| 9838/10440 [22:23<01:28,  6.83it/s][NeMo W 2025-12-14 09:47:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.50it/s]\n",
      " 94%|█████████▍| 9839/10440 [22:23<01:26,  6.92it/s][NeMo W 2025-12-14 09:47:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.31it/s]\n",
      " 94%|█████████▍| 9840/10440 [22:23<01:25,  7.02it/s][NeMo W 2025-12-14 09:47:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.15it/s]\n",
      " 94%|█████████▍| 9841/10440 [22:23<01:28,  6.79it/s][NeMo W 2025-12-14 09:47:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.99it/s]\n",
      " 94%|█████████▍| 9842/10440 [22:23<01:33,  6.37it/s][NeMo W 2025-12-14 09:47:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.64it/s]\n",
      " 94%|█████████▍| 9843/10440 [22:24<01:30,  6.59it/s][NeMo W 2025-12-14 09:47:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.08it/s]\n",
      " 94%|█████████▍| 9844/10440 [22:24<01:32,  6.46it/s][NeMo W 2025-12-14 09:47:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.12it/s]\n",
      " 94%|█████████▍| 9845/10440 [22:24<01:34,  6.29it/s][NeMo W 2025-12-14 09:47:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.93it/s]\n",
      " 94%|█████████▍| 9846/10440 [22:24<01:35,  6.24it/s][NeMo W 2025-12-14 09:47:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.90it/s]\n",
      " 94%|█████████▍| 9847/10440 [22:24<01:29,  6.59it/s][NeMo W 2025-12-14 09:47:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.88it/s]\n",
      " 94%|█████████▍| 9848/10440 [22:24<01:28,  6.70it/s][NeMo W 2025-12-14 09:47:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.11it/s]\n",
      " 94%|█████████▍| 9849/10440 [22:24<01:29,  6.63it/s][NeMo W 2025-12-14 09:47:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.66it/s]\n",
      " 94%|█████████▍| 9850/10440 [22:25<01:28,  6.66it/s][NeMo W 2025-12-14 09:47:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.81it/s]\n",
      " 94%|█████████▍| 9851/10440 [22:25<01:28,  6.67it/s][NeMo W 2025-12-14 09:47:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.81it/s]\n",
      " 94%|█████████▍| 9852/10440 [22:25<01:28,  6.67it/s][NeMo W 2025-12-14 09:47:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.74it/s]\n",
      " 94%|█████████▍| 9853/10440 [22:25<01:31,  6.39it/s][NeMo W 2025-12-14 09:47:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.33it/s]\n",
      " 94%|█████████▍| 9854/10440 [22:25<01:28,  6.58it/s][NeMo W 2025-12-14 09:47:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.53it/s]\n",
      " 94%|█████████▍| 9855/10440 [22:25<01:33,  6.28it/s][NeMo W 2025-12-14 09:47:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.85it/s]\n",
      " 94%|█████████▍| 9856/10440 [22:26<01:32,  6.30it/s][NeMo W 2025-12-14 09:47:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.59it/s]\n",
      " 94%|█████████▍| 9857/10440 [22:26<01:29,  6.48it/s][NeMo W 2025-12-14 09:47:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.13it/s]\n",
      " 94%|█████████▍| 9858/10440 [22:26<01:24,  6.88it/s][NeMo W 2025-12-14 09:47:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.86it/s]\n",
      " 94%|█████████▍| 9859/10440 [22:26<01:20,  7.19it/s][NeMo W 2025-12-14 09:47:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.77it/s]\n",
      " 94%|█████████▍| 9860/10440 [22:26<01:22,  7.03it/s][NeMo W 2025-12-14 09:47:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.87it/s]\n",
      " 94%|█████████▍| 9861/10440 [22:26<01:17,  7.50it/s][NeMo W 2025-12-14 09:47:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.64it/s]\n",
      " 94%|█████████▍| 9862/10440 [22:26<01:15,  7.62it/s][NeMo W 2025-12-14 09:47:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.46it/s]\n",
      " 94%|█████████▍| 9863/10440 [22:26<01:13,  7.89it/s][NeMo W 2025-12-14 09:47:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.66it/s]\n",
      " 94%|█████████▍| 9864/10440 [22:27<01:09,  8.26it/s][NeMo W 2025-12-14 09:47:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.00it/s]\n",
      " 94%|█████████▍| 9865/10440 [22:27<01:10,  8.16it/s][NeMo W 2025-12-14 09:47:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.28it/s]\n",
      " 95%|█████████▍| 9866/10440 [22:27<01:09,  8.24it/s][NeMo W 2025-12-14 09:47:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.05it/s]\n",
      " 95%|█████████▍| 9867/10440 [22:27<01:10,  8.12it/s][NeMo W 2025-12-14 09:47:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.28it/s]\n",
      " 95%|█████████▍| 9868/10440 [22:27<01:11,  8.05it/s][NeMo W 2025-12-14 09:47:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.65it/s]\n",
      " 95%|█████████▍| 9869/10440 [22:27<01:14,  7.62it/s][NeMo W 2025-12-14 09:47:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.43it/s]\n",
      " 95%|█████████▍| 9870/10440 [22:27<01:15,  7.59it/s][NeMo W 2025-12-14 09:47:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.33it/s]\n",
      " 95%|█████████▍| 9871/10440 [22:27<01:15,  7.53it/s][NeMo W 2025-12-14 09:47:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.15it/s]\n",
      " 95%|█████████▍| 9872/10440 [22:28<01:15,  7.55it/s][NeMo W 2025-12-14 09:47:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.83it/s]\n",
      " 95%|█████████▍| 9873/10440 [22:28<01:14,  7.62it/s][NeMo W 2025-12-14 09:47:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.30it/s]\n",
      " 95%|█████████▍| 9874/10440 [22:28<01:11,  7.87it/s][NeMo W 2025-12-14 09:47:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.65it/s]\n",
      " 95%|█████████▍| 9875/10440 [22:28<01:11,  7.92it/s][NeMo W 2025-12-14 09:47:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.94it/s]\n",
      " 95%|█████████▍| 9876/10440 [22:28<01:10,  7.97it/s][NeMo W 2025-12-14 09:47:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.60it/s]\n",
      " 95%|█████████▍| 9877/10440 [22:28<01:10,  8.00it/s][NeMo W 2025-12-14 09:47:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.93it/s]\n",
      " 95%|█████████▍| 9878/10440 [22:28<01:09,  8.03it/s][NeMo W 2025-12-14 09:47:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.65it/s]\n",
      " 95%|█████████▍| 9879/10440 [22:28<01:16,  7.38it/s][NeMo W 2025-12-14 09:47:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.32it/s]\n",
      " 95%|█████████▍| 9880/10440 [22:29<01:13,  7.58it/s][NeMo W 2025-12-14 09:47:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.61it/s]\n",
      " 95%|█████████▍| 9881/10440 [22:29<01:11,  7.86it/s][NeMo W 2025-12-14 09:47:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.56it/s]\n",
      " 95%|█████████▍| 9882/10440 [22:29<01:09,  7.99it/s][NeMo W 2025-12-14 09:47:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.17it/s]\n",
      " 95%|█████████▍| 9883/10440 [22:29<01:10,  7.89it/s][NeMo W 2025-12-14 09:47:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.94it/s]\n",
      " 95%|█████████▍| 9884/10440 [22:29<01:13,  7.57it/s][NeMo W 2025-12-14 09:47:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.69it/s]\n",
      " 95%|█████████▍| 9885/10440 [22:29<01:09,  7.99it/s][NeMo W 2025-12-14 09:47:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.44it/s]\n",
      " 95%|█████████▍| 9886/10440 [22:29<01:07,  8.22it/s][NeMo W 2025-12-14 09:47:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.90it/s]\n",
      " 95%|█████████▍| 9887/10440 [22:29<01:04,  8.52it/s][NeMo W 2025-12-14 09:47:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.68it/s]\n",
      " 95%|█████████▍| 9888/10440 [22:30<01:08,  8.01it/s][NeMo W 2025-12-14 09:47:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.03it/s]\n",
      " 95%|█████████▍| 9889/10440 [22:30<01:10,  7.83it/s][NeMo W 2025-12-14 09:47:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.78it/s]\n",
      " 95%|█████████▍| 9890/10440 [22:30<01:07,  8.18it/s][NeMo W 2025-12-14 09:47:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.66it/s]\n",
      " 95%|█████████▍| 9891/10440 [22:30<01:08,  8.00it/s][NeMo W 2025-12-14 09:47:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.03it/s]\n",
      " 95%|█████████▍| 9892/10440 [22:30<01:13,  7.47it/s][NeMo W 2025-12-14 09:47:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.47it/s]\n",
      " 95%|█████████▍| 9893/10440 [22:30<01:12,  7.59it/s][NeMo W 2025-12-14 09:47:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.21it/s]\n",
      " 95%|█████████▍| 9894/10440 [22:30<01:09,  7.80it/s][NeMo W 2025-12-14 09:47:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  1.70it/s]\n",
      " 95%|█████████▍| 9895/10440 [22:31<02:36,  3.47it/s][NeMo W 2025-12-14 09:47:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.95it/s]\n",
      " 95%|█████████▍| 9896/10440 [22:31<02:09,  4.19it/s][NeMo W 2025-12-14 09:47:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.49it/s]\n",
      " 95%|█████████▍| 9897/10440 [22:31<01:52,  4.81it/s][NeMo W 2025-12-14 09:47:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.08it/s]\n",
      " 95%|█████████▍| 9898/10440 [22:31<01:39,  5.45it/s][NeMo W 2025-12-14 09:47:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.55it/s]\n",
      " 95%|█████████▍| 9899/10440 [22:32<01:29,  6.03it/s][NeMo W 2025-12-14 09:47:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.56it/s]\n",
      " 95%|█████████▍| 9900/10440 [22:32<01:29,  6.03it/s][NeMo W 2025-12-14 09:47:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.84it/s]\n",
      " 95%|█████████▍| 9901/10440 [22:32<01:25,  6.32it/s][NeMo W 2025-12-14 09:47:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.26it/s]\n",
      " 95%|█████████▍| 9902/10440 [22:32<01:19,  6.77it/s][NeMo W 2025-12-14 09:47:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.78it/s]\n",
      " 95%|█████████▍| 9903/10440 [22:32<01:16,  6.98it/s][NeMo W 2025-12-14 09:47:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.14it/s]\n",
      " 95%|█████████▍| 9904/10440 [22:32<01:21,  6.61it/s][NeMo W 2025-12-14 09:47:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.85it/s]\n",
      " 95%|█████████▍| 9905/10440 [22:32<01:15,  7.05it/s][NeMo W 2025-12-14 09:47:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.30it/s]\n",
      " 95%|█████████▍| 9906/10440 [22:33<01:10,  7.57it/s][NeMo W 2025-12-14 09:47:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.33it/s]\n",
      " 95%|█████████▍| 9907/10440 [22:33<01:11,  7.47it/s][NeMo W 2025-12-14 09:47:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.30it/s]\n",
      " 95%|█████████▍| 9908/10440 [22:33<01:13,  7.21it/s][NeMo W 2025-12-14 09:47:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 19.60it/s]\n",
      " 95%|█████████▍| 9909/10440 [22:33<01:10,  7.52it/s][NeMo W 2025-12-14 09:47:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.79it/s]\n",
      " 95%|█████████▍| 9910/10440 [22:33<01:07,  7.81it/s][NeMo W 2025-12-14 09:47:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.56it/s]\n",
      " 95%|█████████▍| 9911/10440 [22:33<01:07,  7.86it/s][NeMo W 2025-12-14 09:47:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.58it/s]\n",
      " 95%|█████████▍| 9912/10440 [22:33<01:04,  8.21it/s][NeMo W 2025-12-14 09:47:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.77it/s]\n",
      " 95%|█████████▍| 9913/10440 [22:33<01:06,  7.95it/s][NeMo W 2025-12-14 09:47:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.16it/s]\n",
      " 95%|█████████▍| 9914/10440 [22:34<01:06,  7.87it/s][NeMo W 2025-12-14 09:47:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.92it/s]\n",
      " 95%|█████████▍| 9915/10440 [22:34<01:05,  8.05it/s][NeMo W 2025-12-14 09:47:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.69it/s]\n",
      " 95%|█████████▍| 9916/10440 [22:34<01:16,  6.87it/s][NeMo W 2025-12-14 09:47:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.07it/s]\n",
      " 95%|█████████▍| 9917/10440 [22:34<01:13,  7.15it/s][NeMo W 2025-12-14 09:47:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.80it/s]\n",
      " 95%|█████████▌| 9918/10440 [22:34<01:10,  7.42it/s][NeMo W 2025-12-14 09:47:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.78it/s]\n",
      " 95%|█████████▌| 9919/10440 [22:34<01:15,  6.92it/s][NeMo W 2025-12-14 09:47:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.42it/s]\n",
      " 95%|█████████▌| 9920/10440 [22:34<01:12,  7.17it/s][NeMo W 2025-12-14 09:47:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.64it/s]\n",
      " 95%|█████████▌| 9921/10440 [22:35<01:10,  7.34it/s][NeMo W 2025-12-14 09:47:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.76it/s]\n",
      " 95%|█████████▌| 9922/10440 [22:35<01:08,  7.58it/s][NeMo W 2025-12-14 09:47:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.64it/s]\n",
      " 95%|█████████▌| 9923/10440 [22:35<01:10,  7.36it/s][NeMo W 2025-12-14 09:47:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.31it/s]\n",
      " 95%|█████████▌| 9924/10440 [22:35<01:10,  7.30it/s][NeMo W 2025-12-14 09:47:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.13it/s]\n",
      " 95%|█████████▌| 9925/10440 [22:35<01:10,  7.33it/s][NeMo W 2025-12-14 09:47:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.42it/s]\n",
      " 95%|█████████▌| 9926/10440 [22:35<01:09,  7.43it/s][NeMo W 2025-12-14 09:47:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.41it/s]\n",
      " 95%|█████████▌| 9927/10440 [22:35<01:10,  7.29it/s][NeMo W 2025-12-14 09:47:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.99it/s]\n",
      " 95%|█████████▌| 9928/10440 [22:35<01:08,  7.47it/s][NeMo W 2025-12-14 09:47:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.69it/s]\n",
      " 95%|█████████▌| 9929/10440 [22:36<01:09,  7.30it/s][NeMo W 2025-12-14 09:47:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.57it/s]\n",
      " 95%|█████████▌| 9930/10440 [22:36<01:13,  6.91it/s][NeMo W 2025-12-14 09:47:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.75it/s]\n",
      " 95%|█████████▌| 9931/10440 [22:36<01:10,  7.19it/s][NeMo W 2025-12-14 09:47:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.62it/s]\n",
      " 95%|█████████▌| 9932/10440 [22:36<01:18,  6.47it/s][NeMo W 2025-12-14 09:47:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.93it/s]\n",
      " 95%|█████████▌| 9933/10440 [22:36<01:17,  6.56it/s][NeMo W 2025-12-14 09:47:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.25it/s]\n",
      " 95%|█████████▌| 9934/10440 [22:36<01:15,  6.66it/s][NeMo W 2025-12-14 09:47:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.75it/s]\n",
      " 95%|█████████▌| 9935/10440 [22:37<01:13,  6.84it/s][NeMo W 2025-12-14 09:47:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.55it/s]\n",
      " 95%|█████████▌| 9936/10440 [22:37<01:11,  7.04it/s][NeMo W 2025-12-14 09:47:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.27it/s]\n",
      " 95%|█████████▌| 9937/10440 [22:37<01:14,  6.75it/s][NeMo W 2025-12-14 09:47:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.99it/s]\n",
      " 95%|█████████▌| 9938/10440 [22:37<01:13,  6.82it/s][NeMo W 2025-12-14 09:47:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.72it/s]\n",
      " 95%|█████████▌| 9939/10440 [22:37<01:11,  7.01it/s][NeMo W 2025-12-14 09:47:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.52it/s]\n",
      " 95%|█████████▌| 9940/10440 [22:37<01:09,  7.20it/s][NeMo W 2025-12-14 09:47:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.66it/s]\n",
      " 95%|█████████▌| 9941/10440 [22:37<01:08,  7.23it/s][NeMo W 2025-12-14 09:47:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.93it/s]\n",
      " 95%|█████████▌| 9942/10440 [22:37<01:07,  7.34it/s][NeMo W 2025-12-14 09:47:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.11it/s]\n",
      " 95%|█████████▌| 9943/10440 [22:38<01:08,  7.24it/s][NeMo W 2025-12-14 09:47:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.40it/s]\n",
      " 95%|█████████▌| 9944/10440 [22:38<01:07,  7.38it/s][NeMo W 2025-12-14 09:47:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.76it/s]\n",
      " 95%|█████████▌| 9945/10440 [22:38<01:09,  7.10it/s][NeMo W 2025-12-14 09:47:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.47it/s]\n",
      " 95%|█████████▌| 9946/10440 [22:38<01:11,  6.94it/s][NeMo W 2025-12-14 09:47:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.90it/s]\n",
      " 95%|█████████▌| 9947/10440 [22:38<01:10,  7.01it/s][NeMo W 2025-12-14 09:47:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.22it/s]\n",
      " 95%|█████████▌| 9948/10440 [22:38<01:10,  7.00it/s][NeMo W 2025-12-14 09:47:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.55it/s]\n",
      " 95%|█████████▌| 9949/10440 [22:38<01:10,  6.92it/s][NeMo W 2025-12-14 09:47:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.66it/s]\n",
      " 95%|█████████▌| 9950/10440 [22:39<01:10,  6.95it/s][NeMo W 2025-12-14 09:47:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.17it/s]\n",
      " 95%|█████████▌| 9951/10440 [22:39<01:11,  6.82it/s][NeMo W 2025-12-14 09:47:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.71it/s]\n",
      " 95%|█████████▌| 9952/10440 [22:39<01:12,  6.77it/s][NeMo W 2025-12-14 09:47:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.99it/s]\n",
      " 95%|█████████▌| 9953/10440 [22:39<01:12,  6.72it/s][NeMo W 2025-12-14 09:47:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.59it/s]\n",
      " 95%|█████████▌| 9954/10440 [22:39<01:15,  6.41it/s][NeMo W 2025-12-14 09:47:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.28it/s]\n",
      " 95%|█████████▌| 9955/10440 [22:39<01:12,  6.67it/s][NeMo W 2025-12-14 09:47:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.31it/s]\n",
      " 95%|█████████▌| 9956/10440 [22:40<01:07,  7.13it/s][NeMo W 2025-12-14 09:47:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.88it/s]\n",
      " 95%|█████████▌| 9957/10440 [22:40<01:07,  7.14it/s][NeMo W 2025-12-14 09:47:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.88it/s]\n",
      " 95%|█████████▌| 9958/10440 [22:40<01:07,  7.17it/s][NeMo W 2025-12-14 09:47:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 19.12it/s]\n",
      " 95%|█████████▌| 9959/10440 [22:40<01:03,  7.63it/s][NeMo W 2025-12-14 09:47:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.34it/s]\n",
      " 95%|█████████▌| 9960/10440 [22:40<01:02,  7.74it/s][NeMo W 2025-12-14 09:47:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.62it/s]\n",
      " 95%|█████████▌| 9961/10440 [22:40<01:03,  7.58it/s][NeMo W 2025-12-14 09:47:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.91it/s]\n",
      " 95%|█████████▌| 9962/10440 [22:40<01:02,  7.70it/s][NeMo W 2025-12-14 09:47:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.23it/s]\n",
      " 95%|█████████▌| 9963/10440 [22:40<01:00,  7.91it/s][NeMo W 2025-12-14 09:47:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.18it/s]\n",
      " 95%|█████████▌| 9964/10440 [22:41<00:59,  8.01it/s][NeMo W 2025-12-14 09:47:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.25it/s]\n",
      " 95%|█████████▌| 9965/10440 [22:41<00:59,  8.05it/s][NeMo W 2025-12-14 09:47:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.59it/s]\n",
      " 95%|█████████▌| 9966/10440 [22:41<01:05,  7.29it/s][NeMo W 2025-12-14 09:47:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.59it/s]\n",
      " 95%|█████████▌| 9967/10440 [22:41<01:03,  7.44it/s][NeMo W 2025-12-14 09:47:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.11it/s]\n",
      " 95%|█████████▌| 9968/10440 [22:41<01:00,  7.76it/s][NeMo W 2025-12-14 09:47:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.93it/s]\n",
      " 95%|█████████▌| 9969/10440 [22:41<01:02,  7.56it/s][NeMo W 2025-12-14 09:47:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.16it/s]\n",
      " 95%|█████████▌| 9970/10440 [22:41<01:01,  7.69it/s][NeMo W 2025-12-14 09:47:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.90it/s]\n",
      " 96%|█████████▌| 9971/10440 [22:41<01:00,  7.78it/s][NeMo W 2025-12-14 09:47:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.73it/s]\n",
      " 96%|█████████▌| 9972/10440 [22:42<00:57,  8.17it/s][NeMo W 2025-12-14 09:47:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.16it/s]\n",
      " 96%|█████████▌| 9973/10440 [22:42<00:55,  8.40it/s][NeMo W 2025-12-14 09:47:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.12it/s]\n",
      " 96%|█████████▌| 9974/10440 [22:42<00:55,  8.47it/s][NeMo W 2025-12-14 09:47:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.99it/s]\n",
      " 96%|█████████▌| 9975/10440 [22:42<00:56,  8.29it/s][NeMo W 2025-12-14 09:47:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.60it/s]\n",
      " 96%|█████████▌| 9976/10440 [22:42<01:07,  6.92it/s][NeMo W 2025-12-14 09:47:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.52it/s]\n",
      " 96%|█████████▌| 9977/10440 [22:42<01:05,  7.07it/s][NeMo W 2025-12-14 09:47:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.15it/s]\n",
      " 96%|█████████▌| 9978/10440 [22:42<01:04,  7.14it/s][NeMo W 2025-12-14 09:47:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.39it/s]\n",
      " 96%|█████████▌| 9979/10440 [22:43<01:02,  7.36it/s][NeMo W 2025-12-14 09:47:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.41it/s]\n",
      " 96%|█████████▌| 9980/10440 [22:43<00:59,  7.73it/s][NeMo W 2025-12-14 09:47:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.23it/s]\n",
      " 96%|█████████▌| 9981/10440 [22:43<00:58,  7.86it/s][NeMo W 2025-12-14 09:47:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.89it/s]\n",
      " 96%|█████████▌| 9982/10440 [22:43<00:55,  8.18it/s][NeMo W 2025-12-14 09:47:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.22it/s]\n",
      " 96%|█████████▌| 9983/10440 [22:43<01:05,  7.02it/s][NeMo W 2025-12-14 09:47:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.93it/s]\n",
      " 96%|█████████▌| 9984/10440 [22:43<01:06,  6.81it/s][NeMo W 2025-12-14 09:47:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.73it/s]\n",
      " 96%|█████████▌| 9985/10440 [22:43<01:07,  6.76it/s][NeMo W 2025-12-14 09:47:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.83it/s]\n",
      " 96%|█████████▌| 9986/10440 [22:43<01:05,  6.98it/s][NeMo W 2025-12-14 09:47:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.24it/s]\n",
      " 96%|█████████▌| 9987/10440 [22:44<01:02,  7.28it/s][NeMo W 2025-12-14 09:47:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.47it/s]\n",
      " 96%|█████████▌| 9988/10440 [22:44<01:00,  7.53it/s][NeMo W 2025-12-14 09:47:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.90it/s]\n",
      " 96%|█████████▌| 9989/10440 [22:44<00:58,  7.76it/s][NeMo W 2025-12-14 09:47:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.12it/s]\n",
      " 96%|█████████▌| 9990/10440 [22:44<00:56,  7.95it/s][NeMo W 2025-12-14 09:47:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.73it/s]\n",
      " 96%|█████████▌| 9991/10440 [22:44<00:59,  7.52it/s][NeMo W 2025-12-14 09:47:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.14it/s]\n",
      " 96%|█████████▌| 9992/10440 [22:44<01:01,  7.25it/s][NeMo W 2025-12-14 09:47:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.81it/s]\n",
      " 96%|█████████▌| 9993/10440 [22:44<00:58,  7.58it/s][NeMo W 2025-12-14 09:47:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.32it/s]\n",
      " 96%|█████████▌| 9994/10440 [22:45<00:56,  7.94it/s][NeMo W 2025-12-14 09:47:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.85it/s]\n",
      " 96%|█████████▌| 9995/10440 [22:45<00:54,  8.14it/s][NeMo W 2025-12-14 09:47:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.29it/s]\n",
      " 96%|█████████▌| 9996/10440 [22:45<00:53,  8.34it/s][NeMo W 2025-12-14 09:47:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.66it/s]\n",
      " 96%|█████████▌| 9997/10440 [22:45<00:52,  8.38it/s][NeMo W 2025-12-14 09:47:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.85it/s]\n",
      " 96%|█████████▌| 9998/10440 [22:45<00:55,  7.90it/s][NeMo W 2025-12-14 09:47:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.77it/s]\n",
      " 96%|█████████▌| 9999/10440 [22:45<01:02,  7.08it/s][NeMo W 2025-12-14 09:47:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.43it/s]\n",
      " 96%|█████████▌| 10000/10440 [22:45<00:59,  7.35it/s][NeMo W 2025-12-14 09:47:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.34it/s]\n",
      " 96%|█████████▌| 10001/10440 [22:45<00:57,  7.58it/s][NeMo W 2025-12-14 09:47:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.93it/s]\n",
      " 96%|█████████▌| 10002/10440 [22:46<00:57,  7.60it/s][NeMo W 2025-12-14 09:47:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.65it/s]\n",
      " 96%|█████████▌| 10003/10440 [22:46<00:57,  7.56it/s][NeMo W 2025-12-14 09:47:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.13it/s]\n",
      " 96%|█████████▌| 10004/10440 [22:46<00:56,  7.71it/s][NeMo W 2025-12-14 09:47:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.95it/s]\n",
      " 96%|█████████▌| 10005/10440 [22:46<00:57,  7.53it/s][NeMo W 2025-12-14 09:47:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.29it/s]\n",
      " 96%|█████████▌| 10006/10440 [22:46<00:54,  7.95it/s][NeMo W 2025-12-14 09:47:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.41it/s]\n",
      " 96%|█████████▌| 10007/10440 [22:46<00:53,  8.14it/s][NeMo W 2025-12-14 09:47:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.30it/s]\n",
      " 96%|█████████▌| 10008/10440 [22:46<00:56,  7.60it/s][NeMo W 2025-12-14 09:47:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.57it/s]\n",
      " 96%|█████████▌| 10009/10440 [22:46<00:59,  7.19it/s][NeMo W 2025-12-14 09:47:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.72it/s]\n",
      " 96%|█████████▌| 10010/10440 [22:47<00:58,  7.39it/s][NeMo W 2025-12-14 09:47:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.35it/s]\n",
      " 96%|█████████▌| 10011/10440 [22:47<00:55,  7.74it/s][NeMo W 2025-12-14 09:47:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.04it/s]\n",
      " 96%|█████████▌| 10012/10440 [22:47<00:54,  7.92it/s][NeMo W 2025-12-14 09:47:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.15it/s]\n",
      " 96%|█████████▌| 10013/10440 [22:47<00:53,  7.92it/s][NeMo W 2025-12-14 09:47:55 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:55 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.98it/s]\n",
      " 96%|█████████▌| 10014/10440 [22:47<00:52,  8.13it/s][NeMo W 2025-12-14 09:47:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.96it/s]\n",
      " 96%|█████████▌| 10015/10440 [22:47<00:51,  8.22it/s][NeMo W 2025-12-14 09:47:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.09it/s]\n",
      " 96%|█████████▌| 10016/10440 [22:47<00:56,  7.48it/s][NeMo W 2025-12-14 09:47:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.24it/s]\n",
      " 96%|█████████▌| 10017/10440 [22:47<00:56,  7.55it/s][NeMo W 2025-12-14 09:47:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.05it/s]\n",
      " 96%|█████████▌| 10018/10440 [22:48<00:55,  7.67it/s][NeMo W 2025-12-14 09:47:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.05it/s]\n",
      " 96%|█████████▌| 10019/10440 [22:48<00:55,  7.64it/s][NeMo W 2025-12-14 09:47:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.83it/s]\n",
      " 96%|█████████▌| 10020/10440 [22:48<00:55,  7.60it/s][NeMo W 2025-12-14 09:47:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.99it/s]\n",
      " 96%|█████████▌| 10021/10440 [22:48<00:52,  7.91it/s][NeMo W 2025-12-14 09:47:56 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:56 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.05it/s]\n",
      " 96%|█████████▌| 10022/10440 [22:48<00:52,  7.91it/s][NeMo W 2025-12-14 09:47:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.31it/s]\n",
      " 96%|█████████▌| 10023/10440 [22:48<00:51,  8.08it/s][NeMo W 2025-12-14 09:47:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.44it/s]\n",
      " 96%|█████████▌| 10024/10440 [22:48<00:53,  7.81it/s][NeMo W 2025-12-14 09:47:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.37it/s]\n",
      " 96%|█████████▌| 10025/10440 [22:49<00:53,  7.77it/s][NeMo W 2025-12-14 09:47:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.33it/s]\n",
      " 96%|█████████▌| 10026/10440 [22:49<00:55,  7.40it/s][NeMo W 2025-12-14 09:47:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.32it/s]\n",
      " 96%|█████████▌| 10027/10440 [22:49<00:53,  7.73it/s][NeMo W 2025-12-14 09:47:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.95it/s]\n",
      " 96%|█████████▌| 10028/10440 [22:49<00:52,  7.86it/s][NeMo W 2025-12-14 09:47:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.58it/s]\n",
      " 96%|█████████▌| 10029/10440 [22:49<00:50,  8.13it/s][NeMo W 2025-12-14 09:47:57 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:57 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.76it/s]\n",
      " 96%|█████████▌| 10030/10440 [22:49<00:52,  7.78it/s][NeMo W 2025-12-14 09:47:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.26it/s]\n",
      " 96%|█████████▌| 10031/10440 [22:49<00:54,  7.50it/s][NeMo W 2025-12-14 09:47:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.95it/s]\n",
      " 96%|█████████▌| 10032/10440 [22:49<00:58,  6.93it/s][NeMo W 2025-12-14 09:47:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.13it/s]\n",
      " 96%|█████████▌| 10033/10440 [22:50<01:01,  6.58it/s][NeMo W 2025-12-14 09:47:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.36it/s]\n",
      " 96%|█████████▌| 10034/10440 [22:50<01:04,  6.33it/s][NeMo W 2025-12-14 09:47:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.29it/s]\n",
      " 96%|█████████▌| 10035/10440 [22:50<01:01,  6.58it/s][NeMo W 2025-12-14 09:47:58 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:58 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.58it/s]\n",
      " 96%|█████████▌| 10036/10440 [22:50<01:03,  6.32it/s][NeMo W 2025-12-14 09:47:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.34it/s]\n",
      " 96%|█████████▌| 10037/10440 [22:50<01:03,  6.35it/s][NeMo W 2025-12-14 09:47:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.22it/s]\n",
      " 96%|█████████▌| 10038/10440 [22:50<00:59,  6.73it/s][NeMo W 2025-12-14 09:47:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.62it/s]\n",
      " 96%|█████████▌| 10039/10440 [22:51<01:01,  6.47it/s][NeMo W 2025-12-14 09:47:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.86it/s]\n",
      " 96%|█████████▌| 10040/10440 [22:51<00:57,  6.90it/s][NeMo W 2025-12-14 09:47:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  8.75it/s]\n",
      " 96%|█████████▌| 10041/10440 [22:51<01:04,  6.19it/s][NeMo W 2025-12-14 09:47:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:47:59 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.97it/s]\n",
      " 96%|█████████▌| 10042/10440 [22:51<01:03,  6.26it/s][NeMo W 2025-12-14 09:47:59 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.31it/s]\n",
      " 96%|█████████▌| 10043/10440 [22:51<01:03,  6.26it/s][NeMo W 2025-12-14 09:48:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.32it/s]\n",
      " 96%|█████████▌| 10044/10440 [22:51<01:02,  6.30it/s][NeMo W 2025-12-14 09:48:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.32it/s]\n",
      " 96%|█████████▌| 10045/10440 [22:51<00:58,  6.77it/s][NeMo W 2025-12-14 09:48:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.42it/s]\n",
      " 96%|█████████▌| 10046/10440 [22:52<01:05,  5.98it/s][NeMo W 2025-12-14 09:48:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.43it/s]\n",
      " 96%|█████████▌| 10047/10440 [22:52<01:05,  6.02it/s][NeMo W 2025-12-14 09:48:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.46it/s]\n",
      " 96%|█████████▌| 10048/10440 [22:52<01:04,  6.04it/s][NeMo W 2025-12-14 09:48:00 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:00 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.74it/s]\n",
      " 96%|█████████▋| 10049/10440 [22:52<01:01,  6.39it/s][NeMo W 2025-12-14 09:48:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.48it/s]\n",
      " 96%|█████████▋| 10050/10440 [22:52<01:05,  5.96it/s][NeMo W 2025-12-14 09:48:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.25it/s]\n",
      " 96%|█████████▋| 10051/10440 [22:53<01:02,  6.18it/s][NeMo W 2025-12-14 09:48:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.77it/s]\n",
      " 96%|█████████▋| 10052/10440 [22:53<00:59,  6.49it/s][NeMo W 2025-12-14 09:48:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.78it/s]\n",
      " 96%|█████████▋| 10053/10440 [22:53<00:59,  6.55it/s][NeMo W 2025-12-14 09:48:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.27it/s]\n",
      " 96%|█████████▋| 10054/10440 [22:53<00:56,  6.78it/s][NeMo W 2025-12-14 09:48:01 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:01 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.96it/s]\n",
      " 96%|█████████▋| 10055/10440 [22:53<00:56,  6.76it/s][NeMo W 2025-12-14 09:48:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.28it/s]\n",
      " 96%|█████████▋| 10056/10440 [22:53<00:55,  6.96it/s][NeMo W 2025-12-14 09:48:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.92it/s]\n",
      " 96%|█████████▋| 10057/10440 [22:53<00:55,  6.84it/s][NeMo W 2025-12-14 09:48:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.00it/s]\n",
      " 96%|█████████▋| 10058/10440 [22:54<00:56,  6.80it/s][NeMo W 2025-12-14 09:48:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.94it/s]\n",
      " 96%|█████████▋| 10059/10440 [22:54<00:53,  7.07it/s][NeMo W 2025-12-14 09:48:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.31it/s]\n",
      " 96%|█████████▋| 10060/10440 [22:54<00:54,  6.98it/s][NeMo W 2025-12-14 09:48:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.55it/s]\n",
      " 96%|█████████▋| 10061/10440 [22:54<00:55,  6.88it/s][NeMo W 2025-12-14 09:48:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:02 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.29it/s]\n",
      " 96%|█████████▋| 10062/10440 [22:54<00:53,  7.09it/s][NeMo W 2025-12-14 09:48:02 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.50it/s]\n",
      " 96%|█████████▋| 10063/10440 [22:54<00:51,  7.38it/s][NeMo W 2025-12-14 09:48:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.39it/s]\n",
      " 96%|█████████▋| 10064/10440 [22:54<00:49,  7.59it/s][NeMo W 2025-12-14 09:48:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.41it/s]\n",
      " 96%|█████████▋| 10065/10440 [22:54<00:49,  7.56it/s][NeMo W 2025-12-14 09:48:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.53it/s]\n",
      " 96%|█████████▋| 10066/10440 [22:55<00:49,  7.63it/s][NeMo W 2025-12-14 09:48:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.94it/s]\n",
      " 96%|█████████▋| 10067/10440 [22:55<00:50,  7.41it/s][NeMo W 2025-12-14 09:48:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.16it/s]\n",
      " 96%|█████████▋| 10068/10440 [22:55<00:54,  6.85it/s][NeMo W 2025-12-14 09:48:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.80it/s]\n",
      " 96%|█████████▋| 10069/10440 [22:55<00:52,  7.10it/s][NeMo W 2025-12-14 09:48:03 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:03 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.58it/s]\n",
      " 96%|█████████▋| 10070/10440 [22:55<00:50,  7.32it/s][NeMo W 2025-12-14 09:48:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.91it/s]\n",
      " 96%|█████████▋| 10071/10440 [22:55<00:49,  7.44it/s][NeMo W 2025-12-14 09:48:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.35it/s]\n",
      " 96%|█████████▋| 10072/10440 [22:55<00:50,  7.32it/s][NeMo W 2025-12-14 09:48:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.60it/s]\n",
      " 96%|█████████▋| 10073/10440 [22:56<00:48,  7.54it/s][NeMo W 2025-12-14 09:48:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.07it/s]\n",
      " 96%|█████████▋| 10074/10440 [22:56<00:47,  7.74it/s][NeMo W 2025-12-14 09:48:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.49it/s]\n",
      " 97%|█████████▋| 10075/10440 [22:56<00:47,  7.61it/s][NeMo W 2025-12-14 09:48:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.12it/s]\n",
      " 97%|█████████▋| 10076/10440 [22:56<00:48,  7.58it/s][NeMo W 2025-12-14 09:48:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.10it/s]\n",
      " 97%|█████████▋| 10077/10440 [22:56<00:47,  7.63it/s][NeMo W 2025-12-14 09:48:04 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:04 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.51it/s]\n",
      " 97%|█████████▋| 10078/10440 [22:56<00:46,  7.87it/s][NeMo W 2025-12-14 09:48:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.63it/s]\n",
      " 97%|█████████▋| 10079/10440 [22:56<00:46,  7.84it/s][NeMo W 2025-12-14 09:48:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.12it/s]\n",
      " 97%|█████████▋| 10080/10440 [22:56<00:45,  7.93it/s][NeMo W 2025-12-14 09:48:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.55it/s]\n",
      " 97%|█████████▋| 10081/10440 [22:57<00:46,  7.68it/s][NeMo W 2025-12-14 09:48:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.05it/s]\n",
      " 97%|█████████▋| 10082/10440 [22:57<00:46,  7.70it/s][NeMo W 2025-12-14 09:48:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.88it/s]\n",
      " 97%|█████████▋| 10083/10440 [22:57<00:47,  7.51it/s][NeMo W 2025-12-14 09:48:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.37it/s]\n",
      " 97%|█████████▋| 10084/10440 [22:57<00:49,  7.26it/s][NeMo W 2025-12-14 09:48:05 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:05 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.15it/s]\n",
      " 97%|█████████▋| 10085/10440 [22:57<00:47,  7.47it/s][NeMo W 2025-12-14 09:48:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.47it/s]\n",
      " 97%|█████████▋| 10086/10440 [22:57<00:45,  7.81it/s][NeMo W 2025-12-14 09:48:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.75it/s]\n",
      " 97%|█████████▋| 10087/10440 [22:57<00:45,  7.82it/s][NeMo W 2025-12-14 09:48:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.36it/s]\n",
      " 97%|█████████▋| 10088/10440 [22:57<00:44,  7.98it/s][NeMo W 2025-12-14 09:48:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.88it/s]\n",
      " 97%|█████████▋| 10089/10440 [22:58<00:43,  8.14it/s][NeMo W 2025-12-14 09:48:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.24it/s]\n",
      " 97%|█████████▋| 10090/10440 [22:58<00:42,  8.29it/s][NeMo W 2025-12-14 09:48:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.96it/s]\n",
      " 97%|█████████▋| 10091/10440 [22:58<00:41,  8.47it/s][NeMo W 2025-12-14 09:48:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  8.78it/s]\n",
      " 97%|█████████▋| 10092/10440 [22:58<00:49,  7.05it/s][NeMo W 2025-12-14 09:48:06 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:06 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.39it/s]\n",
      " 97%|█████████▋| 10093/10440 [22:58<00:50,  6.87it/s][NeMo W 2025-12-14 09:48:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.38it/s]\n",
      " 97%|█████████▋| 10094/10440 [22:58<00:52,  6.64it/s][NeMo W 2025-12-14 09:48:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.50it/s]\n",
      " 97%|█████████▋| 10095/10440 [22:58<00:50,  6.81it/s][NeMo W 2025-12-14 09:48:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.14it/s]\n",
      " 97%|█████████▋| 10096/10440 [22:59<00:48,  7.16it/s][NeMo W 2025-12-14 09:48:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.96it/s]\n",
      " 97%|█████████▋| 10097/10440 [22:59<00:48,  7.13it/s][NeMo W 2025-12-14 09:48:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.44it/s]\n",
      " 97%|█████████▋| 10098/10440 [22:59<00:47,  7.20it/s][NeMo W 2025-12-14 09:48:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:07 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.47it/s]\n",
      " 97%|█████████▋| 10099/10440 [22:59<00:51,  6.64it/s][NeMo W 2025-12-14 09:48:07 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.50it/s]\n",
      " 97%|█████████▋| 10100/10440 [22:59<00:51,  6.60it/s][NeMo W 2025-12-14 09:48:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.00it/s]\n",
      " 97%|█████████▋| 10101/10440 [22:59<00:47,  7.12it/s][NeMo W 2025-12-14 09:48:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.15it/s]\n",
      " 97%|█████████▋| 10102/10440 [22:59<00:45,  7.50it/s][NeMo W 2025-12-14 09:48:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.25it/s]\n",
      " 97%|█████████▋| 10103/10440 [23:00<00:43,  7.79it/s][NeMo W 2025-12-14 09:48:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.65it/s]\n",
      " 97%|█████████▋| 10104/10440 [23:00<00:42,  7.95it/s][NeMo W 2025-12-14 09:48:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.16it/s]\n",
      " 97%|█████████▋| 10105/10440 [23:00<00:41,  8.10it/s][NeMo W 2025-12-14 09:48:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.35it/s]\n",
      " 97%|█████████▋| 10106/10440 [23:00<00:39,  8.42it/s][NeMo W 2025-12-14 09:48:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.70it/s]\n",
      " 97%|█████████▋| 10107/10440 [23:00<00:39,  8.34it/s][NeMo W 2025-12-14 09:48:08 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:08 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.31it/s]\n",
      " 97%|█████████▋| 10108/10440 [23:00<00:41,  8.07it/s][NeMo W 2025-12-14 09:48:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.73it/s]\n",
      " 97%|█████████▋| 10109/10440 [23:00<00:43,  7.59it/s][NeMo W 2025-12-14 09:48:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.04it/s]\n",
      " 97%|█████████▋| 10110/10440 [23:00<00:44,  7.37it/s][NeMo W 2025-12-14 09:48:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.45it/s]\n",
      " 97%|█████████▋| 10111/10440 [23:01<00:45,  7.27it/s][NeMo W 2025-12-14 09:48:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.45it/s]\n",
      " 97%|█████████▋| 10112/10440 [23:01<00:42,  7.71it/s][NeMo W 2025-12-14 09:48:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.34it/s]\n",
      " 97%|█████████▋| 10113/10440 [23:01<00:41,  7.85it/s][NeMo W 2025-12-14 09:48:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.86it/s]\n",
      " 97%|█████████▋| 10114/10440 [23:01<00:41,  7.90it/s][NeMo W 2025-12-14 09:48:09 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:09 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.50it/s]\n",
      " 97%|█████████▋| 10115/10440 [23:01<00:41,  7.88it/s][NeMo W 2025-12-14 09:48:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.12it/s]\n",
      " 97%|█████████▋| 10116/10440 [23:01<00:41,  7.82it/s][NeMo W 2025-12-14 09:48:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.27it/s]\n",
      " 97%|█████████▋| 10117/10440 [23:01<00:41,  7.74it/s][NeMo W 2025-12-14 09:48:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.98it/s]\n",
      " 97%|█████████▋| 10118/10440 [23:01<00:44,  7.30it/s][NeMo W 2025-12-14 09:48:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.24it/s]\n",
      " 97%|█████████▋| 10119/10440 [23:02<00:42,  7.49it/s][NeMo W 2025-12-14 09:48:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.51it/s]\n",
      " 97%|█████████▋| 10120/10440 [23:02<00:42,  7.59it/s][NeMo W 2025-12-14 09:48:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.04it/s]\n",
      " 97%|█████████▋| 10121/10440 [23:02<00:41,  7.74it/s][NeMo W 2025-12-14 09:48:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.17it/s]\n",
      " 97%|█████████▋| 10122/10440 [23:02<00:42,  7.42it/s][NeMo W 2025-12-14 09:48:10 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:10 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.91it/s]\n",
      " 97%|█████████▋| 10123/10440 [23:02<00:41,  7.60it/s][NeMo W 2025-12-14 09:48:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.24it/s]\n",
      " 97%|█████████▋| 10124/10440 [23:02<00:44,  7.07it/s][NeMo W 2025-12-14 09:48:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.18it/s]\n",
      " 97%|█████████▋| 10125/10440 [23:02<00:44,  7.01it/s][NeMo W 2025-12-14 09:48:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.65it/s]\n",
      " 97%|█████████▋| 10126/10440 [23:03<00:45,  6.95it/s][NeMo W 2025-12-14 09:48:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.30it/s]\n",
      " 97%|█████████▋| 10127/10440 [23:03<00:42,  7.31it/s][NeMo W 2025-12-14 09:48:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.68it/s]\n",
      " 97%|█████████▋| 10128/10440 [23:03<00:40,  7.76it/s][NeMo W 2025-12-14 09:48:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.68it/s]\n",
      " 97%|█████████▋| 10129/10440 [23:03<00:39,  7.81it/s][NeMo W 2025-12-14 09:48:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:11 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.49it/s]\n",
      " 97%|█████████▋| 10130/10440 [23:03<00:37,  8.21it/s][NeMo W 2025-12-14 09:48:11 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.45it/s]\n",
      " 97%|█████████▋| 10131/10440 [23:03<00:41,  7.38it/s][NeMo W 2025-12-14 09:48:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.59it/s]\n",
      " 97%|█████████▋| 10132/10440 [23:03<00:45,  6.78it/s][NeMo W 2025-12-14 09:48:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.07it/s]\n",
      " 97%|█████████▋| 10133/10440 [23:04<00:45,  6.76it/s][NeMo W 2025-12-14 09:48:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.43it/s]\n",
      " 97%|█████████▋| 10134/10440 [23:04<00:45,  6.67it/s][NeMo W 2025-12-14 09:48:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.32it/s]\n",
      " 97%|█████████▋| 10135/10440 [23:04<00:44,  6.78it/s][NeMo W 2025-12-14 09:48:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.49it/s]\n",
      " 97%|█████████▋| 10136/10440 [23:04<00:43,  6.94it/s][NeMo W 2025-12-14 09:48:12 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:12 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.50it/s]\n",
      " 97%|█████████▋| 10137/10440 [23:04<00:42,  7.08it/s][NeMo W 2025-12-14 09:48:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.23it/s]\n",
      " 97%|█████████▋| 10138/10440 [23:04<00:44,  6.83it/s][NeMo W 2025-12-14 09:48:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.33it/s]\n",
      " 97%|█████████▋| 10139/10440 [23:04<00:43,  6.93it/s][NeMo W 2025-12-14 09:48:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.91it/s]\n",
      " 97%|█████████▋| 10140/10440 [23:05<00:45,  6.57it/s][NeMo W 2025-12-14 09:48:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.61it/s]\n",
      " 97%|█████████▋| 10141/10440 [23:05<00:47,  6.35it/s][NeMo W 2025-12-14 09:48:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.71it/s]\n",
      " 97%|█████████▋| 10142/10440 [23:05<00:47,  6.25it/s][NeMo W 2025-12-14 09:48:13 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:13 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.86it/s]\n",
      " 97%|█████████▋| 10143/10440 [23:05<00:46,  6.33it/s][NeMo W 2025-12-14 09:48:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.90it/s]\n",
      " 97%|█████████▋| 10144/10440 [23:05<00:47,  6.17it/s][NeMo W 2025-12-14 09:48:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.17it/s]\n",
      " 97%|█████████▋| 10145/10440 [23:05<00:46,  6.29it/s][NeMo W 2025-12-14 09:48:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.83it/s]\n",
      " 97%|█████████▋| 10146/10440 [23:06<00:44,  6.59it/s][NeMo W 2025-12-14 09:48:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.38it/s]\n",
      " 97%|█████████▋| 10147/10440 [23:06<00:44,  6.55it/s][NeMo W 2025-12-14 09:48:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.06it/s]\n",
      " 97%|█████████▋| 10148/10440 [23:06<00:47,  6.16it/s][NeMo W 2025-12-14 09:48:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.71it/s]\n",
      " 97%|█████████▋| 10149/10440 [23:06<00:44,  6.50it/s][NeMo W 2025-12-14 09:48:14 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:14 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.57it/s]\n",
      " 97%|█████████▋| 10150/10440 [23:06<00:44,  6.50it/s][NeMo W 2025-12-14 09:48:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.83it/s]\n",
      " 97%|█████████▋| 10151/10440 [23:06<00:44,  6.50it/s][NeMo W 2025-12-14 09:48:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.09it/s]\n",
      " 97%|█████████▋| 10152/10440 [23:06<00:43,  6.61it/s][NeMo W 2025-12-14 09:48:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.78it/s]\n",
      " 97%|█████████▋| 10153/10440 [23:07<00:42,  6.82it/s][NeMo W 2025-12-14 09:48:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.74it/s]\n",
      " 97%|█████████▋| 10154/10440 [23:07<00:45,  6.31it/s][NeMo W 2025-12-14 09:48:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.08it/s]\n",
      " 97%|█████████▋| 10155/10440 [23:07<00:44,  6.46it/s][NeMo W 2025-12-14 09:48:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:15 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.24it/s]\n",
      " 97%|█████████▋| 10156/10440 [23:07<00:41,  6.80it/s][NeMo W 2025-12-14 09:48:15 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.61it/s]\n",
      " 97%|█████████▋| 10157/10440 [23:07<00:41,  6.80it/s][NeMo W 2025-12-14 09:48:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.93it/s]\n",
      " 97%|█████████▋| 10158/10440 [23:07<00:41,  6.77it/s][NeMo W 2025-12-14 09:48:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.72it/s]\n",
      " 97%|█████████▋| 10159/10440 [23:08<00:41,  6.70it/s][NeMo W 2025-12-14 09:48:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.60it/s]\n",
      " 97%|█████████▋| 10160/10440 [23:08<00:40,  6.92it/s][NeMo W 2025-12-14 09:48:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.82it/s]\n",
      " 97%|█████████▋| 10161/10440 [23:08<00:39,  7.08it/s][NeMo W 2025-12-14 09:48:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.85it/s]\n",
      " 97%|█████████▋| 10162/10440 [23:08<00:38,  7.26it/s][NeMo W 2025-12-14 09:48:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.91it/s]\n",
      " 97%|█████████▋| 10163/10440 [23:08<00:37,  7.42it/s][NeMo W 2025-12-14 09:48:16 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:16 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.21it/s]\n",
      " 97%|█████████▋| 10164/10440 [23:08<00:37,  7.42it/s][NeMo W 2025-12-14 09:48:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.84it/s]\n",
      " 97%|█████████▋| 10165/10440 [23:08<00:37,  7.27it/s][NeMo W 2025-12-14 09:48:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.22it/s]\n",
      " 97%|█████████▋| 10166/10440 [23:08<00:40,  6.74it/s][NeMo W 2025-12-14 09:48:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.49it/s]\n",
      " 97%|█████████▋| 10167/10440 [23:09<00:38,  7.18it/s][NeMo W 2025-12-14 09:48:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.87it/s]\n",
      " 97%|█████████▋| 10168/10440 [23:09<00:35,  7.69it/s][NeMo W 2025-12-14 09:48:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.53it/s]\n",
      " 97%|█████████▋| 10169/10440 [23:09<00:36,  7.53it/s][NeMo W 2025-12-14 09:48:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.02it/s]\n",
      " 97%|█████████▋| 10170/10440 [23:09<00:34,  7.75it/s][NeMo W 2025-12-14 09:48:17 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:17 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.94it/s]\n",
      " 97%|█████████▋| 10171/10440 [23:09<00:34,  7.86it/s][NeMo W 2025-12-14 09:48:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.75it/s]\n",
      " 97%|█████████▋| 10172/10440 [23:09<00:33,  7.94it/s][NeMo W 2025-12-14 09:48:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.65it/s]\n",
      " 97%|█████████▋| 10173/10440 [23:09<00:33,  8.02it/s][NeMo W 2025-12-14 09:48:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.13it/s]\n",
      " 97%|█████████▋| 10174/10440 [23:09<00:32,  8.18it/s][NeMo W 2025-12-14 09:48:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.33it/s]\n",
      " 97%|█████████▋| 10175/10440 [23:10<00:33,  7.93it/s][NeMo W 2025-12-14 09:48:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.63it/s]\n",
      " 97%|█████████▋| 10176/10440 [23:10<00:37,  7.12it/s][NeMo W 2025-12-14 09:48:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.79it/s]\n",
      " 97%|█████████▋| 10177/10440 [23:10<00:35,  7.39it/s][NeMo W 2025-12-14 09:48:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.77it/s]\n",
      " 97%|█████████▋| 10178/10440 [23:10<00:35,  7.34it/s][NeMo W 2025-12-14 09:48:18 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:18 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.78it/s]\n",
      " 98%|█████████▊| 10179/10440 [23:10<00:34,  7.55it/s][NeMo W 2025-12-14 09:48:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.40it/s]\n",
      " 98%|█████████▊| 10180/10440 [23:10<00:34,  7.50it/s][NeMo W 2025-12-14 09:48:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.96it/s]\n",
      " 98%|█████████▊| 10181/10440 [23:10<00:34,  7.60it/s][NeMo W 2025-12-14 09:48:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.60it/s]\n",
      " 98%|█████████▊| 10182/10440 [23:11<00:35,  7.24it/s][NeMo W 2025-12-14 09:48:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.47it/s]\n",
      " 98%|█████████▊| 10183/10440 [23:11<00:37,  6.91it/s][NeMo W 2025-12-14 09:48:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.54it/s]\n",
      " 98%|█████████▊| 10184/10440 [23:11<00:35,  7.30it/s][NeMo W 2025-12-14 09:48:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.88it/s]\n",
      " 98%|█████████▊| 10185/10440 [23:11<00:34,  7.40it/s][NeMo W 2025-12-14 09:48:19 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:19 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.34it/s]\n",
      " 98%|█████████▊| 10186/10440 [23:11<00:34,  7.44it/s][NeMo W 2025-12-14 09:48:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.50it/s]\n",
      " 98%|█████████▊| 10187/10440 [23:11<00:33,  7.45it/s][NeMo W 2025-12-14 09:48:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.78it/s]\n",
      " 98%|█████████▊| 10188/10440 [23:11<00:33,  7.48it/s][NeMo W 2025-12-14 09:48:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.53it/s]\n",
      " 98%|█████████▊| 10189/10440 [23:12<00:33,  7.57it/s][NeMo W 2025-12-14 09:48:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.91it/s]\n",
      " 98%|█████████▊| 10190/10440 [23:12<00:31,  7.83it/s][NeMo W 2025-12-14 09:48:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.22it/s]\n",
      " 98%|█████████▊| 10191/10440 [23:12<00:34,  7.12it/s][NeMo W 2025-12-14 09:48:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.36it/s]\n",
      " 98%|█████████▊| 10192/10440 [23:12<00:34,  7.24it/s][NeMo W 2025-12-14 09:48:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:20 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.77it/s]\n",
      " 98%|█████████▊| 10193/10440 [23:12<00:34,  7.26it/s][NeMo W 2025-12-14 09:48:20 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.71it/s]\n",
      " 98%|█████████▊| 10194/10440 [23:12<00:33,  7.34it/s][NeMo W 2025-12-14 09:48:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.97it/s]\n",
      " 98%|█████████▊| 10195/10440 [23:12<00:32,  7.51it/s][NeMo W 2025-12-14 09:48:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.69it/s]\n",
      " 98%|█████████▊| 10196/10440 [23:12<00:32,  7.58it/s][NeMo W 2025-12-14 09:48:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.35it/s]\n",
      " 98%|█████████▊| 10197/10440 [23:13<00:31,  7.75it/s][NeMo W 2025-12-14 09:48:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.96it/s]\n",
      " 98%|█████████▊| 10198/10440 [23:13<00:31,  7.76it/s][NeMo W 2025-12-14 09:48:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.49it/s]\n",
      " 98%|█████████▊| 10199/10440 [23:13<00:30,  7.92it/s][NeMo W 2025-12-14 09:48:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.42it/s]\n",
      " 98%|█████████▊| 10200/10440 [23:13<00:30,  7.92it/s][NeMo W 2025-12-14 09:48:21 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:21 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.20it/s]\n",
      " 98%|█████████▊| 10201/10440 [23:13<00:32,  7.43it/s][NeMo W 2025-12-14 09:48:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.84it/s]\n",
      " 98%|█████████▊| 10202/10440 [23:13<00:32,  7.28it/s][NeMo W 2025-12-14 09:48:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.52it/s]\n",
      " 98%|█████████▊| 10203/10440 [23:13<00:32,  7.31it/s][NeMo W 2025-12-14 09:48:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.92it/s]\n",
      " 98%|█████████▊| 10204/10440 [23:14<00:31,  7.43it/s][NeMo W 2025-12-14 09:48:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.56it/s]\n",
      " 98%|█████████▊| 10205/10440 [23:14<00:30,  7.73it/s][NeMo W 2025-12-14 09:48:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.03it/s]\n",
      " 98%|█████████▊| 10206/10440 [23:14<00:29,  7.94it/s][NeMo W 2025-12-14 09:48:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.93it/s]\n",
      " 98%|█████████▊| 10207/10440 [23:14<00:29,  7.93it/s][NeMo W 2025-12-14 09:48:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.87it/s]\n",
      " 98%|█████████▊| 10208/10440 [23:14<00:32,  7.10it/s][NeMo W 2025-12-14 09:48:22 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:22 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.51it/s]\n",
      " 98%|█████████▊| 10209/10440 [23:14<00:33,  6.91it/s][NeMo W 2025-12-14 09:48:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.43it/s]\n",
      " 98%|█████████▊| 10210/10440 [23:14<00:31,  7.32it/s][NeMo W 2025-12-14 09:48:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.74it/s]\n",
      " 98%|█████████▊| 10211/10440 [23:14<00:31,  7.36it/s][NeMo W 2025-12-14 09:48:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.13it/s]\n",
      " 98%|█████████▊| 10212/10440 [23:15<00:29,  7.61it/s][NeMo W 2025-12-14 09:48:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.59it/s]\n",
      " 98%|█████████▊| 10213/10440 [23:15<00:29,  7.63it/s][NeMo W 2025-12-14 09:48:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.15it/s]\n",
      " 98%|█████████▊| 10214/10440 [23:15<00:29,  7.79it/s][NeMo W 2025-12-14 09:48:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.27it/s]\n",
      " 98%|█████████▊| 10215/10440 [23:15<00:28,  7.91it/s][NeMo W 2025-12-14 09:48:23 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:23 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.89it/s]\n",
      " 98%|█████████▊| 10216/10440 [23:15<00:30,  7.38it/s][NeMo W 2025-12-14 09:48:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.78it/s]\n",
      " 98%|█████████▊| 10217/10440 [23:15<00:31,  7.03it/s][NeMo W 2025-12-14 09:48:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.48it/s]\n",
      " 98%|█████████▊| 10218/10440 [23:15<00:31,  7.09it/s][NeMo W 2025-12-14 09:48:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.60it/s]\n",
      " 98%|█████████▊| 10219/10440 [23:16<00:29,  7.39it/s][NeMo W 2025-12-14 09:48:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.87it/s]\n",
      " 98%|█████████▊| 10220/10440 [23:16<00:29,  7.56it/s][NeMo W 2025-12-14 09:48:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.79it/s]\n",
      " 98%|█████████▊| 10221/10440 [23:16<00:28,  7.66it/s][NeMo W 2025-12-14 09:48:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.36it/s]\n",
      " 98%|█████████▊| 10222/10440 [23:16<00:27,  7.90it/s][NeMo W 2025-12-14 09:48:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.07it/s]\n",
      " 98%|█████████▊| 10223/10440 [23:16<00:26,  8.13it/s][NeMo W 2025-12-14 09:48:24 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:24 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.71it/s]\n",
      " 98%|█████████▊| 10224/10440 [23:16<00:27,  8.00it/s][NeMo W 2025-12-14 09:48:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.22it/s]\n",
      " 98%|█████████▊| 10225/10440 [23:16<00:26,  8.20it/s][NeMo W 2025-12-14 09:48:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.70it/s]\n",
      " 98%|█████████▊| 10226/10440 [23:16<00:29,  7.13it/s][NeMo W 2025-12-14 09:48:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.66it/s]\n",
      " 98%|█████████▊| 10227/10440 [23:17<00:29,  7.26it/s][NeMo W 2025-12-14 09:48:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.03it/s]\n",
      " 98%|█████████▊| 10228/10440 [23:17<00:28,  7.52it/s][NeMo W 2025-12-14 09:48:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.28it/s]\n",
      " 98%|█████████▊| 10229/10440 [23:17<00:26,  7.87it/s][NeMo W 2025-12-14 09:48:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.52it/s]\n",
      " 98%|█████████▊| 10230/10440 [23:17<00:26,  7.88it/s][NeMo W 2025-12-14 09:48:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:25 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.60it/s]\n",
      " 98%|█████████▊| 10231/10440 [23:17<00:27,  7.73it/s][NeMo W 2025-12-14 09:48:25 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.04it/s]\n",
      " 98%|█████████▊| 10232/10440 [23:17<00:28,  7.25it/s][NeMo W 2025-12-14 09:48:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.88it/s]\n",
      " 98%|█████████▊| 10233/10440 [23:17<00:29,  7.07it/s][NeMo W 2025-12-14 09:48:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.16it/s]\n",
      " 98%|█████████▊| 10234/10440 [23:18<00:28,  7.22it/s][NeMo W 2025-12-14 09:48:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.92it/s]\n",
      " 98%|█████████▊| 10235/10440 [23:18<00:30,  6.78it/s][NeMo W 2025-12-14 09:48:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.79it/s]\n",
      " 98%|█████████▊| 10236/10440 [23:18<00:31,  6.53it/s][NeMo W 2025-12-14 09:48:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.66it/s]\n",
      " 98%|█████████▊| 10237/10440 [23:18<00:31,  6.42it/s][NeMo W 2025-12-14 09:48:26 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:26 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.09it/s]\n",
      " 98%|█████████▊| 10238/10440 [23:18<00:30,  6.53it/s][NeMo W 2025-12-14 09:48:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.97it/s]\n",
      " 98%|█████████▊| 10239/10440 [23:18<00:31,  6.41it/s][NeMo W 2025-12-14 09:48:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.17it/s]\n",
      " 98%|█████████▊| 10240/10440 [23:18<00:31,  6.44it/s][NeMo W 2025-12-14 09:48:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.38it/s]\n",
      " 98%|█████████▊| 10241/10440 [23:19<00:32,  6.19it/s][NeMo W 2025-12-14 09:48:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.04it/s]\n",
      " 98%|█████████▊| 10242/10440 [23:19<00:34,  5.74it/s][NeMo W 2025-12-14 09:48:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.01it/s]\n",
      " 98%|█████████▊| 10243/10440 [23:19<00:32,  6.03it/s][NeMo W 2025-12-14 09:48:27 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:27 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.81it/s]\n",
      " 98%|█████████▊| 10244/10440 [23:19<00:32,  6.03it/s][NeMo W 2025-12-14 09:48:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.60it/s]\n",
      " 98%|█████████▊| 10245/10440 [23:19<00:33,  5.89it/s][NeMo W 2025-12-14 09:48:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.77it/s]\n",
      " 98%|█████████▊| 10246/10440 [23:19<00:31,  6.20it/s][NeMo W 2025-12-14 09:48:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.10it/s]\n",
      " 98%|█████████▊| 10247/10440 [23:20<00:30,  6.43it/s][NeMo W 2025-12-14 09:48:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.56it/s]\n",
      " 98%|█████████▊| 10248/10440 [23:20<00:31,  6.18it/s][NeMo W 2025-12-14 09:48:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.38it/s]\n",
      " 98%|█████████▊| 10249/10440 [23:20<00:30,  6.25it/s][NeMo W 2025-12-14 09:48:28 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:28 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.54it/s]\n",
      " 98%|█████████▊| 10250/10440 [23:20<00:29,  6.50it/s][NeMo W 2025-12-14 09:48:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.82it/s]\n",
      " 98%|█████████▊| 10251/10440 [23:20<00:26,  7.10it/s][NeMo W 2025-12-14 09:48:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.54it/s]\n",
      " 98%|█████████▊| 10252/10440 [23:20<00:26,  7.14it/s][NeMo W 2025-12-14 09:48:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.65it/s]\n",
      " 98%|█████████▊| 10253/10440 [23:21<00:27,  6.84it/s][NeMo W 2025-12-14 09:48:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.04it/s]\n",
      " 98%|█████████▊| 10254/10440 [23:21<00:27,  6.70it/s][NeMo W 2025-12-14 09:48:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.18it/s]\n",
      " 98%|█████████▊| 10255/10440 [23:21<00:26,  6.93it/s][NeMo W 2025-12-14 09:48:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.61it/s]\n",
      " 98%|█████████▊| 10256/10440 [23:21<00:26,  7.03it/s][NeMo W 2025-12-14 09:48:29 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:29 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.49it/s]\n",
      " 98%|█████████▊| 10257/10440 [23:21<00:26,  6.99it/s][NeMo W 2025-12-14 09:48:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.66it/s]\n",
      " 98%|█████████▊| 10258/10440 [23:21<00:26,  6.98it/s][NeMo W 2025-12-14 09:48:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.29it/s]\n",
      " 98%|█████████▊| 10259/10440 [23:21<00:25,  7.17it/s][NeMo W 2025-12-14 09:48:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.01it/s]\n",
      " 98%|█████████▊| 10260/10440 [23:21<00:24,  7.27it/s][NeMo W 2025-12-14 09:48:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.31it/s]\n",
      " 98%|█████████▊| 10261/10440 [23:22<00:23,  7.52it/s][NeMo W 2025-12-14 09:48:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.01it/s]\n",
      " 98%|█████████▊| 10262/10440 [23:22<00:24,  7.34it/s][NeMo W 2025-12-14 09:48:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.44it/s]\n",
      " 98%|█████████▊| 10263/10440 [23:22<00:22,  7.72it/s][NeMo W 2025-12-14 09:48:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.15it/s]\n",
      " 98%|█████████▊| 10264/10440 [23:22<00:23,  7.38it/s][NeMo W 2025-12-14 09:48:30 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:30 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.95it/s]\n",
      " 98%|█████████▊| 10265/10440 [23:22<00:23,  7.47it/s][NeMo W 2025-12-14 09:48:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.52it/s]\n",
      " 98%|█████████▊| 10266/10440 [23:22<00:22,  7.76it/s][NeMo W 2025-12-14 09:48:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.47it/s]\n",
      " 98%|█████████▊| 10267/10440 [23:22<00:22,  7.69it/s][NeMo W 2025-12-14 09:48:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.01it/s]\n",
      " 98%|█████████▊| 10268/10440 [23:23<00:22,  7.74it/s][NeMo W 2025-12-14 09:48:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.75it/s]\n",
      " 98%|█████████▊| 10269/10440 [23:23<00:23,  7.18it/s][NeMo W 2025-12-14 09:48:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.37it/s]\n",
      " 98%|█████████▊| 10270/10440 [23:23<00:23,  7.31it/s][NeMo W 2025-12-14 09:48:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.45it/s]\n",
      " 98%|█████████▊| 10271/10440 [23:23<00:21,  7.77it/s][NeMo W 2025-12-14 09:48:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.51it/s]\n",
      " 98%|█████████▊| 10272/10440 [23:23<00:21,  7.76it/s][NeMo W 2025-12-14 09:48:31 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:31 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.24it/s]\n",
      " 98%|█████████▊| 10273/10440 [23:23<00:22,  7.46it/s][NeMo W 2025-12-14 09:48:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.11it/s]\n",
      " 98%|█████████▊| 10274/10440 [23:23<00:21,  7.63it/s][NeMo W 2025-12-14 09:48:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.70it/s]\n",
      " 98%|█████████▊| 10275/10440 [23:23<00:22,  7.45it/s][NeMo W 2025-12-14 09:48:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.29it/s]\n",
      " 98%|█████████▊| 10276/10440 [23:24<00:22,  7.45it/s][NeMo W 2025-12-14 09:48:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.76it/s]\n",
      " 98%|█████████▊| 10277/10440 [23:24<00:21,  7.75it/s][NeMo W 2025-12-14 09:48:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.89it/s]\n",
      " 98%|█████████▊| 10278/10440 [23:24<00:20,  7.89it/s][NeMo W 2025-12-14 09:48:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.24it/s]\n",
      " 98%|█████████▊| 10279/10440 [23:24<00:21,  7.44it/s][NeMo W 2025-12-14 09:48:32 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:32 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.74it/s]\n",
      " 98%|█████████▊| 10280/10440 [23:24<00:21,  7.60it/s][NeMo W 2025-12-14 09:48:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.37it/s]\n",
      " 98%|█████████▊| 10281/10440 [23:24<00:21,  7.32it/s][NeMo W 2025-12-14 09:48:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.91it/s]\n",
      " 98%|█████████▊| 10282/10440 [23:24<00:20,  7.60it/s][NeMo W 2025-12-14 09:48:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.10it/s]\n",
      " 98%|█████████▊| 10283/10440 [23:25<00:20,  7.63it/s][NeMo W 2025-12-14 09:48:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.51it/s]\n",
      " 99%|█████████▊| 10284/10440 [23:25<00:19,  7.83it/s][NeMo W 2025-12-14 09:48:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.74it/s]\n",
      " 99%|█████████▊| 10285/10440 [23:25<00:19,  7.97it/s][NeMo W 2025-12-14 09:48:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.46it/s]\n",
      " 99%|█████████▊| 10286/10440 [23:25<00:20,  7.57it/s][NeMo W 2025-12-14 09:48:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.86it/s]\n",
      " 99%|█████████▊| 10287/10440 [23:25<00:20,  7.59it/s][NeMo W 2025-12-14 09:48:33 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:33 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.75it/s]\n",
      " 99%|█████████▊| 10288/10440 [23:25<00:20,  7.53it/s][NeMo W 2025-12-14 09:48:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.77it/s]\n",
      " 99%|█████████▊| 10289/10440 [23:25<00:19,  7.69it/s][NeMo W 2025-12-14 09:48:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.79it/s]\n",
      " 99%|█████████▊| 10290/10440 [23:25<00:18,  7.99it/s][NeMo W 2025-12-14 09:48:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.53it/s]\n",
      " 99%|█████████▊| 10291/10440 [23:26<00:18,  8.04it/s][NeMo W 2025-12-14 09:48:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.22it/s]\n",
      " 99%|█████████▊| 10292/10440 [23:26<00:19,  7.66it/s][NeMo W 2025-12-14 09:48:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.95it/s]\n",
      " 99%|█████████▊| 10293/10440 [23:26<00:19,  7.73it/s][NeMo W 2025-12-14 09:48:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.12it/s]\n",
      " 99%|█████████▊| 10294/10440 [23:26<00:18,  7.96it/s][NeMo W 2025-12-14 09:48:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:34 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.01it/s]\n",
      " 99%|█████████▊| 10295/10440 [23:26<00:19,  7.62it/s][NeMo W 2025-12-14 09:48:34 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.47it/s]\n",
      " 99%|█████████▊| 10296/10440 [23:26<00:19,  7.53it/s][NeMo W 2025-12-14 09:48:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.21it/s]\n",
      " 99%|█████████▊| 10297/10440 [23:26<00:18,  7.60it/s][NeMo W 2025-12-14 09:48:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.94it/s]\n",
      " 99%|█████████▊| 10298/10440 [23:26<00:18,  7.52it/s][NeMo W 2025-12-14 09:48:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.84it/s]\n",
      " 99%|█████████▊| 10299/10440 [23:27<00:18,  7.69it/s][NeMo W 2025-12-14 09:48:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.94it/s]\n",
      " 99%|█████████▊| 10300/10440 [23:27<00:18,  7.55it/s][NeMo W 2025-12-14 09:48:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.46it/s]\n",
      " 99%|█████████▊| 10301/10440 [23:27<00:17,  7.77it/s][NeMo W 2025-12-14 09:48:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.85it/s]\n",
      " 99%|█████████▊| 10302/10440 [23:27<00:17,  7.95it/s][NeMo W 2025-12-14 09:48:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:35 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.01it/s]\n",
      " 99%|█████████▊| 10303/10440 [23:27<00:16,  8.26it/s][NeMo W 2025-12-14 09:48:35 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.04it/s]\n",
      " 99%|█████████▊| 10304/10440 [23:27<00:16,  8.24it/s][NeMo W 2025-12-14 09:48:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.80it/s]\n",
      " 99%|█████████▊| 10305/10440 [23:27<00:17,  7.66it/s][NeMo W 2025-12-14 09:48:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.49it/s]\n",
      " 99%|█████████▊| 10306/10440 [23:27<00:17,  7.55it/s][NeMo W 2025-12-14 09:48:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.20it/s]\n",
      " 99%|█████████▊| 10307/10440 [23:28<00:16,  7.94it/s][NeMo W 2025-12-14 09:48:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.22it/s]\n",
      " 99%|█████████▊| 10308/10440 [23:28<00:16,  8.05it/s][NeMo W 2025-12-14 09:48:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.62it/s]\n",
      " 99%|█████████▊| 10309/10440 [23:28<00:16,  8.11it/s][NeMo W 2025-12-14 09:48:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.51it/s]\n",
      " 99%|█████████▉| 10310/10440 [23:28<00:15,  8.14it/s][NeMo W 2025-12-14 09:48:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.87it/s]\n",
      " 99%|█████████▉| 10311/10440 [23:28<00:15,  8.25it/s][NeMo W 2025-12-14 09:48:36 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:36 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.59it/s]\n",
      " 99%|█████████▉| 10312/10440 [23:28<00:15,  8.52it/s][NeMo W 2025-12-14 09:48:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.45it/s]\n",
      " 99%|█████████▉| 10313/10440 [23:28<00:15,  8.08it/s][NeMo W 2025-12-14 09:48:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.27it/s]\n",
      " 99%|█████████▉| 10314/10440 [23:29<00:18,  6.84it/s][NeMo W 2025-12-14 09:48:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.64it/s]\n",
      " 99%|█████████▉| 10315/10440 [23:29<00:18,  6.72it/s][NeMo W 2025-12-14 09:48:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.61it/s]\n",
      " 99%|█████████▉| 10316/10440 [23:29<00:17,  6.90it/s][NeMo W 2025-12-14 09:48:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.35it/s]\n",
      " 99%|█████████▉| 10317/10440 [23:29<00:16,  7.39it/s][NeMo W 2025-12-14 09:48:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.18it/s]\n",
      " 99%|█████████▉| 10318/10440 [23:29<00:16,  7.45it/s][NeMo W 2025-12-14 09:48:37 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:37 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.44it/s]\n",
      " 99%|█████████▉| 10319/10440 [23:29<00:16,  7.54it/s][NeMo W 2025-12-14 09:48:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.15it/s]\n",
      " 99%|█████████▉| 10320/10440 [23:29<00:15,  7.70it/s][NeMo W 2025-12-14 09:48:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.42it/s]\n",
      " 99%|█████████▉| 10321/10440 [23:29<00:15,  7.81it/s][NeMo W 2025-12-14 09:48:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.90it/s]\n",
      " 99%|█████████▉| 10322/10440 [23:30<00:16,  7.28it/s][NeMo W 2025-12-14 09:48:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.18it/s]\n",
      " 99%|█████████▉| 10323/10440 [23:30<00:16,  7.31it/s][NeMo W 2025-12-14 09:48:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.85it/s]\n",
      " 99%|█████████▉| 10324/10440 [23:30<00:15,  7.59it/s][NeMo W 2025-12-14 09:48:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.55it/s]\n",
      " 99%|█████████▉| 10325/10440 [23:30<00:14,  7.79it/s][NeMo W 2025-12-14 09:48:38 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:38 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.88it/s]\n",
      " 99%|█████████▉| 10326/10440 [23:30<00:14,  7.78it/s][NeMo W 2025-12-14 09:48:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.62it/s]\n",
      " 99%|█████████▉| 10327/10440 [23:30<00:14,  7.73it/s][NeMo W 2025-12-14 09:48:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.89it/s]\n",
      " 99%|█████████▉| 10328/10440 [23:30<00:14,  7.83it/s][NeMo W 2025-12-14 09:48:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.97it/s]\n",
      " 99%|█████████▉| 10329/10440 [23:30<00:13,  8.10it/s][NeMo W 2025-12-14 09:48:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.42it/s]\n",
      " 99%|█████████▉| 10330/10440 [23:31<00:13,  8.20it/s][NeMo W 2025-12-14 09:48:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.64it/s]\n",
      " 99%|█████████▉| 10331/10440 [23:31<00:13,  8.07it/s][NeMo W 2025-12-14 09:48:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.69it/s]\n",
      " 99%|█████████▉| 10332/10440 [23:31<00:13,  7.95it/s][NeMo W 2025-12-14 09:48:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.91it/s]\n",
      " 99%|█████████▉| 10333/10440 [23:31<00:12,  8.25it/s][NeMo W 2025-12-14 09:48:39 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:39 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.24it/s]\n",
      " 99%|█████████▉| 10334/10440 [23:31<00:12,  8.20it/s][NeMo W 2025-12-14 09:48:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.12it/s]\n",
      " 99%|█████████▉| 10335/10440 [23:31<00:13,  7.85it/s][NeMo W 2025-12-14 09:48:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.43it/s]\n",
      " 99%|█████████▉| 10336/10440 [23:31<00:13,  7.47it/s][NeMo W 2025-12-14 09:48:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.49it/s]\n",
      " 99%|█████████▉| 10337/10440 [23:32<00:14,  6.94it/s][NeMo W 2025-12-14 09:48:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.93it/s]\n",
      " 99%|█████████▉| 10338/10440 [23:32<00:15,  6.72it/s][NeMo W 2025-12-14 09:48:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.55it/s]\n",
      " 99%|█████████▉| 10339/10440 [23:32<00:14,  6.88it/s][NeMo W 2025-12-14 09:48:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.18it/s]\n",
      " 99%|█████████▉| 10340/10440 [23:32<00:15,  6.66it/s][NeMo W 2025-12-14 09:48:40 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:40 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.80it/s]\n",
      " 99%|█████████▉| 10341/10440 [23:32<00:14,  7.03it/s][NeMo W 2025-12-14 09:48:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.32it/s]\n",
      " 99%|█████████▉| 10342/10440 [23:32<00:13,  7.03it/s][NeMo W 2025-12-14 09:48:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.45it/s]\n",
      " 99%|█████████▉| 10343/10440 [23:32<00:14,  6.63it/s][NeMo W 2025-12-14 09:48:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.01it/s]\n",
      " 99%|█████████▉| 10344/10440 [23:33<00:14,  6.46it/s][NeMo W 2025-12-14 09:48:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.30it/s]\n",
      " 99%|█████████▉| 10345/10440 [23:33<00:16,  5.85it/s][NeMo W 2025-12-14 09:48:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.73it/s]\n",
      " 99%|█████████▉| 10346/10440 [23:33<00:16,  5.58it/s][NeMo W 2025-12-14 09:48:41 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:41 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.08it/s]\n",
      " 99%|█████████▉| 10347/10440 [23:33<00:15,  5.86it/s][NeMo W 2025-12-14 09:48:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.41it/s]\n",
      " 99%|█████████▉| 10348/10440 [23:33<00:15,  6.05it/s][NeMo W 2025-12-14 09:48:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.28it/s]\n",
      " 99%|█████████▉| 10349/10440 [23:33<00:15,  6.03it/s][NeMo W 2025-12-14 09:48:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.48it/s]\n",
      " 99%|█████████▉| 10350/10440 [23:34<00:14,  6.29it/s][NeMo W 2025-12-14 09:48:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.06it/s]\n",
      " 99%|█████████▉| 10351/10440 [23:34<00:13,  6.36it/s][NeMo W 2025-12-14 09:48:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.89it/s]\n",
      " 99%|█████████▉| 10352/10440 [23:34<00:13,  6.36it/s][NeMo W 2025-12-14 09:48:42 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:42 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.95it/s]\n",
      " 99%|█████████▉| 10353/10440 [23:34<00:13,  6.53it/s][NeMo W 2025-12-14 09:48:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.05it/s]\n",
      " 99%|█████████▉| 10354/10440 [23:34<00:13,  6.43it/s][NeMo W 2025-12-14 09:48:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.48it/s]\n",
      " 99%|█████████▉| 10355/10440 [23:34<00:13,  6.47it/s][NeMo W 2025-12-14 09:48:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.49it/s]\n",
      " 99%|█████████▉| 10356/10440 [23:35<00:13,  6.32it/s][NeMo W 2025-12-14 09:48:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.77it/s]\n",
      " 99%|█████████▉| 10357/10440 [23:35<00:12,  6.43it/s][NeMo W 2025-12-14 09:48:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 10.95it/s]\n",
      " 99%|█████████▉| 10358/10440 [23:35<00:13,  6.18it/s][NeMo W 2025-12-14 09:48:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.36it/s]\n",
      " 99%|█████████▉| 10359/10440 [23:35<00:12,  6.28it/s][NeMo W 2025-12-14 09:48:43 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:43 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.11it/s]\n",
      " 99%|█████████▉| 10360/10440 [23:35<00:13,  6.03it/s][NeMo W 2025-12-14 09:48:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00,  9.97it/s]\n",
      " 99%|█████████▉| 10361/10440 [23:35<00:13,  5.82it/s][NeMo W 2025-12-14 09:48:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.66it/s]\n",
      " 99%|█████████▉| 10362/10440 [23:36<00:12,  6.36it/s][NeMo W 2025-12-14 09:48:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.55it/s]\n",
      " 99%|█████████▉| 10363/10440 [23:36<00:11,  6.52it/s][NeMo W 2025-12-14 09:48:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.28it/s]\n",
      " 99%|█████████▉| 10364/10440 [23:36<00:10,  6.91it/s][NeMo W 2025-12-14 09:48:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.52it/s]\n",
      " 99%|█████████▉| 10365/10440 [23:36<00:10,  7.11it/s][NeMo W 2025-12-14 09:48:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.33it/s]\n",
      " 99%|█████████▉| 10366/10440 [23:36<00:10,  7.12it/s][NeMo W 2025-12-14 09:48:44 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:44 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.05it/s]\n",
      " 99%|█████████▉| 10367/10440 [23:36<00:10,  7.24it/s][NeMo W 2025-12-14 09:48:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.30it/s]\n",
      " 99%|█████████▉| 10368/10440 [23:36<00:10,  7.09it/s][NeMo W 2025-12-14 09:48:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.08it/s]\n",
      " 99%|█████████▉| 10369/10440 [23:36<00:09,  7.23it/s][NeMo W 2025-12-14 09:48:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.21it/s]\n",
      " 99%|█████████▉| 10370/10440 [23:37<00:09,  7.36it/s][NeMo W 2025-12-14 09:48:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.07it/s]\n",
      " 99%|█████████▉| 10371/10440 [23:37<00:09,  7.23it/s][NeMo W 2025-12-14 09:48:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.30it/s]\n",
      " 99%|█████████▉| 10372/10440 [23:37<00:08,  7.69it/s][NeMo W 2025-12-14 09:48:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.68it/s]\n",
      " 99%|█████████▉| 10373/10440 [23:37<00:08,  7.83it/s][NeMo W 2025-12-14 09:48:45 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:45 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.15it/s]\n",
      " 99%|█████████▉| 10374/10440 [23:37<00:08,  7.43it/s][NeMo W 2025-12-14 09:48:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.28it/s]\n",
      " 99%|█████████▉| 10375/10440 [23:37<00:08,  7.57it/s][NeMo W 2025-12-14 09:48:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.93it/s]\n",
      " 99%|█████████▉| 10376/10440 [23:37<00:08,  7.87it/s][NeMo W 2025-12-14 09:48:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.65it/s]\n",
      " 99%|█████████▉| 10377/10440 [23:37<00:07,  8.15it/s][NeMo W 2025-12-14 09:48:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.54it/s]\n",
      " 99%|█████████▉| 10378/10440 [23:38<00:07,  7.95it/s][NeMo W 2025-12-14 09:48:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.04it/s]\n",
      " 99%|█████████▉| 10379/10440 [23:38<00:07,  8.12it/s][NeMo W 2025-12-14 09:48:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 19.04it/s]\n",
      " 99%|█████████▉| 10380/10440 [23:38<00:07,  8.45it/s][NeMo W 2025-12-14 09:48:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.38it/s]\n",
      " 99%|█████████▉| 10381/10440 [23:38<00:06,  8.59it/s][NeMo W 2025-12-14 09:48:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.38it/s]\n",
      " 99%|█████████▉| 10382/10440 [23:38<00:06,  8.75it/s][NeMo W 2025-12-14 09:48:46 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:46 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.15it/s]\n",
      " 99%|█████████▉| 10383/10440 [23:38<00:06,  8.53it/s][NeMo W 2025-12-14 09:48:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.43it/s]\n",
      " 99%|█████████▉| 10384/10440 [23:38<00:06,  8.56it/s][NeMo W 2025-12-14 09:48:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.54it/s]\n",
      " 99%|█████████▉| 10385/10440 [23:38<00:06,  8.51it/s][NeMo W 2025-12-14 09:48:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.29it/s]\n",
      " 99%|█████████▉| 10386/10440 [23:39<00:06,  8.63it/s][NeMo W 2025-12-14 09:48:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.62it/s]\n",
      " 99%|█████████▉| 10387/10440 [23:39<00:06,  8.69it/s][NeMo W 2025-12-14 09:48:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.75it/s]\n",
      "100%|█████████▉| 10388/10440 [23:39<00:06,  8.21it/s][NeMo W 2025-12-14 09:48:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.81it/s]\n",
      "100%|█████████▉| 10389/10440 [23:39<00:06,  8.05it/s][NeMo W 2025-12-14 09:48:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.06it/s]\n",
      "100%|█████████▉| 10390/10440 [23:39<00:06,  8.11it/s][NeMo W 2025-12-14 09:48:47 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:47 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.75it/s]\n",
      "100%|█████████▉| 10391/10440 [23:39<00:06,  7.50it/s][NeMo W 2025-12-14 09:48:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.41it/s]\n",
      "100%|█████████▉| 10392/10440 [23:39<00:06,  7.37it/s][NeMo W 2025-12-14 09:48:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.39it/s]\n",
      "100%|█████████▉| 10393/10440 [23:39<00:06,  7.31it/s][NeMo W 2025-12-14 09:48:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.08it/s]\n",
      "100%|█████████▉| 10394/10440 [23:40<00:06,  7.27it/s][NeMo W 2025-12-14 09:48:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.87it/s]\n",
      "100%|█████████▉| 10395/10440 [23:40<00:05,  7.58it/s][NeMo W 2025-12-14 09:48:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.34it/s]\n",
      "100%|█████████▉| 10396/10440 [23:40<00:05,  7.66it/s][NeMo W 2025-12-14 09:48:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.08it/s]\n",
      "100%|█████████▉| 10397/10440 [23:40<00:05,  7.39it/s][NeMo W 2025-12-14 09:48:48 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:48 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.27it/s]\n",
      "100%|█████████▉| 10398/10440 [23:40<00:05,  7.56it/s][NeMo W 2025-12-14 09:48:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.13it/s]\n",
      "100%|█████████▉| 10399/10440 [23:40<00:05,  7.06it/s][NeMo W 2025-12-14 09:48:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.18it/s]\n",
      "100%|█████████▉| 10400/10440 [23:40<00:05,  6.96it/s][NeMo W 2025-12-14 09:48:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.44it/s]\n",
      "100%|█████████▉| 10401/10440 [23:41<00:05,  7.37it/s][NeMo W 2025-12-14 09:48:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.30it/s]\n",
      "100%|█████████▉| 10402/10440 [23:41<00:05,  7.30it/s][NeMo W 2025-12-14 09:48:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 11.96it/s]\n",
      "100%|█████████▉| 10403/10440 [23:41<00:05,  7.00it/s][NeMo W 2025-12-14 09:48:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.85it/s]\n",
      "100%|█████████▉| 10404/10440 [23:41<00:04,  7.36it/s][NeMo W 2025-12-14 09:48:49 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:49 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.55it/s]\n",
      "100%|█████████▉| 10405/10440 [23:41<00:04,  7.73it/s][NeMo W 2025-12-14 09:48:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.37it/s]\n",
      "100%|█████████▉| 10406/10440 [23:41<00:05,  6.66it/s][NeMo W 2025-12-14 09:48:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.26it/s]\n",
      "100%|█████████▉| 10407/10440 [23:41<00:04,  6.73it/s][NeMo W 2025-12-14 09:48:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.67it/s]\n",
      "100%|█████████▉| 10408/10440 [23:42<00:04,  7.03it/s][NeMo W 2025-12-14 09:48:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.24it/s]\n",
      "100%|█████████▉| 10409/10440 [23:42<00:04,  7.31it/s][NeMo W 2025-12-14 09:48:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.81it/s]\n",
      "100%|█████████▉| 10410/10440 [23:42<00:03,  7.51it/s][NeMo W 2025-12-14 09:48:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.47it/s]\n",
      "100%|█████████▉| 10411/10440 [23:42<00:03,  7.46it/s][NeMo W 2025-12-14 09:48:50 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:50 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.19it/s]\n",
      "100%|█████████▉| 10412/10440 [23:42<00:03,  7.29it/s][NeMo W 2025-12-14 09:48:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.75it/s]\n",
      "100%|█████████▉| 10413/10440 [23:42<00:03,  7.49it/s][NeMo W 2025-12-14 09:48:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.47it/s]\n",
      "100%|█████████▉| 10414/10440 [23:42<00:03,  7.63it/s][NeMo W 2025-12-14 09:48:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.80it/s]\n",
      "100%|█████████▉| 10415/10440 [23:42<00:03,  7.45it/s][NeMo W 2025-12-14 09:48:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.04it/s]\n",
      "100%|█████████▉| 10416/10440 [23:43<00:03,  7.63it/s][NeMo W 2025-12-14 09:48:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.36it/s]\n",
      "100%|█████████▉| 10417/10440 [23:43<00:02,  7.83it/s][NeMo W 2025-12-14 09:48:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.23it/s]\n",
      "100%|█████████▉| 10418/10440 [23:43<00:02,  7.81it/s][NeMo W 2025-12-14 09:48:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.73it/s]\n",
      "100%|█████████▉| 10419/10440 [23:43<00:02,  8.15it/s][NeMo W 2025-12-14 09:48:51 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:51 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.06it/s]\n",
      "100%|█████████▉| 10420/10440 [23:43<00:02,  7.68it/s][NeMo W 2025-12-14 09:48:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.40it/s]\n",
      "100%|█████████▉| 10421/10440 [23:43<00:02,  7.87it/s][NeMo W 2025-12-14 09:48:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.17it/s]\n",
      "100%|█████████▉| 10422/10440 [23:43<00:02,  7.79it/s][NeMo W 2025-12-14 09:48:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.02it/s]\n",
      "100%|█████████▉| 10423/10440 [23:43<00:02,  7.65it/s][NeMo W 2025-12-14 09:48:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.35it/s]\n",
      "100%|█████████▉| 10424/10440 [23:44<00:02,  7.35it/s][NeMo W 2025-12-14 09:48:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 12.20it/s]\n",
      "100%|█████████▉| 10425/10440 [23:44<00:02,  6.91it/s][NeMo W 2025-12-14 09:48:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.07it/s]\n",
      "100%|█████████▉| 10426/10440 [23:44<00:01,  7.05it/s][NeMo W 2025-12-14 09:48:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.89it/s]\n",
      "100%|█████████▉| 10427/10440 [23:44<00:01,  7.43it/s][NeMo W 2025-12-14 09:48:52 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:52 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.52it/s]\n",
      "100%|█████████▉| 10428/10440 [23:44<00:01,  7.78it/s][NeMo W 2025-12-14 09:48:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.78it/s]\n",
      "100%|█████████▉| 10429/10440 [23:44<00:01,  8.01it/s][NeMo W 2025-12-14 09:48:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 17.93it/s]\n",
      "100%|█████████▉| 10430/10440 [23:44<00:01,  8.17it/s][NeMo W 2025-12-14 09:48:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.50it/s]\n",
      "100%|█████████▉| 10431/10440 [23:45<00:01,  8.11it/s][NeMo W 2025-12-14 09:48:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 13.89it/s]\n",
      "100%|█████████▉| 10432/10440 [23:45<00:01,  7.85it/s][NeMo W 2025-12-14 09:48:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 18.15it/s]\n",
      "100%|█████████▉| 10433/10440 [23:45<00:00,  8.03it/s][NeMo W 2025-12-14 09:48:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 16.74it/s]\n",
      "100%|█████████▉| 10434/10440 [23:45<00:00,  8.10it/s][NeMo W 2025-12-14 09:48:53 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:53 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.16it/s]\n",
      "100%|█████████▉| 10435/10440 [23:45<00:00,  7.71it/s][NeMo W 2025-12-14 09:48:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.38it/s]\n",
      "100%|█████████▉| 10436/10440 [23:45<00:00,  7.35it/s][NeMo W 2025-12-14 09:48:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 15.92it/s]\n",
      "100%|█████████▉| 10437/10440 [23:45<00:00,  7.49it/s][NeMo W 2025-12-14 09:48:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.33it/s]\n",
      "100%|█████████▉| 10438/10440 [23:45<00:00,  7.53it/s][NeMo W 2025-12-14 09:48:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 1it [00:00, 14.63it/s]\n",
      "100%|█████████▉| 10439/10440 [23:46<00:00,  7.45it/s][NeMo W 2025-12-14 09:48:54 nemo_logging:405] The following configuration keys are ignored by Lhotse dataloader: use_start_end_token\n",
      "[NeMo W 2025-12-14 09:48:54 nemo_logging:405] You are using a non-tarred dataset and requested tokenization during data sampling (pretokenize=True). This will cause the tokenization to happen in the main (GPU) process,possibly impacting the training speed if your tokenizer is very large.If the impact is noticable, set pretokenize=False in dataloader config.(note: that will disable token-per-second filtering and 2D bucketing features)\n",
      "\n",
      "Transcribing: 0it [00:00, ?it/s]\u001b[A\n",
      "Transcribing: 1it [00:00,  9.36it/s]\n",
      "100%|██████████| 10440/10440 [23:46<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Transcribed 10440 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "import os\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "# Run inference\n",
    "print(\"\\nRunning inference on test set...\")\n",
    "references = []\n",
    "hypotheses = []\n",
    "\n",
    "MAX_SAMPLES = None\n",
    "\n",
    "# Handle None case for MAX_SAMPLES\n",
    "if MAX_SAMPLES is None:\n",
    "    test_samples = test_dataset\n",
    "else:\n",
    "    test_samples = test_dataset.select(range(min(MAX_SAMPLES, len(test_dataset))))\n",
    "\n",
    "for idx, sample in enumerate(tqdm(test_samples)):\n",
    "    try:\n",
    "        # Get audio and reference\n",
    "        audio_array = sample[\"audio\"][\"array\"]\n",
    "        sampling_rate = sample[\"audio\"][\"sampling_rate\"]\n",
    "        reference_text = sample[\"sentence\"]\n",
    "\n",
    "        # Save audio to temp file\n",
    "        with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_audio:\n",
    "            temp_path = temp_audio.name\n",
    "            sf.write(temp_path, audio_array, sampling_rate)\n",
    "\n",
    "            # Transcribe\n",
    "            result = model.transcribe([temp_path])\n",
    "\n",
    "            # Extract text from Hypothesis object\n",
    "            hypothesis_text = result[0].text\n",
    "\n",
    "            # Clean up\n",
    "            os.unlink(temp_path)\n",
    "\n",
    "        # Store as strings\n",
    "        references.append(str(reference_text).strip())\n",
    "        hypotheses.append(str(hypothesis_text).strip())\n",
    "\n",
    "        # Print first 3 examples\n",
    "        if idx < 3:\n",
    "            print(f\"\\nExample {idx+1}:\")\n",
    "            print(f\"  Reference:  {reference_text}\")\n",
    "            print(f\"  Prediction: {hypothesis_text}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n⚠️  Error on sample {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✅ Transcribed {len(hypotheses)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ufe9ZT83YWaU",
    "outputId": "2734329b-31d5-4447-f3b5-49f3a7727007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 Word Error Rate (WER): 11.81%\n",
      "📊 Character Error Rate (CER): 3.33%\n",
      "\n",
      "📈 Match Error Rate (MER): 11.78%\n",
      "📈 Word Information Lost (WIL): 20.89%\n"
     ]
    }
   ],
   "source": [
    "wer = jiwer.wer(references, hypotheses)\n",
    "cer = jiwer.cer(references, hypotheses)\n",
    "mer = jiwer.mer(references, hypotheses)\n",
    "wil = jiwer.wil(references, hypotheses)\n",
    "\n",
    "print(f\"\\n📊 Word Error Rate (WER): {wer*100:.2f}%\")\n",
    "print(f\"📊 Character Error Rate (CER): {cer*100:.2f}%\")\n",
    "print(f\"\\n📈 Match Error Rate (MER): {mer*100:.2f}%\")\n",
    "print(f\"📈 Word Information Lost (WIL): {wil*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dqZiijfsYccd",
    "outputId": "b48f2377-50eb-4d42-a205-a14785a09128"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1:\n",
      "  Reference:  اتوبوس مسافری\n",
      "  Prediction: اتوبوس مسافر\n",
      "  WER: 50.00%\n",
      "\n",
      "Example 2:\n",
      "  Reference:  انعکاس مثبت دهید و اعتبار ببخشید\n",
      "  Prediction: انعکاس مثبت دهید و اعتبار ببخشید\n",
      "  WER: 0.00%\n",
      "\n",
      "Example 3:\n",
      "  Reference:  جنگ افزارهای ساده\n",
      "  Prediction: جنگ افزارهای ساده\n",
      "  WER: 0.00%\n",
      "\n",
      "Example 4:\n",
      "  Reference:  آیا حضرت عالی مرا صدا زدید؟\n",
      "  Prediction: آیا حضرت عالی مرا صدا زدید\n",
      "  WER: 16.67%\n",
      "\n",
      "Example 5:\n",
      "  Reference:  باید باهاش حرف بزنم -\n",
      "  Prediction: باید باهاش حرف بزنم\n",
      "  WER: 20.00%\n",
      "\n",
      "Example 6:\n",
      "  Reference:  غدد لنفاوی من ورم کرده اند.\n",
      "  Prediction: غدد لنفاوی من ورم کرده اند\n",
      "  WER: 16.67%\n",
      "\n",
      "Example 7:\n",
      "  Reference:  تو این دنیا یکی دلیل زندگیمه\n",
      "  Prediction: تو این دنیا یکی دلیل زندگیمه\n",
      "  WER: 0.00%\n",
      "\n",
      "Example 8:\n",
      "  Reference:  من خیلی بهتون علاقه دارم و ازتون میخوام که اگه شما هم منو میپسندین و بهم علاقه دارین بیشتر اشنا بشیم\n",
      "  Prediction: من خیلی بهتون علاقه دارم و ازتون میخوام که اگه شما هم منو میپسندین و بهم علاقه دارین بیشتر اشنا بشیم\n",
      "  WER: 0.00%\n",
      "\n",
      "Example 9:\n",
      "  Reference:  و خیلیها مادر شدند وقتی خودشان هنوز بچه بودند\n",
      "  Prediction: و خیلیها مادر شدند وقتی خودشان هنوز بچه بودند\n",
      "  WER: 0.00%\n",
      "\n",
      "Example 10:\n",
      "  Reference:  می خوام بچه دار بشم\n",
      "  Prediction: می خوام بچه دار بشم\n",
      "  WER: 0.00%\n"
     ]
    }
   ],
   "source": [
    "for i in range(min(10, len(references))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"  Reference:  {references[i]}\")\n",
    "    print(f\"  Prediction: {hypotheses[i]}\")\n",
    "    sample_wer = jiwer.wer([references[i]], [hypotheses[i]])\n",
    "    print(f\"  WER: {sample_wer*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
